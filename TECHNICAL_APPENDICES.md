# ðŸ“š ANNEXES TECHNIQUES
## Bible du DÃ©veloppeur AI/LLM 2026

---

# ANNEXE A : FORMULAIRE MATHÃ‰MATIQUE

## A.1 Attention Mechanism

### **Scaled Dot-Product Attention**
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

oÃ¹:
- Q âˆˆ â„^(nÃ—d_k) : Query matrix
- K âˆˆ â„^(mÃ—d_k) : Key matrix
- V âˆˆ â„^(mÃ—d_v) : Value matrix
- d_k : dimension des keys
- n : longueur de la sÃ©quence query
- m : longueur de la sÃ©quence key
```

### **Multi-Head Attention**
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O

oÃ¹ head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)

ParamÃ¨tres:
- W^Q_i âˆˆ â„^(d_modelÃ—d_k)
- W^K_i âˆˆ â„^(d_modelÃ—d_k)
- W^V_i âˆˆ â„^(d_modelÃ—d_v)
- W^O âˆˆ â„^(hd_vÃ—d_model)
- h : nombre de heads
- d_k = d_v = d_model/h
```

### **Self-Attention (cas particulier)**
```
SelfAttention(X) = Attention(XW^Q, XW^K, XW^V)
oÃ¹ X âˆˆ â„^(nÃ—d_model)
```

### **Causal Attention (masking)**
```
M_{ij} = {
  0   si i >= j (autoriser attention)
  -âˆž  si i < j  (masquer le futur)
}

Attention_causal(Q, K, V) = softmax((QK^T / âˆšd_k) + M) V
```

## A.2 Positional Encoding

### **Sinusoidal Positional Encoding (Vaswani et al.)**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

oÃ¹:
- pos : position dans la sÃ©quence
- i : dimension index
- d_model : dimension du modÃ¨le
```

### **Rotary Position Embedding (RoPE)**
```
RoPE(x_m, m) = [
  cos(mÎ¸_i)  -sin(mÎ¸_i)
  sin(mÎ¸_i)   cos(mÎ¸_i)
] [x_{2i}]
  [x_{2i+1}]

oÃ¹ Î¸_i = 10000^(-2i/d)
```

### **ALiBi (Attention with Linear Biases)**
```
softmax(q_i K^T + mÂ·(i-j))

oÃ¹ m est un slope spÃ©cifique Ã  chaque head
```

## A.3 Loss Functions

### **Cross-Entropy Loss (Language Modeling)**
```
L_CE = -âˆ‘_{i=1}^{V} y_i log(Å·_i)

Pour un batch:
L = -1/N âˆ‘_{n=1}^{N} âˆ‘_{i=1}^{V} y_{n,i} log(Å·_{n,i})

oÃ¹:
- V : taille du vocabulaire
- N : batch size
- y : one-hot encoded target
- Å· : predicted probabilities
```

### **Perplexity**
```
Perplexity = exp(L_CE) = exp(-1/N âˆ‘ log P(x_i))

InterprÃ©tation: "En moyenne, le modÃ¨le hÃ©site entre perplexity choix"
```

### **KL Divergence**
```
D_KL(P || Q) = âˆ‘_x P(x) log(P(x)/Q(x))

UtilisÃ© dans:
- RLHF (contrainte KL avec policy originale)
- Distillation (match distributions teacher-student)
```

## A.4 Optimization

### **Gradient Descent**
```
Î¸_{t+1} = Î¸_t - Î· âˆ‡_Î¸ L(Î¸_t)

oÃ¹:
- Î¸ : paramÃ¨tres
- Î· : learning rate
- âˆ‡_Î¸ L : gradient de la loss
```

### **SGD with Momentum**
```
v_t = Î²v_{t-1} + âˆ‡_Î¸ L(Î¸_t)
Î¸_{t+1} = Î¸_t - Î· v_t

oÃ¹ Î² âˆˆ [0,1] (typiquement 0.9)
```

### **Adam Optimizer**
```
m_t = Î²_1 m_{t-1} + (1-Î²_1) g_t              # 1st moment
v_t = Î²_2 v_{t-1} + (1-Î²_2) g_t^2            # 2nd moment

mÌ‚_t = m_t / (1-Î²_1^t)                        # bias correction
vÌ‚_t = v_t / (1-Î²_2^t)

Î¸_{t+1} = Î¸_t - Î· mÌ‚_t / (âˆšvÌ‚_t + Îµ)

oÃ¹:
- Î²_1 = 0.9 (typiquement)
- Î²_2 = 0.999
- Îµ = 1e-8
- g_t : gradient au temps t
```

### **AdamW (Adam with decoupled Weight Decay)**
```
Î¸_{t+1} = Î¸_t - Î· (mÌ‚_t / (âˆšvÌ‚_t + Îµ) + Î»Î¸_t)

oÃ¹ Î» est le coefficient de weight decay (typiquement 0.1)
```

### **Learning Rate Schedules**

**Linear Warmup:**
```
Î·(t) = Î·_max Â· min(1, t/t_warmup)
```

**Cosine Decay:**
```
Î·(t) = Î·_min + 0.5(Î·_max - Î·_min)(1 + cos(Ï€t/T))

oÃ¹ T est le nombre total de steps
```

**Inverse Square Root:**
```
Î·(t) = Î·_0 Â· min(1/âˆšt, t/t_warmup^(3/2))
```

## A.5 Normalization

### **Layer Normalization**
```
LN(x) = Î³ âŠ™ (x - Î¼)/âˆš(Ïƒ^2 + Îµ) + Î²

oÃ¹:
- Î¼ = 1/d âˆ‘_{i=1}^d x_i
- Ïƒ^2 = 1/d âˆ‘_{i=1}^d (x_i - Î¼)^2
- Î³, Î² : paramÃ¨tres apprenables
- d : feature dimension
```

### **RMSNorm (Root Mean Square Norm)**
```
RMSNorm(x) = x / RMS(x) Â· Î³

oÃ¹ RMS(x) = âˆš(1/d âˆ‘_{i=1}^d x_i^2)
```

## A.6 Information Theory

### **Entropy (Shannon)**
```
H(X) = -âˆ‘_x P(x) log P(x)

UnitÃ©s: bits (log base 2) ou nats (log naturel)
```

### **Cross-Entropy**
```
H(P, Q) = -âˆ‘_x P(x) log Q(x)
```

### **Mutual Information**
```
I(X; Y) = H(X) + H(Y) - H(X,Y)
        = âˆ‘âˆ‘ P(x,y) log(P(x,y)/(P(x)P(y)))
```

## A.7 Scaling Laws

### **Kaplan Scaling Laws (OpenAI, 2020)**
```
L(N, D) = (N_c/N)^Î±_N + (D_c/D)^Î±_D

oÃ¹:
- L : loss
- N : nombre de paramÃ¨tres
- D : taille du dataset (tokens)
- N_c, D_c, Î±_N, Î±_D : constantes empiriques
```

### **Chinchilla Scaling (DeepMind, 2022)**
```
N_optimal âˆ C^0.50
D_optimal âˆ C^0.50

oÃ¹ C est le compute budget (FLOPs)

RÃ¨gle: Pour compute optimal, utiliser autant de tokens que de paramÃ¨tres
Exemple: modÃ¨le 70B â†’ entraÃ®ner sur 70B tokens (minimum)
```

### **Calcul de FLOPs pour Training**
```
FLOPs â‰ˆ 6ND

oÃ¹:
- N : nombre de paramÃ¨tres
- D : nombre de tokens

Pour un forward pass:
FLOPs_forward â‰ˆ 2ND

Pour backward pass (2x forward):
FLOPs_backward â‰ˆ 4ND

Total: 6ND
```

## A.8 Fine-tuning

### **LoRA (Low-Rank Adaptation)**
```
W' = W_0 + Î”W = W_0 + BA

oÃ¹:
- W_0 âˆˆ â„^(dÃ—k) : poids prÃ©-entraÃ®nÃ©s (frozen)
- B âˆˆ â„^(dÃ—r) : down-projection (trainable)
- A âˆˆ â„^(rÃ—k) : up-projection (trainable)
- r << min(d,k) : rank (typiquement 8, 16, 32)

Nombre de paramÃ¨tres:
- Original: d Ã— k
- LoRA: r(d + k)
- RÃ©duction: ~1000x si r=8, d=k=4096
```

### **LoRA scaling**
```
h = W_0 x + (Î±/r) BA x

oÃ¹ Î± est un hyperparamÃ¨tre de scaling (souvent Î± = r)
```

## A.9 RLHF

### **Reward Model**
```
r_Î¸(x, y) : score de qualitÃ© de la rÃ©ponse y Ã  la question x

Loss (Bradley-Terry):
L_R(Î¸) = -E_{(x,y_w,y_l)} [log Ïƒ(r_Î¸(x,y_w) - r_Î¸(x,y_l))]

oÃ¹:
- y_w : rÃ©ponse prÃ©fÃ©rÃ©e (winner)
- y_l : rÃ©ponse rejetÃ©e (loser)
- Ïƒ : sigmoid
```

### **PPO (Proximal Policy Optimization)**
```
L^{CLIP}(Î¸) = ÃŠ_t[min(r_t(Î¸)Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Ã‚_t)]

oÃ¹:
- r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_{Î¸_old}(a_t|s_t)
- Ã‚_t : advantage estimÃ©
- Îµ : clip range (typiquement 0.2)

Objectif LLM complet:
L_PPO(Î¸) = E[r_Î¸(x,y) - Î²Â·D_KL(Ï€_Î¸ || Ï€_ref)]

oÃ¹:
- r_Î¸ : reward model
- Î² : coefficient KL (typiquement 0.01-0.1)
- Ï€_ref : policy de rÃ©fÃ©rence (SFT)
```

### **DPO (Direct Preference Optimization)**
```
L_DPO(Î¸) = -E_{(x,y_w,y_l)} [log Ïƒ(Î² log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x))
                                    - Î² log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)))]

Avantage: pas besoin d'entraÃ®ner reward model sÃ©parÃ©
```

---

# ANNEXE B : MÃ‰TRIQUES & BENCHMARKS

## B.1 MÃ©triques de GÃ©nÃ©ration de Texte

### **BLEU (Bilingual Evaluation Understudy)**
```
BLEU = BP Â· exp(âˆ‘_{n=1}^N w_n log p_n)

oÃ¹:
- p_n : precision des n-grams
- BP : brevity penalty = min(1, exp(1 - r/c))
- r : longueur rÃ©fÃ©rence
- c : longueur candidate
- N : max n-gram order (typiquement 4)

Limites:
- Ne capture pas sÃ©mantique
- Sensible Ã  l'ordre des mots
- Peu utilisÃ© pour LLMs modernes
```

### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**
```
ROUGE-N = âˆ‘_{SâˆˆRefs} âˆ‘_{gram_nâˆˆS} Count_match(gram_n) /
          âˆ‘_{SâˆˆRefs} âˆ‘_{gram_nâˆˆS} Count(gram_n)

ROUGE-L : basÃ© sur longest common subsequence

UtilisÃ© pour: rÃ©sumÃ©s, gÃ©nÃ©ration de texte
```

### **BERTScore**
```
R_BERT = 1/|x| âˆ‘_{x_iâˆˆx} max_{Å·_jâˆˆÅ·} x_i^T Å·_j
P_BERT = 1/|Å·| âˆ‘_{Å·_jâˆˆÅ·} max_{x_iâˆˆx} x_i^T Å·_j
F_BERT = 2Â·P_BERTÂ·R_BERT / (P_BERT + R_BERT)

oÃ¹ x_i, Å·_j sont des embeddings BERT

Avantage: capture similaritÃ© sÃ©mantique
```

## B.2 Benchmarks pour LLMs

### **MMLU (Massive Multitask Language Understanding)**
- **57 tÃ¢ches** (STEM, humanities, social sciences, etc.)
- **Format**: QCM (4 choix)
- **MÃ©trique**: Accuracy (%)
- **SOTA (2026)**: GPT-4: 86.4%, Claude 3 Opus: 86.8%

### **HellaSwag (Commonsense Reasoning)**
- **TÃ¢che**: Sentence completion
- **Format**: 4 choix
- **MÃ©trique**: Accuracy (%)
- **SOTA**: ~95% (GPT-4, Claude 3)

### **TruthfulQA**
- **TÃ¢che**: RÃ©pondre de maniÃ¨re factuelle (Ã©viter hallucinations)
- **Format**: QA
- **MÃ©trique**: % rÃ©ponses vraies
- **DifficultÃ©**: MÃªme humans ~90%

### **GSM8K (Grade School Math)**
- **8,500 problÃ¨mes** de mathÃ©matiques niveau primaire
- **Format**: Question â†’ rÃ©ponse numÃ©rique
- **MÃ©trique**: Exact match (%)
- **SOTA**: GPT-4: 92%, o1: 95%+

### **HumanEval (Code Generation)**
- **164 problÃ¨mes** de programmation Python
- **Format**: Docstring â†’ fonction complÃ¨te
- **MÃ©trique**: pass@k (% qui passent tests unitaires)
- **SOTA**: GPT-4: 67% (pass@1), Codex: 72%, AlphaCode: 50%

### **MATH (Competition Mathematics)**
- **12,500 problÃ¨mes** niveau compÃ©tition
- **Format**: LaTeX â†’ rÃ©ponse numÃ©rique
- **MÃ©trique**: Accuracy (%)
- **SOTA**: GPT-4: 42.5%, Minerva: 50.3%

### **BBHard (BIG-Bench Hard)**
- **23 tÃ¢ches** difficiles de BIG-Bench
- **TÃ¢ches** oÃ¹ CoT aide significativement
- **MÃ©trique**: Accuracy moyenne
- **SOTA**: GPT-4: 86%, PaLM 2: 78%

### **MT-Bench (Multi-Turn Conversations)**
- **80 conversations** multi-tours
- **CatÃ©gories**: Writing, Roleplay, Reasoning, Math, Coding, STEM, Humanities
- **MÃ©trique**: Score 1-10 (GPT-4 as judge)
- **SOTA**: GPT-4-Turbo: 9.32, Claude 3 Opus: 9.18

### **AlpacaEval (Instruction Following)**
- **805 instructions** diverses
- **Format**: Instruction â†’ rÃ©ponse
- **MÃ©trique**: % win vs rÃ©fÃ©rence (GPT-4 as judge)
- **SOTA**: GPT-4: 95%, Claude 3: 91%

## B.3 MÃ©triques RAG

### **Retrieval Metrics**

**Recall@k**
```
Recall@k = |relevant docs in top-k| / |total relevant docs|
```

**Precision@k**
```
Precision@k = |relevant docs in top-k| / k
```

**MRR (Mean Reciprocal Rank)**
```
MRR = 1/|Q| âˆ‘_{i=1}^{|Q|} 1/rank_i

oÃ¹ rank_i est le rang du premier doc pertinent pour query i
```

**NDCG (Normalized Discounted Cumulative Gain)**
```
DCG@k = âˆ‘_{i=1}^k (2^{rel_i} - 1) / log_2(i+1)
NDCG@k = DCG@k / IDCG@k

oÃ¹ IDCG est le DCG idÃ©al (ordre optimal)
```

### **Generation Metrics (RAGAS Framework)**

**Faithfulness**
```
Faithfulness = |statements supportÃ©s| / |total statements|

VÃ©rifie si la gÃ©nÃ©ration est ancrÃ©e dans les documents rÃ©cupÃ©rÃ©s
```

**Answer Relevancy**
```
Relevancy = similaritÃ©_cosine(question, generated_answer)

Utilise embeddings pour mesurer pertinence
```

**Context Precision**
```
Precision = âˆ‘_{k=1}^K (P(k) Ã— rel(k)) / |relevant docs|
```

**Context Recall**
```
Recall = |ground_truth claims in context| / |total ground_truth claims|
```

## B.4 MÃ©triques d'EfficacitÃ©

### **Latency**
- **Time to First Token (TTFT)**: Temps avant premier token gÃ©nÃ©rÃ©
- **Inter-Token Latency (ITL)**: Temps entre tokens
- **Total Latency**: Temps total gÃ©nÃ©ration

**Cibles production**:
- TTFT < 500ms (conversational)
- ITL < 50ms
- Total pour 100 tokens < 5s

### **Throughput**
```
Throughput = nombre de tokens gÃ©nÃ©rÃ©s / seconde

Batch throughput: tokens/sec avec batching
```

### **Memory Usage**
```
Memory (inference) â‰ˆ 2 Ã— N bytes (FP16)

Exemple:
- 7B params â†’ 14GB VRAM (FP16)
- 13B params â†’ 26GB VRAM
- 70B params â†’ 140GB VRAM

Avec quantization (INT8):
- 7B â†’ 7GB
- 70B â†’ 70GB
```

### **Cost Metrics**
```
Cost per 1M tokens = (inference_time Ã— GPU_cost_per_hour) / throughput

Exemple vLLM sur A100:
- Llama 2 7B: ~$0.20/1M tokens
- Llama 2 70B: ~$2.00/1M tokens
```

## B.5 Comparaison ModÃ¨les (2026)

| ModÃ¨le | Params | MMLU | HumanEval | Latence (ms/token) | CoÃ»t ($/1M tok) |
|--------|--------|------|-----------|-------------------|----------------|
| **GPT-4 Turbo** | ? | 86.4 | 67.0 | 50 | $10.00 |
| **GPT-4o** | ? | 87.2 | 72.0 | 30 | $5.00 |
| **Claude 3 Opus** | ? | 86.8 | 84.9 | 45 | $15.00 |
| **Claude 3.5 Sonnet** | ? | 88.7 | 92.0 | 35 | $3.00 |
| **Gemini 1.5 Pro** | ? | 85.9 | 71.9 | 40 | $7.00 |
| **Llama 3.1 405B** | 405B | 85.2 | 61.0 | 80 | $3.50 |
| **Llama 3.3 70B** | 70B | 82.0 | 58.0 | 25 | $0.60 |
| **Llama 3 8B** | 8B | 68.4 | 48.1 | 8 | $0.10 |
| **Mistral Large** | ? | 81.2 | 45.1 | 35 | $4.00 |
| **DeepSeek-V3** | 671B | 88.5 | 65.0 | 90 | $0.50 |
| **Qwen 2.5 72B** | 72B | 84.2 | 56.0 | 28 | $0.80 |

*(Valeurs indicatives 2026)*

---

# ANNEXE C : GLOSSAIRE COMPLET

## A

**Adapter Layers**: Couches supplÃ©mentaires entraÃ®nables insÃ©rÃ©es dans un modÃ¨le prÃ©-entraÃ®nÃ© (PEFT).

**Adversarial Examples**: Inputs conÃ§us pour tromper un modÃ¨le.

**Agent**: SystÃ¨me autonome capable d'utiliser des outils et de raisonner.

**ALiBi** (Attention with Linear Biases): MÃ©thode d'encodage positionnel par biais linÃ©aires.

**Alignment**: Processus de rendre un LLM utile, honnÃªte et inoffensif (RLHF, etc.).

**Attention**: MÃ©canisme permettant Ã  un modÃ¨le de se concentrer sur des parties pertinentes de l'input.

**Autoregressive**: GÃ©nÃ©ration sÃ©quentielle oÃ¹ chaque token dÃ©pend des prÃ©cÃ©dents.

**AWQ** (Activation-aware Weight Quantization): MÃ©thode de quantization prÃ©servant prÃ©cision.

## B

**Backpropagation**: Algorithme de calcul des gradients pour training.

**Batch Size**: Nombre d'exemples traitÃ©s simultanÃ©ment.

**BERT** (Bidirectional Encoder Representations from Transformers): ModÃ¨le encoder-only prÃ©-entraÃ®nÃ©.

**BF16** (Brain Float 16): Format numÃ©rique 16-bit optimisÃ© pour ML.

**Bias**: Dans attention, terme additionnel; aussi biais dans les donnÃ©es.

**BPE** (Byte-Pair Encoding): Algorithme de tokenization.

## C

**Causal Attention**: Attention masquÃ©e pour prÃ©venir accÃ¨s au futur (autoregressive).

**Checkpoint**: Sauvegarde de l'Ã©tat d'un modÃ¨le durant training.

**Chinchilla Scaling**: Loi d'Ã©chelle optimale (DeepMind 2022).

**Chunking**: DÃ©coupage de documents en morceaux pour RAG.

**CLM** (Causal Language Modeling): Objectif d'entraÃ®nement autoregressive.

**Constitutional AI**: MÃ©thode d'alignment par principes (Anthropic).

**Context Length**: Nombre maximum de tokens en input.

**Context Window**: FenÃªtre de contexte accessible au modÃ¨le.

**CoT** (Chain-of-Thought): Prompting incitant au raisonnement Ã©tape par Ã©tape.

**Cross-Attention**: Attention entre deux sÃ©quences diffÃ©rentes.

**Cross-Entropy**: Loss function pour classification/gÃ©nÃ©ration.

**CUDA**: Plateforme de calcul parallÃ¨le NVIDIA.

## D

**Decoding Strategy**: MÃ©thode de sÃ©lection des tokens (greedy, sampling, beam search).

**DeepSpeed**: BibliothÃ¨que d'optimisation de training distribuÃ© (Microsoft).

**Deterministic**: GÃ©nÃ©ration reproductible (temperature=0 ou seed fixe).

**Distillation**: Transfer de connaissances d'un grand modÃ¨le vers un petit.

**DPO** (Direct Preference Optimization): Alternative Ã  RLHF sans reward model.

**Dropout**: RÃ©gularisation par dÃ©sactivation alÃ©atoire de neurones.

## E

**Embedding**: ReprÃ©sentation vectorielle dense d'un token/mot.

**Encoder-Decoder**: Architecture avec encoder (comprÃ©hension) et decoder (gÃ©nÃ©ration).

**Epoch**: Une passe complÃ¨te sur le dataset d'entraÃ®nement.

**EOS** (End of Sequence): Token spÃ©cial marquant la fin.

## F

**Few-Shot Learning**: Apprentissage avec quelques exemples en contexte.

**Fine-Tuning**: EntraÃ®nement additionnel sur donnÃ©es spÃ©cifiques.

**Flash Attention**: ImplÃ©mentation optimisÃ©e de l'attention (IO-aware).

**FLOPs**: Floating Point Operations (mesure de compute).

**FP16/FP32**: Float 16-bit / 32-bit precision.

**FSDP** (Fully Sharded Data Parallel): StratÃ©gie de parallÃ©lisme (PyTorch).

**Function Calling**: CapacitÃ© du LLM Ã  appeler des fonctions externes.

## G

**GELU** (Gaussian Error Linear Unit): Fonction d'activation.

**Gradient Accumulation**: Accumuler gradients sur plusieurs mini-batches.

**Gradient Clipping**: Limiter la norme des gradients.

**GPTQ**: MÃ©thode de quantization post-training.

**Greedy Decoding**: SÃ©lection du token le plus probable Ã  chaque Ã©tape.

## H

**Hallucination**: GÃ©nÃ©ration d'informations fausses ou inventÃ©es.

**Head** (Attention): Une des tÃªtes d'attention dans multi-head attention.

**Hidden State**: ReprÃ©sentation interne dans les couches du modÃ¨le.

**HuggingFace**: Plateforme et bibliothÃ¨ques pour ML/NLP.

**Hybrid Search**: Combinaison de dense et sparse retrieval.

**Hyperparameter**: ParamÃ¨tre de configuration (learning rate, batch size, etc.).

## I

**In-Context Learning**: Apprentissage via exemples dans le prompt.

**Inference**: Utilisation du modÃ¨le pour faire des prÃ©dictions.

**Instruction Tuning**: Fine-tuning sur instructions/tÃ¢ches variÃ©es.

**INT8/INT4**: Quantization 8-bit ou 4-bit integer.

## J

**Jailbreak**: Contournement des guardrails d'un modÃ¨le.

**JSON Mode**: GÃ©nÃ©ration structurÃ©e en format JSON.

## K

**KL Divergence** (Kullback-Leibler): Mesure de divergence entre distributions.

**KV Cache**: Cache des Keys et Values pour accÃ©lÃ©rer inference autoregressive.

## L

**Latent Space**: Espace des reprÃ©sentations internes.

**Layer Normalization**: Normalisation par couche.

**Learning Rate**: Taux d'apprentissage pour l'optimiseur.

**LLM** (Large Language Model): Grand modÃ¨le de langage.

**LLMOps**: MLOps appliquÃ© aux LLMs.

**LoRA** (Low-Rank Adaptation): PEFT par matrices low-rank.

**Loss**: Fonction de coÃ»t Ã  minimiser.

**LSH** (Locality-Sensitive Hashing): Hashing pour recherche approximative.

## M

**Mamba**: Architecture State Space Model (alternative aux transformers).

**Masked Language Modeling**: PrÃ©dire tokens masquÃ©s (BERT).

**Maximum Likelihood**: Principe d'optimisation statistique.

**Memory (Agent)**: SystÃ¨me de mÃ©moire court/long terme pour agents.

**MLP** (Multi-Layer Perceptron): RÃ©seau fully-connected.

**MMLU**: Benchmark multitÃ¢che.

**MoE** (Mixture of Experts): Architecture avec routage vers experts.

**Multi-Head Attention**: Attention avec plusieurs tÃªtes parallÃ¨les.

**Multimodal**: ModÃ¨le traitant plusieurs modalitÃ©s (texte, image, audio).

## N

**nanoGPT**: ImplÃ©mentation minimaliste de GPT (Karpathy).

**NCCL**: BibliothÃ¨que de communication collective NVIDIA.

**NDCG**: MÃ©trique de ranking.

**Normalization**: Technique de stabilisation (LayerNorm, RMSNorm).

**Nucleus Sampling** (Top-p): Sampling dans le top-p% de probabilitÃ© cumulÃ©e.

**NumPy**: BibliothÃ¨que Python de calcul numÃ©rique.

## O

**One-Shot Learning**: Apprentissage avec un seul exemple.

**ORPO** (Odds Ratio Preference Optimization): MÃ©thode d'alignment (2024).

**Overfitting**: Sur-apprentissage sur les donnÃ©es d'entraÃ®nement.

**OpenAI**: Entreprise crÃ©atrice de GPT-3, GPT-4, ChatGPT.

## P

**Padding**: Ajout de tokens spÃ©ciaux pour uniformiser longueurs.

**Parameter**: Poids apprenables du modÃ¨le.

**Parameter-Efficient Fine-Tuning (PEFT)**: Fine-tuning de peu de paramÃ¨tres.

**Perplexity**: Mesure de performance (exp(loss)).

**PII** (Personally Identifiable Information): DonnÃ©es personnelles sensibles.

**Pipeline Parallelism**: ParallÃ©lisme par dÃ©coupage du modÃ¨le en stages.

**Position Embedding**: Encodage de la position des tokens.

**PPO** (Proximal Policy Optimization): Algorithme RL utilisÃ© dans RLHF.

**Prefix Tuning**: PEFT par prÃ©fixes entraÃ®nables.

**Prompt**: Input textuel donnÃ© au modÃ¨le.

**Prompt Engineering**: Art de concevoir des prompts efficaces.

**Prompt Injection**: Attaque par manipulation du prompt.

**Pruning**: Suppression de poids/neurones non importants.

**PyTorch**: Framework de deep learning.

## Q

**QLoRA**: LoRA avec quantization 4-bit.

**Quantization**: RÃ©duction de prÃ©cision numÃ©rique (FP16â†’INT8).

**Query**: Dans attention, vecteur de requÃªte.

## R

**RAG** (Retrieval-Augmented Generation): GÃ©nÃ©ration augmentÃ©e par retrieval.

**Rank** (LoRA): Dimension des matrices low-rank.

**ReAct**: Architecture d'agent (Reasoning + Acting).

**Recall**: MÃ©trique de retrieval (proportion de pertinents rÃ©cupÃ©rÃ©s).

**Regularization**: Techniques contre l'overfitting.

**Reinforcement Learning**: Apprentissage par rÃ©compenses.

**Replay Buffer**: MÃ©moire de transitions pour RL.

**Re-ranking**: Re-ordonnancement des rÃ©sultats de retrieval.

**Residual Connection**: Connexion rÃ©siduelle (x + F(x)).

**Reward Model**: ModÃ¨le de rÃ©compense pour RLHF.

**RLHF** (Reinforcement Learning from Human Feedback): Alignment par RL.

**RMSNorm**: Root Mean Square Normalization.

**RoPE** (Rotary Position Embedding): Encodage positionnel rotatif.

## S

**Sampling**: SÃ©lection stochastique de tokens.

**Scaling Laws**: Lois empiriques d'Ã©chelle (performance vs taille/data).

**Self-Attention**: Attention d'une sÃ©quence sur elle-mÃªme.

**Semantic Search**: Recherche par similaritÃ© sÃ©mantique.

**Sentence Transformers**: ModÃ¨les d'embeddings de phrases.

**SGD** (Stochastic Gradient Descent): Descente de gradient stochastique.

**SFT** (Supervised Fine-Tuning): Fine-tuning supervisÃ© sur instructions.

**Softmax**: Fonction de normalisation en probabilitÃ©s.

**Speculative Decoding**: GÃ©nÃ©ration avec modÃ¨le draft + vÃ©rification.

**SSM** (State Space Model): ModÃ¨le d'espace d'Ã©tats (Mamba).

**Stop Sequence**: SÃ©quence dÃ©clenchant l'arrÃªt de gÃ©nÃ©ration.

**Streaming**: GÃ©nÃ©ration token par token en temps rÃ©el.

**Supervised Learning**: Apprentissage avec labels.

## T

**T5**: ModÃ¨le encoder-decoder (Google).

**Teacher Forcing**: Utiliser vraies cibles durant training (pas prÃ©dictions).

**Temperature**: HyperparamÃ¨tre contrÃ´lant randomness de gÃ©nÃ©ration.

**Tensor**: Matrice multi-dimensionnelle.

**Tensor Parallelism**: ParallÃ©lisme par dÃ©coupage des tensors.

**Tokenization**: DÃ©coupage du texte en tokens.

**Top-k Sampling**: Sampling parmi les k tokens les plus probables.

**Top-p Sampling**: Nucleus sampling.

**TPU** (Tensor Processing Unit): AccÃ©lÃ©rateur Google.

**Training**: EntraÃ®nement du modÃ¨le.

**Transfer Learning**: RÃ©utilisation d'un modÃ¨le prÃ©-entraÃ®nÃ©.

**Transformer**: Architecture "Attention is All You Need" (2017).

**TRL** (Transformer Reinforcement Learning): BibliothÃ¨que HuggingFace pour RLHF.

## U

**Underfitting**: Sous-apprentissage (modÃ¨le trop simple).

**Unsloth**: Framework d'entraÃ®nement optimisÃ© (vitesse + mÃ©moire).

## V

**Validation Set**: DonnÃ©es pour Ã©valuation durant training.

**Vector Database**: Base de donnÃ©es pour embeddings (Pinecone, Qdrant, etc.).

**vLLM**: BibliothÃ¨que d'inference optimisÃ©e (PagedAttention).

**Vocabulary**: Ensemble des tokens connus du modÃ¨le.

**VQA** (Visual Question Answering): QA sur images.

## W

**Warmup**: Phase d'augmentation progressive du learning rate.

**Weight Decay**: RÃ©gularisation L2 sur les poids.

**Weights**: ParamÃ¨tres apprenables du modÃ¨le.

## Z

**Zero-Shot Learning**: InfÃ©rence sans exemples en contexte.

**ZeRO** (Zero Redundancy Optimizer): Optimisation mÃ©moire (DeepSpeed).

---

# ANNEXE D : RESSOURCES & LIENS

## D.1 Papers Fondateurs

### **Transformers**
1. [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017
2. [BERT](https://arxiv.org/abs/1810.04805) - Devlin et al., 2018
3. [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) - Radford et al., 2018
4. [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - Radford et al., 2019
5. [T5](https://arxiv.org/abs/1910.10683) - Raffel et al., 2020
6. [GPT-3](https://arxiv.org/abs/2005.14165) - Brown et al., 2020

### **Scaling & Training**
7. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Kaplan et al., 2020
8. [Training Compute-Optimal LLMs (Chinchilla)](https://arxiv.org/abs/2203.15556) - Hoffmann et al., 2022
9. [ZeRO](https://arxiv.org/abs/1910.02054) - Rajbhandari et al., 2020
10. [Megatron-LM](https://arxiv.org/abs/1909.08053) - Shoeybi et al., 2020

### **Fine-tuning & Alignment**
11. [LoRA](https://arxiv.org/abs/2106.09685) - Hu et al., 2021
12. [QLoRA](https://arxiv.org/abs/2305.14314) - Dettmers et al., 2023
13. [InstructGPT (RLHF)](https://arxiv.org/abs/2203.02155) - Ouyang et al., 2022
14. [DPO](https://arxiv.org/abs/2305.18290) - Rafailov et al., 2023
15. [Constitutional AI](https://arxiv.org/abs/2212.08073) - Bai et al., 2022

### **Open-Source Models**
16. [Llama](https://arxiv.org/abs/2302.13971) - Touvron et al., 2023
17. [Llama 2](https://arxiv.org/abs/2307.09288) - Touvron et al., 2023
18. [Mistral 7B](https://arxiv.org/abs/2310.06825) - Jiang et al., 2023
19. [Mixtral 8x7B](https://arxiv.org/abs/2401.04088) - Jiang et al., 2024

### **Agents & RAG**
20. [ReAct](https://arxiv.org/abs/2210.03629) - Yao et al., 2022
21. [RAG](https://arxiv.org/abs/2005.11401) - Lewis et al., 2020
22. [Toolformer](https://arxiv.org/abs/2302.04761) - Schick et al., 2023
23. [Reflexion](https://arxiv.org/abs/2303.11366) - Shinn et al., 2023

### **Multimodal**
24. [CLIP](https://arxiv.org/abs/2103.00020) - Radford et al., 2021
25. [Flamingo](https://arxiv.org/abs/2204.14198) - Alayrac et al., 2022
26. [LLaVA](https://arxiv.org/abs/2304.08485) - Liu et al., 2023
27. [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) - OpenAI, 2023

### **Optimization & Efficiency**
28. [FlashAttention](https://arxiv.org/abs/2205.14135) - Dao et al., 2022
29. [FlashAttention-2](https://arxiv.org/abs/2307.08691) - Dao, 2023
30. [GPTQ](https://arxiv.org/abs/2210.17323) - Frantar et al., 2023
31. [AWQ](https://arxiv.org/abs/2306.00978) - Lin et al., 2023

## D.2 Cours en Ligne

### **Fondations ML/DL**
- [Fast.ai - Practical Deep Learning](https://course.fast.ai/)
- [Stanford CS231n - CNN](http://cs231n.stanford.edu/)
- [Stanford CS224n - NLP](http://web.stanford.edu/class/cs224n/)
- [MIT 6.S191 - Intro to Deep Learning](http://introtodeeplearning.com/)

### **LLMs SpÃ©cifiques**
- [Andrej Karpathy - Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/)
- [DeepLearning.AI - LLM Specialization](https://www.deeplearning.ai/courses/)
- [fast.ai - From Deep Learning Foundations to Stable Diffusion](https://www.fast.ai/posts/part2-2022.html)

### **Production & MLOps**
- [Full Stack Deep Learning](https://fullstackdeeplearning.com/)
- [Made With ML](https://madewithml.com/)

## D.3 Blogs & Newsletters

### **Blogs Techniques**
- [Jay Alammar - Visualizing ML](https://jalammar.github.io/)
- [Lil'Log - Lilian Weng (OpenAI)](https://lilianweng.github.io/)
- [Sebastian Raschka](https://sebastianraschka.com/blog/)
- [Andrej Karpathy](https://karpathy.github.io/)
- [Hugging Face Blog](https://huggingface.co/blog)
- [OpenAI Research](https://openai.com/research)
- [Anthropic Research](https://www.anthropic.com/research)

### **Newsletters**
- [The Batch (DeepLearning.AI)](https://www.deeplearning.ai/the-batch/)
- [Import AI (Jack Clark)](https://jack-clark.net/)
- [TLDR AI](https://tldr.tech/ai)
- [The Gradient](https://thegradient.pub/)

## D.4 Outils & Frameworks

### **Training**
- [PyTorch](https://pytorch.org/) - Framework principal
- [JAX](https://jax.readthedocs.io/) - Alternative fonctionnelle
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)
- [DeepSpeed](https://www.deepspeed.ai/)
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
- [Unsloth](https://github.com/unslothai/unsloth)
- [torchtune](https://github.com/pytorch/torchtune)

### **Inference**
- [vLLM](https://github.com/vllm-project/vllm)
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Ollama](https://ollama.ai/)
- [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference)

### **Agents & RAG**
- [LangChain](https://python.langchain.com/)
- [LlamaIndex](https://www.llamaindex.ai/)
- [Haystack](https://haystack.deepset.ai/)
- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
- [CrewAI](https://www.crewai.com/)

### **Vector Databases**
- [Pinecone](https://www.pinecone.io/)
- [Qdrant](https://qdrant.tech/)
- [Weaviate](https://weaviate.io/)
- [Milvus](https://milvus.io/)
- [Chroma](https://www.trychroma.com/)
- [FAISS](https://github.com/facebookresearch/faiss)

### **Observability**
- [Weights & Biases](https://wandb.ai/)
- [MLflow](https://mlflow.org/)
- [LangSmith](https://www.langchain.com/langsmith)
- [Arize Phoenix](https://phoenix.arize.com/)
- [LangFuse](https://langfuse.com/)

## D.5 CommunautÃ©s

### **Discord/Slack**
- Hugging Face Discord
- EleutherAI Discord
- LAION Discord
- LangChain Discord

### **Forums**
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)
- [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)

### **Twitter/X**
- @karpathy (Andrej Karpathy)
- @ylecun (Yann LeCun)
- @goodfellow_ian (Ian Goodfellow)
- @AndrewYNg (Andrew Ng)
- @jackclarkSF (Jack Clark)

## D.6 Datasets

### **PrÃ©-training**
- [The Pile](https://pile.eleuther.ai/)
- [RedPajama](https://www.together.ai/blog/redpajama)
- [C4](https://huggingface.co/datasets/c4)
- [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)

### **Instruction Tuning**
- [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
- [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)
- [UltraChat](https://huggingface.co/datasets/stingning/ultrachat)

### **RLHF**
- [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1)

---

## ðŸ“– COMMENT UTILISER CES ANNEXES

### **Annexe A (Formules)**
- RÃ©fÃ©rence rapide durant implÃ©mentation
- VÃ©rifier formulations mathÃ©matiques
- Comprendre intuitions thÃ©oriques

### **Annexe B (MÃ©triques)**
- Ã‰valuer vos modÃ¨les
- Comparer avec SOTA
- Choisir mÃ©triques appropriÃ©es

### **Annexe C (Glossaire)**
- Lookup rapide de termes
- Clarifier jargon
- RÃ©fÃ©rence durant lecture de papers

### **Annexe D (Ressources)**
- Approfondir sujets spÃ©cifiques
- Rester Ã  jour (papers rÃ©cents)
- Trouver outils pour projets

---

**Ces annexes sont des compagnons essentiels de votre parcours. Bookmarkez-les!** ðŸ“š

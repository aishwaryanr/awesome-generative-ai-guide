{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cSbUkWG4peR"
   },
   "source": [
    "# V1: Action Autonomy - Router Agent\n",
    "\n",
    "## The Autonomy Ladder\n",
    "\n",
    "Building effective AI agents requires a deliberate approach to increasing autonomy:\n",
    "\n",
    "![Autonomy Ladder](assets/diagrams/autonomy_ladder.png)\n",
    "\n",
    "**Key Philosophy:** Start with a narrow, well-defined scope. Validate thoroughly. Then expand deliberately.\n",
    "\n",
    "## What is Action Autonomy?\n",
    "\n",
    "**Definition:** Agent performs single, well-defined classification or routing actions.\n",
    "\n",
    "**Use Case:** Customer support routing\n",
    "- Input: Customer message\n",
    "- Action: Classify intent and route to department\n",
    "- Output: Routing decision\n",
    "- Handoff: Human agent takes over\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_6HuEyA4peS"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and set up environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-HsGV0H4peT",
    "outputId": "2f34de79-fc51-49e8-e381-160547ce0f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install -q openai pandas python-dotenv\n",
    "!pip install -q 'arize-phoenix[evals]' openinference-instrumentation-openai\n",
    "\n",
    "print(\"Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6S03_Gi34peT",
    "outputId": "b3e4d705-2d2f-4dee-fd1c-478b22c9903f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'awesome-generative-ai-guide'...\n",
      "remote: Enumerating objects: 2054, done.\u001b[K\n",
      "remote: Counting objects: 100% (595/595), done.\u001b[K\n",
      "remote: Compressing objects: 100% (275/275), done.\u001b[K\n",
      "remote: Total 2054 (delta 438), reused 356 (delta 319), pack-reused 1459 (from 2)\u001b[K\n",
      "Receiving objects: 100% (2054/2054), 150.43 MiB | 16.97 MiB/s, done.\n",
      "Resolving deltas: 100% (1092/1092), done.\n",
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup for Colab vs Local\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repository for data access\n",
    "    if not os.path.exists('awesome-generative-ai-guide'):\n",
    "        !git clone https://github.com/aishwaryanr/awesome-generative-ai-guide.git\n",
    "\n",
    "    # Navigate to course directory\n",
    "    os.chdir('awesome-generative-ai-guide/resources/agentic_ai_course_lil')\n",
    "\n",
    "    # Get API key from Colab secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "    # Local environment - use .env file\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in Colab Secrets or .env file\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKGtQY404peU"
   },
   "source": [
    "## Building the Router Agent\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Our V1 agent has a simple 4-step process:\n",
    "\n",
    "<img src=\"assets/diagrams/v1_architecture.png\" alt=\"V1 Router Architecture\" width=\"400\"/>\n",
    "\n",
    "<img src=\"assets/diagrams/v1_data_flow.png\" alt=\"Data Flow Through System\" width=\"400\"/>\n",
    "\n",
    "### Key Design Choices\n",
    "\n",
    "1. **Model:** GPT-4o-mini (cost-effective for classification)\n",
    "2. **Temperature:** 0.1 (consistent results)\n",
    "3. **Output:** JSON mode (structured response)\n",
    "4. **Fallback:** ESCALATION if invalid department\n",
    "\n",
    "Let's build it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PySnnUAe4peU",
    "outputId": "fd8864bb-fe4a-4d12-beae-8eb66b4d3df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data structures defined!\n",
      "\n",
      "Available departments: ['BILLING', 'RETURNS', 'TECHNICAL_SUPPORT', 'ORDER_STATUS', 'PRODUCT_INQUIRY', 'ACCOUNT_MANAGEMENT', 'ESCALATION']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define data structures\n",
    "\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Department(Enum):\n",
    "    \"\"\"Available departments for routing.\"\"\"\n",
    "    BILLING = \"billing\"\n",
    "    RETURNS = \"returns\"\n",
    "    TECHNICAL_SUPPORT = \"technical_support\"\n",
    "    ORDER_STATUS = \"order_status\"\n",
    "    PRODUCT_INQUIRY = \"product_inquiry\"\n",
    "    ACCOUNT_MANAGEMENT = \"account_management\"\n",
    "    ESCALATION = \"escalation\"\n",
    "\n",
    "@dataclass\n",
    "class RoutingDecision:\n",
    "    \"\"\"Result of routing decision.\"\"\"\n",
    "    department: Department\n",
    "    reasoning: str\n",
    "    customer_message: str\n",
    "\n",
    "print(\"Data structures defined!\")\n",
    "print(f\"\\nAvailable departments: {[d.name for d in Department]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rr73JRF94peU",
    "outputId": "d782e30e-e6c7-4643-efbc-59c5d4c62568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 (baseline) defined!\n",
      "Prompt length: 258 chars\n",
      "\n",
      "Note: This is intentionally minimal. We'll see what happens...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define Prompt 1 (baseline)\n",
    "\n",
    "# Starting with minimal prompt - no department descriptions\n",
    "# We'll discover what's missing through evaluation\n",
    "\n",
    "SYSTEM_PROMPT_1 = \"\"\"Route customer messages to departments.\n",
    "\n",
    "Available departments: BILLING, RETURNS, TECHNICAL_SUPPORT, ORDER_STATUS, PRODUCT_INQUIRY, ACCOUNT_MANAGEMENT, ESCALATION\n",
    "\n",
    "Respond with JSON:\n",
    "{\n",
    "    \"department\": \"DEPARTMENT_NAME\",\n",
    "    \"reasoning\": \"Your reasoning\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt 1 (baseline) defined!\")\n",
    "print(f\"Prompt length: {len(SYSTEM_PROMPT_1)} chars\")\n",
    "print(\"\\nNote: This is intentionally minimal. We'll see what happens...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1tsvyTJ4peU",
    "outputId": "ac00dcdd-d320-46d3-93c5-aafa3ddf0d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RouterAgent class defined!\n",
      "Ready to route customer messages.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build the RouterAgent class\n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "class RouterAgent:\n",
    "    \"\"\"V1 Action Autonomy Agent - Routes customer messages to departments.\"\"\"\n",
    "\n",
    "    def __init__(self, system_prompt):\n",
    "        \"\"\"Initialize agent with a system prompt.\"\"\"\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def route(self, customer_message: str) -> RoutingDecision:\n",
    "        \"\"\"Route a customer message to appropriate department.\"\"\"\n",
    "\n",
    "        # Step 1: Call OpenAI API\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": customer_message}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        # Step 2: Parse JSON response\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        # Step 3: Validate department\n",
    "        dept_name = result.get(\"department\", \"ESCALATION\").upper()\n",
    "        try:\n",
    "            department = Department[dept_name]\n",
    "        except KeyError:\n",
    "            department = Department.ESCALATION\n",
    "\n",
    "        # Step 4: Return structured decision\n",
    "        return RoutingDecision(\n",
    "            department=department,\n",
    "            reasoning=result.get(\"reasoning\", \"No reasoning provided\"),\n",
    "            customer_message=customer_message\n",
    "        )\n",
    "\n",
    "print(\"RouterAgent class defined!\")\n",
    "print(\"Ready to route customer messages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fiT_muM4peU"
   },
   "source": [
    "## Demo: See the Agent in Action\n",
    "\n",
    "Let's test our agent with a few examples before formal evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UScRtDzx4peV",
    "outputId": "1a9bced8-73e7-4352-d665-958b45925418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ROUTER AGENT DEMO (Prompt 1 Baseline)\n",
      "======================================================================\n",
      "\n",
      "[1] Customer: I was charged twice for my order!\n",
      "    -> Department: BILLING\n",
      "    -> Reasoning: The customer is reporting an issue related to being charged twice, which falls under billing inquiries.\n",
      "\n",
      "[2] Customer: Where is my package? It's been 2 weeks!\n",
      "    -> Department: ORDER_STATUS\n",
      "    -> Reasoning: The customer is inquiring about the status of their package, which falls under order status inquiries.\n",
      "\n",
      "[3] Customer: I want to return these shoes, they don't fit\n",
      "    -> Department: RETURNS\n",
      "    -> Reasoning: The customer is requesting to return a product due to sizing issues, which falls under the returns department.\n",
      "\n",
      "[4] Customer: Is the blue wireless headphone in stock?\n",
      "    -> Department: PRODUCT_INQUIRY\n",
      "    -> Reasoning: The customer is asking about the availability of a specific product, which falls under product inquiries.\n",
      "\n",
      "[5] Customer: I can't log into my account, it says password invalid\n",
      "    -> Department: TECHNICAL_SUPPORT\n",
      "    -> Reasoning: The issue involves a login problem related to account access, which falls under technical support.\n",
      "\n",
      "[6] Customer: This is ridiculous! I've called 3 times and nobody helps me!\n",
      "    -> Department: ESCALATION\n",
      "    -> Reasoning: The customer is expressing frustration with previous attempts to get help, indicating a need for urgent attention and escalation to ensure their issue is addressed.\n",
      "\n",
      "Demo looks good! But let's evaluate systematically...\n"
     ]
    }
   ],
   "source": [
    "# Initialize agent with Prompt 1\n",
    "agent = RouterAgent(system_prompt=SYSTEM_PROMPT_1)\n",
    "\n",
    "# Test messages covering different departments\n",
    "test_messages = [\n",
    "    \"I was charged twice for my order!\",\n",
    "    \"Where is my package? It's been 2 weeks!\",\n",
    "    \"I want to return these shoes, they don't fit\",\n",
    "    \"Is the blue wireless headphone in stock?\",\n",
    "    \"I can't log into my account, it says password invalid\",\n",
    "    \"This is ridiculous! I've called 3 times and nobody helps me!\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ROUTER AGENT DEMO (Prompt 1 Baseline)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "for i, message in enumerate(test_messages, 1):\n",
    "    print(f\"[{i}] Customer: {message}\")\n",
    "\n",
    "    decision = agent.route(message)\n",
    "\n",
    "    print(f\"    -> Department: {decision.department.name}\")\n",
    "    print(f\"    -> Reasoning: {decision.reasoning}\")\n",
    "    print()\n",
    "\n",
    "print(\"Demo looks good! But let's evaluate systematically...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a5d82e3"
   },
   "source": [
    "---\n",
    "\n",
    "## üé¨ End of Chapter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv2VJf4U4peV"
   },
   "source": [
    "## Evaluation Setup\n",
    "\n",
    "### Why Evaluate?\n",
    "\n",
    "Demo showed it works, but we need systematic evaluation:\n",
    "- Does it handle edge cases?\n",
    "- What's the accuracy across all departments?\n",
    "- Where does it fail and why?\n",
    "\n",
    "### Evaluation Metric: Routing Accuracy\n",
    "\n",
    "For Prompt 2 (Action Autonomy), routing accuracy is the right metric:\n",
    "- **Clear ground truth:** Each message has one correct department\n",
    "- **Binary outcome:** Either correct or incorrect\n",
    "- **Easy to interpret:** 85% accuracy means 85% of routings are correct\n",
    "\n",
    "### Test Dataset\n",
    "\n",
    "30 test cases covering:\n",
    "- All 7 departments\n",
    "- Simple cases (clear keywords)\n",
    "- Ambiguous cases (multiple possible departments)\n",
    "- Edge cases (unusual requests)\n",
    "\n",
    "### Evaluation Workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr-PfN_p4peV",
    "outputId": "e1c0f107-680f-4135-b177-d3b3b57ac351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 test cases\n",
      "\n",
      "Columns: ['test_id', 'customer_message', 'expected_department', 'category']\n",
      "\n",
      "Department distribution:\n",
      "expected_department\n",
      "BILLING               6\n",
      "TECHNICAL_SUPPORT     6\n",
      "RETURNS               5\n",
      "PRODUCT_INQUIRY       5\n",
      "ACCOUNT_MANAGEMENT    4\n",
      "ORDER_STATUS          2\n",
      "ESCALATION            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample test cases:\n",
      "  test_id                                   customer_message  \\\n",
      "0   TC001                   I was charged twice for my order   \n",
      "1   TC002  Where is my package? Tracking says delivered b...   \n",
      "2   TC003            I want to return these shoes wrong size   \n",
      "3   TC004             Do you have the iPhone 15 case in red?   \n",
      "4   TC005                        I can't log into my account   \n",
      "\n",
      "  expected_department          category  \n",
      "0             BILLING  duplicate_charge  \n",
      "1        ORDER_STATUS  missing_delivery  \n",
      "2             RETURNS     size_exchange  \n",
      "3     PRODUCT_INQUIRY      availability  \n",
      "4   TECHNICAL_SUPPORT       login_issue  \n"
     ]
    }
   ],
   "source": [
    "# Load test cases\n",
    "import pandas as pd\n",
    "\n",
    "# Load from repository data directory\n",
    "test_df = pd.read_csv('/content/awesome-generative-ai-guide/resources/agentic_ai_course_lil/data/v1_test_cases.csv')\n",
    "\n",
    "print(f\"Loaded {len(test_df)} test cases\")\n",
    "print(f\"\\nColumns: {list(test_df.columns)}\")\n",
    "print(f\"\\nDepartment distribution:\")\n",
    "print(test_df['expected_department'].value_counts())\n",
    "\n",
    "# Show a few examples\n",
    "print(f\"\\nSample test cases:\")\n",
    "print(test_df[['test_id', 'customer_message', 'expected_department', 'category']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ5HMJ524peV"
   },
   "source": [
    "## Setup Arize Phoenix for Observability\n",
    "\n",
    "### Why Phoenix?\n",
    "\n",
    "Phoenix captures every LLM call as a \"trace\":\n",
    "- Input: Customer message\n",
    "- Prompt: System prompt sent to LLM\n",
    "- Output: Department and reasoning\n",
    "- Metadata: Tokens, latency, cost\n",
    "\n",
    "This lets us:\n",
    "1. See exactly what the agent is thinking\n",
    "2. Understand why failures happen\n",
    "3. Identify patterns in errors\n",
    "4. Make targeted improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "RVCBgmDH4peV",
    "outputId": "c8202ec6-118f-45a7-8a96-1230babe2913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Arize Phoenix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
      "  next(self.gen)\n",
      "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit https://jr5w3q56kyc1-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
      "üìñ For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
      "Phoenix session url: https://jr5w3q56kyc1-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
      "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
      "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(async (port, path, text, element) => {\n",
       "    if (!google.colab.kernel.accessAllowed) {\n",
       "      return;\n",
       "    }\n",
       "    element.appendChild(document.createTextNode(''));\n",
       "    const url = await google.colab.kernel.proxyPort(port);\n",
       "    const anchor = document.createElement('a');\n",
       "    anchor.href = new URL(path, url).toString();\n",
       "    anchor.target = '_blank';\n",
       "    anchor.setAttribute('data-href', url + path);\n",
       "    anchor.textContent = text;\n",
       "    element.appendChild(anchor);\n",
       "  })(6006, \"/\", \"https://localhost:6006/\", window.element)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Phoenix running on Colab at port 6006\n",
      "\n",
      "Click the link above to open Phoenix UI in a new tab.\n",
      "Keep this tab open while running evaluations.\n"
     ]
    }
   ],
   "source": [
    "# Start Phoenix (Colab-compatible setup)\n",
    "import os\n",
    "\n",
    "# Configure Phoenix for Colab/local compatibility\n",
    "os.environ[\"PHOENIX_HOST\"] = \"0.0.0.0\"\n",
    "os.environ[\"PHOENIX_PORT\"] = \"6006\"\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "print(\"Starting Arize Phoenix...\")\n",
    "session = px.launch_app()  # don't pass port parameter\n",
    "print(\"Phoenix session url:\", session.url)\n",
    "\n",
    "# For Google Colab compatibility\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_window(6006)\n",
    "    print(\"‚úì Phoenix running on Colab at port 6006\")\n",
    "except ImportError:\n",
    "    print(\"‚úì Phoenix running locally at http://localhost:6006\")\n",
    "\n",
    "print(\"\\nClick the link above to open Phoenix UI in a new tab.\")\n",
    "print(\"Keep this tab open while running evaluations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Efr4EMk4peV"
   },
   "source": [
    "## Run Prompt 1 Evaluation\n",
    "\n",
    "Let's evaluate the baseline (Prompt 1) to establish our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuhNz-4g4peW",
    "outputId": "d6a7795a-0a07-4953-80fe-e9022747c17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling tracing for project: V1_action_autonomy_prompt_1\n",
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: V1_action_autonomy_prompt_1\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n",
      "Tracing enabled! All API calls will be captured in Phoenix.\n"
     ]
    }
   ],
   "source": [
    "# Enable tracing for Prompt 1\n",
    "project_name = \"V1_action_autonomy_prompt_1\"\n",
    "print(f\"Enabling tracing for project: {project_name}\")\n",
    "\n",
    "tracer_provider = register(project_name=project_name)\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "\n",
    "print(\"Tracing enabled! All API calls will be captured in Phoenix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67dB6rcH4peW",
    "outputId": "211da3ba-d6c2-4b1e-bf83-2d82b46e9c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 1 evaluation on 30 test cases...\n",
      "(Each routing decision is being traced in Phoenix)\n",
      "\n",
      "[1/30] TC001: PASS (Expected: BILLING, Got: BILLING)\n",
      "[2/30] TC002: PASS (Expected: ORDER_STATUS, Got: ORDER_STATUS)\n",
      "[3/30] TC003: PASS (Expected: RETURNS, Got: RETURNS)\n",
      "[4/30] TC004: PASS (Expected: PRODUCT_INQUIRY, Got: PRODUCT_INQUIRY)\n",
      "[5/30] TC005: FAIL (Expected: TECHNICAL_SUPPORT, Got: ACCOUNT_MANAGEMENT)\n",
      "[6/30] TC006: PASS (Expected: ACCOUNT_MANAGEMENT, Got: ACCOUNT_MANAGEMENT)\n",
      "[7/30] TC007: PASS (Expected: ESCALATION, Got: ESCALATION)\n",
      "[8/30] TC008: FAIL (Expected: BILLING, Got: RETURNS)\n",
      "[9/30] TC009: PASS (Expected: TECHNICAL_SUPPORT, Got: TECHNICAL_SUPPORT)\n",
      "[10/30] TC010: PASS (Expected: ORDER_STATUS, Got: ORDER_STATUS)\n",
      "[11/30] TC011: PASS (Expected: RETURNS, Got: RETURNS)\n",
      "[12/30] TC012: PASS (Expected: PRODUCT_INQUIRY, Got: PRODUCT_INQUIRY)\n",
      "[13/30] TC013: PASS (Expected: ACCOUNT_MANAGEMENT, Got: ACCOUNT_MANAGEMENT)\n",
      "[14/30] TC014: PASS (Expected: BILLING, Got: BILLING)\n",
      "[15/30] TC015: PASS (Expected: TECHNICAL_SUPPORT, Got: TECHNICAL_SUPPORT)\n",
      "[16/30] TC016: PASS (Expected: RETURNS, Got: RETURNS)\n",
      "[17/30] TC017: PASS (Expected: PRODUCT_INQUIRY, Got: PRODUCT_INQUIRY)\n",
      "[18/30] TC018: FAIL (Expected: TECHNICAL_SUPPORT, Got: ACCOUNT_MANAGEMENT)\n",
      "[19/30] TC019: PASS (Expected: ESCALATION, Got: ESCALATION)\n",
      "[20/30] TC020: PASS (Expected: ACCOUNT_MANAGEMENT, Got: ACCOUNT_MANAGEMENT)\n",
      "[21/30] TC021: FAIL (Expected: BILLING, Got: ACCOUNT_MANAGEMENT)\n",
      "[22/30] TC022: FAIL (Expected: PRODUCT_INQUIRY, Got: ESCALATION)\n",
      "[23/30] TC023: PASS (Expected: TECHNICAL_SUPPORT, Got: TECHNICAL_SUPPORT)\n",
      "[24/30] TC024: FAIL (Expected: ACCOUNT_MANAGEMENT, Got: BILLING)\n",
      "[25/30] TC025: PASS (Expected: RETURNS, Got: RETURNS)\n",
      "[26/30] TC026: PASS (Expected: BILLING, Got: BILLING)\n",
      "[27/30] TC027: PASS (Expected: RETURNS, Got: RETURNS)\n",
      "[28/30] TC028: PASS (Expected: PRODUCT_INQUIRY, Got: PRODUCT_INQUIRY)\n",
      "[29/30] TC029: FAIL (Expected: TECHNICAL_SUPPORT, Got: ACCOUNT_MANAGEMENT)\n",
      "[30/30] TC030: FAIL (Expected: BILLING, Got: RETURNS)\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Run Prompt 1 evaluation\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Result of a single evaluation.\"\"\"\n",
    "    test_id: str\n",
    "    message: str\n",
    "    expected: str\n",
    "    predicted: str\n",
    "    correct: bool\n",
    "    reasoning: str\n",
    "    category: str\n",
    "\n",
    "# Initialize agent with Prompt 1\n",
    "agent_p1 = RouterAgent(system_prompt=SYSTEM_PROMPT_1)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "results_p1 = []\n",
    "\n",
    "print(\"Running Prompt 1 evaluation on 30 test cases...\")\n",
    "print(\"(Each routing decision is being traced in Phoenix)\\n\")\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    i = idx + 1\n",
    "    test_id = row['test_id']\n",
    "\n",
    "    # Create custom span for better Phoenix visualization\n",
    "    with tracer.start_as_current_span(f\"test_case_{test_id}\") as span:\n",
    "        span.set_attribute(\"test.id\", test_id)\n",
    "        span.set_attribute(\"test.category\", row['category'])\n",
    "        span.set_attribute(\"test.expected_department\", row['expected_department'])\n",
    "\n",
    "        # Route the message\n",
    "        decision = agent_p1.route(row['customer_message'])\n",
    "        correct = decision.department.name == row['expected_department']\n",
    "\n",
    "        # Record result in span\n",
    "        span.set_attribute(\"result.predicted_department\", decision.department.name)\n",
    "        span.set_attribute(\"result.correct\", correct)\n",
    "\n",
    "        if correct:\n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "        else:\n",
    "            span.set_status(Status(StatusCode.ERROR, \"Incorrect routing\"))\n",
    "            span.set_attribute(\"error.expected\", row['expected_department'])\n",
    "            span.set_attribute(\"error.got\", decision.department.name)\n",
    "\n",
    "        # Store result\n",
    "        result = EvalResult(\n",
    "            test_id=test_id,\n",
    "            message=row['customer_message'],\n",
    "            expected=row['expected_department'],\n",
    "            predicted=decision.department.name,\n",
    "            correct=correct,\n",
    "            reasoning=decision.reasoning,\n",
    "            category=row['category']\n",
    "        )\n",
    "        results_p1.append(result)\n",
    "\n",
    "        # Show progress\n",
    "        status = \"PASS\" if correct else \"FAIL\"\n",
    "        print(f\"[{i}/30] {test_id}: {status} (Expected: {result.expected}, Got: {result.predicted})\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbYLBReg4peW",
    "outputId": "ebf1b3c5-5472-4b9c-ef40-d5b8d8d2d2db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROMPT 1 EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Overall Accuracy: 73.3% (22/30 correct)\n",
      "\n",
      "Per-Department Accuracy:\n",
      "  ACCOUNT_MANAGEMENT   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 75%\n",
      "  BILLING              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 50%\n",
      "  ESCALATION           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%\n",
      "  ORDER_STATUS         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%\n",
      "  PRODUCT_INQUIRY      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 80%\n",
      "  RETURNS              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%\n",
      "  TECHNICAL_SUPPORT    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 50%\n",
      "\n",
      "Errors (8 cases):\n",
      "\n",
      "  [TC005] I can't log into my account...\n",
      "  Expected: TECHNICAL_SUPPORT -> Got: ACCOUNT_MANAGEMENT\n",
      "  Category: login_issue\n",
      "\n",
      "  [TC008] My refund still hasn't shown up it's been 2 weeks...\n",
      "  Expected: BILLING -> Got: RETURNS\n",
      "  Category: refund_status\n",
      "\n",
      "  [TC018] I forgot my password and the reset email isn't coming...\n",
      "  Expected: TECHNICAL_SUPPORT -> Got: ACCOUNT_MANAGEMENT\n",
      "  Category: password_reset\n",
      "\n",
      "  [TC021] Why didn't I get my loyalty points for this purchase?...\n",
      "  Expected: BILLING -> Got: ACCOUNT_MANAGEMENT\n",
      "  Category: points_missing\n",
      "\n",
      "  [TC022] Your prices are way too high! This is ridiculous!...\n",
      "  Expected: PRODUCT_INQUIRY -> Got: ESCALATION\n",
      "  Category: price_complaint\n",
      "\n",
      "  [TC024] I need to update my credit card on file...\n",
      "  Expected: ACCOUNT_MANAGEMENT -> Got: BILLING\n",
      "  Category: payment_update\n",
      "\n",
      "  [TC029] I reset my password but still can't access my account...\n",
      "  Expected: TECHNICAL_SUPPORT -> Got: ACCOUNT_MANAGEMENT\n",
      "  Category: access_issue\n",
      "\n",
      "  [TC030] Why was I charged a restocking fee?...\n",
      "  Expected: BILLING -> Got: RETURNS\n",
      "  Category: fee_inquiry\n"
     ]
    }
   ],
   "source": [
    "# Compute Prompt 1 metrics\n",
    "total = len(results_p1)\n",
    "correct = sum(1 for r in results_p1 if r.correct)\n",
    "accuracy = correct / total\n",
    "\n",
    "# Per-department accuracy\n",
    "dept_correct = defaultdict(int)\n",
    "dept_total = defaultdict(int)\n",
    "for r in results_p1:\n",
    "    dept_total[r.expected] += 1\n",
    "    if r.correct:\n",
    "        dept_correct[r.expected] += 1\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROMPT 1 EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.1%} ({correct}/{total} correct)\")\n",
    "\n",
    "print(f\"\\nPer-Department Accuracy:\")\n",
    "for dept in sorted(dept_total.keys()):\n",
    "    acc = dept_correct[dept] / dept_total[dept]\n",
    "    bar = \"‚ñà\" * int(acc * 20) + \"‚ñë\" * (20 - int(acc * 20))\n",
    "    print(f\"  {dept:20} {bar} {acc:.0%}\")\n",
    "\n",
    "# Show errors\n",
    "errors = [r for r in results_p1 if not r.correct]\n",
    "if errors:\n",
    "    print(f\"\\nErrors ({len(errors)} cases):\")\n",
    "    for r in errors:\n",
    "        print(f\"\\n  [{r.test_id}] {r.message[:60]}...\")\n",
    "        print(f\"  Expected: {r.expected} -> Got: {r.predicted}\")\n",
    "        print(f\"  Category: {r.category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "586f961f"
   },
   "source": [
    "---\n",
    "\n",
    "## üé¨ End of Chapter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce832e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Continuous Calibration (CC) Phase\n",
    "\n",
    "**Goal:** Understand WHY the system fails and design metrics to measure performance.\n",
    "\n",
    "**In this phase:**\n",
    "- Observe failures in Phoenix traces\n",
    "- Analyze error patterns\n",
    "- Design evaluation metrics\n",
    "- Identify root causes\n",
    "\n",
    "**Output:** Clear understanding of what to fix and how to measure it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ulUOlY4peW"
   },
   "source": [
    "## Analyze Failures in Arize Phoenix\n",
    "\n",
    "Now comes the key part: **Understanding WHY failures happened**\n",
    "\n",
    "### How to Use Phoenix\n",
    "\n",
    "1. Open the Phoenix URL from above\n",
    "2. Click \"Traces\" in the left sidebar\n",
    "3. Select project \"V1_action_autonomy_prompt_1\"\n",
    "4. Filter for failed cases (red status)\n",
    "5. Click on each trace to see:\n",
    "   - Customer message\n",
    "   - System prompt sent to LLM\n",
    "   - LLM's response (department + reasoning)\n",
    "   - Why it was incorrect\n",
    "\n",
    "### Common Failure Patterns\n",
    "\n",
    "Look for patterns like:\n",
    "- **Ambiguous keywords:** \"refund\" could be BILLING or RETURNS\n",
    "- **Multi-issue messages:** Customer mentions both shipping and refund\n",
    "- **Missing context:** Prompt 1 lacks department descriptions\n",
    "- **Over-escalation:** Negative sentiment triggers ESCALATION unnecessarily\n",
    "\n",
    "**Exercise:** Analyze 3-5 failed traces and note patterns you observe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b7c5b9b"
   },
   "source": [
    "---\n",
    "\n",
    "## üé¨ End of Chapter\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb1343",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Continuous Deployment (CD) Phase\n",
    "\n",
    "**Goal:** Improve the system based on CC insights and measure impact.\n",
    "\n",
    "**In this phase:**\n",
    "- Make targeted improvements (Prompt 2)\n",
    "- Re-evaluate with same metrics\n",
    "- Compare before/after performance\n",
    "- Validate improvements worked\n",
    "\n",
    "**Output:** Better system with measured improvements.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yI-iMR604peW"
   },
   "source": [
    "## Improve to Prompt 2\n",
    "\n",
    "Based on Phoenix analysis, we identified these issues in Prompt 1:\n",
    "\n",
    "1. **No department descriptions** ‚Üí LLM guesses based on keywords alone\n",
    "2. **Ambiguous boundaries** ‚Üí \"refund status\" routed to RETURNS instead of BILLING\n",
    "3. **Password resets** ‚Üí Routed to ACCOUNT_MANAGEMENT instead of TECHNICAL_SUPPORT\n",
    "\n",
    "### V1 Improvements\n",
    "\n",
    "The Prompt 2 adds:\n",
    "- Clear descriptions for each department\n",
    "- Explicit disambiguation rules\n",
    "- Examples of edge cases\n",
    "\n",
    "Let's see if it helps!\n",
    "\n",
    "### The Iterative Improvement Cycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tD5gZh9v4peW",
    "outputId": "770a02fe-579f-4300-90df-72c586ed86af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 2 (improved) created with improvements!\n",
      "\n",
      "Prompt 1 length: 258 chars\n",
      "Prompt 2 length: 926 chars\n",
      "\n",
      "Added 668 chars of context\n"
     ]
    }
   ],
   "source": [
    "# Now let's create Prompt 2 with improvements based on what we learned\n",
    "\n",
    "SYSTEM_PROMPT_2 = \"\"\"Route customer messages to departments.\n",
    "\n",
    "Available departments:\n",
    "- BILLING: Payment issues, charges, refunds, refund status, account balances, fees\n",
    "- RETURNS: Return requests, exchanges, return status, return policies\n",
    "- TECHNICAL_SUPPORT: Login problems, password reset issues, website errors, checkout failures\n",
    "- ORDER_STATUS: Order tracking, shipping updates, delivery questions, missing items\n",
    "- PRODUCT_INQUIRY: Product questions, specifications, availability, pricing\n",
    "- ACCOUNT_MANAGEMENT: Profile updates, changing saved payment methods, preferences, address changes\n",
    "- ESCALATION: Very upset customers demanding managers, supervisor requests\n",
    "\n",
    "Important:\n",
    "- Login/password problems = TECHNICAL_SUPPORT (not ACCOUNT_MANAGEMENT)\n",
    "- Updating payment methods = ACCOUNT_MANAGEMENT (not BILLING)\n",
    "- Refund status = BILLING (not RETURNS)\n",
    "\n",
    "Respond with JSON:\n",
    "{\n",
    "    \\\"department\\\": \\\"DEPARTMENT_NAME\\\",\n",
    "    \\\"reasoning\\\": \\\"Your reasoning\\\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt 2 (improved) created with improvements!\")\n",
    "print(f\"\\nPrompt 1 length: {len(SYSTEM_PROMPT_1)} chars\")\n",
    "print(f\"Prompt 2 length: {len(SYSTEM_PROMPT_2)} chars\")\n",
    "print(f\"\\nAdded {len(SYSTEM_PROMPT_2) - len(SYSTEM_PROMPT_1)} chars of context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drUhV6W04peW",
    "outputId": "4c8c1009-339c-47bb-8cf0-3c085dbabb8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling tracing for project: V1_action_autonomy_prompt_2\n",
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: V1_action_autonomy_prompt_2\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n",
      "Tracing enabled for Prompt 2!\n"
     ]
    }
   ],
   "source": [
    "# Enable tracing for Prompt 2 (separate project)\n",
    "\n",
    "# Uninstrument previous tracer to avoid overwriting Prompt 1 traces\n",
    "OpenAIInstrumentor().uninstrument()\n",
    "\n",
    "project_name_p2 = \"V1_action_autonomy_prompt_2\"\n",
    "print(f\"Enabling tracing for project: {project_name_p2}\")\n",
    "\n",
    "tracer_provider_p2 = register(project_name=project_name_p2)\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider_p2)\n",
    "\n",
    "print(\"Tracing enabled for Prompt 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDp-khou4peX",
    "outputId": "154deee6-ca99-4b96-8aef-839a46d3f9f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prompt 2 evaluation on 30 test cases...\n",
      "\n",
      "[1/30] TC001: PASS\n",
      "[2/30] TC002: PASS\n",
      "[3/30] TC003: PASS\n",
      "[4/30] TC004: PASS\n",
      "[5/30] TC005: PASS\n",
      "[6/30] TC006: PASS\n",
      "[7/30] TC007: PASS\n",
      "[8/30] TC008: PASS\n",
      "[9/30] TC009: PASS\n",
      "[10/30] TC010: PASS\n",
      "[11/30] TC011: PASS\n",
      "[12/30] TC012: PASS\n",
      "[13/30] TC013: PASS\n",
      "[14/30] TC014: PASS\n",
      "[15/30] TC015: PASS\n",
      "[16/30] TC016: PASS\n",
      "[17/30] TC017: PASS\n",
      "[18/30] TC018: PASS\n",
      "[19/30] TC019: PASS\n",
      "[20/30] TC020: PASS\n",
      "[21/30] TC021: FAIL\n",
      "[22/30] TC022: FAIL\n",
      "[23/30] TC023: PASS\n",
      "[24/30] TC024: PASS\n",
      "[25/30] TC025: PASS\n",
      "[26/30] TC026: PASS\n",
      "[27/30] TC027: PASS\n",
      "[28/30] TC028: PASS\n",
      "[29/30] TC029: PASS\n",
      "[30/30] TC030: PASS\n",
      "\n",
      "Prompt 2 evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Run Prompt 2 evaluation\n",
    "agent_p2 = RouterAgent(system_prompt=SYSTEM_PROMPT_2)\n",
    "results_p2 = []\n",
    "\n",
    "print(\"Running Prompt 2 evaluation on 30 test cases...\\n\")\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    i = idx + 1\n",
    "    test_id = row['test_id']\n",
    "\n",
    "    with tracer.start_as_current_span(f\"test_case_{test_id}\") as span:\n",
    "        span.set_attribute(\"test.id\", test_id)\n",
    "        span.set_attribute(\"test.expected_department\", row['expected_department'])\n",
    "\n",
    "        decision = agent_p2.route(row['customer_message'])\n",
    "        correct = decision.department.name == row['expected_department']\n",
    "\n",
    "        span.set_attribute(\"result.correct\", correct)\n",
    "\n",
    "        if correct:\n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "        else:\n",
    "            span.set_status(Status(StatusCode.ERROR, \"Incorrect routing\"))\n",
    "            span.set_attribute(\"error.expected\", row['expected_department'])\n",
    "            span.set_attribute(\"error.got\", decision.department.name)\n",
    "\n",
    "        result = EvalResult(\n",
    "            test_id=test_id,\n",
    "            message=row['customer_message'],\n",
    "            expected=row['expected_department'],\n",
    "            predicted=decision.department.name,\n",
    "            correct=correct,\n",
    "            reasoning=decision.reasoning,\n",
    "            category=row['category']\n",
    "        )\n",
    "        results_p2.append(result)\n",
    "\n",
    "        status = \"PASS\" if correct else \"FAIL\"\n",
    "        print(f\"[{i}/30] {test_id}: {status}\")\n",
    "\n",
    "print(\"\\nPrompt 2 evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7rmZ4oU4peX",
    "outputId": "75759f05-6917-429b-9811-023be6c54950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROMPT 1 vs PROMPT 2 COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Overall Accuracy:\n",
      "  Prompt 1: 73.3% (True/30)\n",
      "  Prompt 2: 93.3% (28/30)\n",
      "  Improvement: +20.0%\n",
      "\n",
      "Fixed in Prompt 2 (6 cases):\n",
      "  [TC005] I can't log into my account...\n",
      "  [TC008] My refund still hasn't shown up it's been 2 weeks...\n",
      "  [TC018] I forgot my password and the reset email isn't com...\n",
      "  [TC024] I need to update my credit card on file...\n",
      "  [TC029] I reset my password but still can't access my acco...\n",
      "  [TC030] Why was I charged a restocking fee?...\n",
      "\n",
      "Still Failing (2 cases):\n",
      "  [TC021] Why didn't I get my loyalty points for this purcha...\n",
      "  [TC022] Your prices are way too high! This is ridiculous!...\n"
     ]
    }
   ],
   "source": [
    "# Compare V0 vs V1\n",
    "correct_v1 = sum(1 for r in results_p2 if r.correct)\n",
    "accuracy_v1 = correct_v1 / len(results_p2)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROMPT 1 vs PROMPT 2 COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOverall Accuracy:\")\n",
    "print(f\"  Prompt 1: {accuracy:.1%} ({correct}/{total})\")\n",
    "print(f\"  Prompt 2: {accuracy_v1:.1%} ({correct_v1}/{total})\")\n",
    "improvement = accuracy_v1 - accuracy\n",
    "print(f\"  Improvement: +{improvement:.1%}\")\n",
    "\n",
    "# Which errors got fixed?\n",
    "v0_errors = {r.test_id for r in results_p1 if not r.correct}\n",
    "v1_errors = {r.test_id for r in results_p2 if not r.correct}\n",
    "\n",
    "fixed = v0_errors - v1_errors\n",
    "still_failing = v0_errors & v1_errors\n",
    "\n",
    "if fixed:\n",
    "    print(f\"\\nFixed in Prompt 2 ({len(fixed)} cases):\")\n",
    "    for test_id in sorted(fixed):\n",
    "        r = next(r for r in results_p1 if r.test_id == test_id)\n",
    "        print(f\"  [{test_id}] {r.message[:50]}...\")\n",
    "\n",
    "if still_failing:\n",
    "    print(f\"\\nStill Failing ({len(still_failing)} cases):\")\n",
    "    for test_id in sorted(still_failing):\n",
    "        r = next(r for r in results_p2 if r.test_id == test_id)\n",
    "        print(f\"  [{test_id}] {r.message[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcUmt52X4peX"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A V1 Action Autonomy agent that:\n",
    "- Routes customer messages to departments\n",
    "- Achieves ~90% accuracy on diverse test cases\n",
    "- Provides reasoning for decisions\n",
    "- Falls back to escalation for edge cases\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Start Simple:** Action autonomy is perfect for classification tasks\n",
    "2. **Observability is Key:** Phoenix traces revealed failure patterns\n",
    "3. **Iterate Based on Data:** V0 ‚Üí V1 improvements were targeted\n",
    "4. **Clear Metrics Matter:** Routing accuracy was appropriate for this task\n",
    "\n",
    "### When to Use Prompt 2 (Action Autonomy)\n",
    "\n",
    "V1 is appropriate when:\n",
    "- Task is well-defined classification/routing\n",
    "- Success criteria is clear (correct category)\n",
    "- Human takes over after classification\n",
    "- No multi-step reasoning required\n",
    "\n",
    "### When V1 is NOT Enough\n",
    "\n",
    "V1 limitations:\n",
    "- Can't solve multi-step problems\n",
    "- Can't retrieve relevant documentation\n",
    "- Can't generate action plans\n",
    "- Can't handle context from multiple sources\n",
    "\n",
    "**That's where V2 comes in!**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the V2 notebook, we'll expand scope to **Planning Autonomy**:\n",
    "- Retrieve relevant SOPs using keyword search\n",
    "- Generate multi-step action plans\n",
    "- Evaluate with more complex metrics\n",
    "- Learn when to add vs avoid complexity\n",
    "\n",
    "**Key Philosophy:** V1 isn't \"bad\" - it's appropriately scoped. V2 expands scope deliberately with proper guardrails."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

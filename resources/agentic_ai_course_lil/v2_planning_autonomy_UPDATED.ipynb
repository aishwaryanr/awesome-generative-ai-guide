{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e243e6e",
   "metadata": {},
   "source": [
    "# V2: Planning Autonomy\n",
    "\n",
    "## From Single Actions to Multi-Step Plans\n",
    "\n",
    "In V1, we built an **action autonomy** agent that performed a single classification: routing customer messages to departments.\n",
    "\n",
    "Now we move up the autonomy ladder to **planning autonomy**: generating multi-step action plans by retrieving relevant procedures and reasoning over them.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **RAG Systems**: Use BM25 to retrieve relevant Standard Operating Procedures (SOPs)\n",
    "2. **Multi-Step Planning**: Generate detailed action plans instead of single actions\n",
    "3. **Custom Metrics**: Design evaluation metrics from observed failures\n",
    "4. **LLM-as-Judge**: Use GPT-4o to evaluate GPT-5 outputs\n",
    "5. **Trace-First Evaluation**: Observe â†’ Discover â†’ Measure â†’ Improve\n",
    "\n",
    "### The Incremental Building Story\n",
    "\n",
    "**V1 Achievement:**\n",
    "- Built routing from 73% â†’ 93% accuracy\n",
    "- Prompt 1 (baseline) â†’ Prompt 2 (improved with descriptions)\n",
    "\n",
    "**V2 Builds On V1:**\n",
    "- **KEEPS** V1's 93% routing (don't regress!)\n",
    "- **ADDS** BM25 retrieval to find relevant SOPs\n",
    "- **ADDS** multi-step plan generation\n",
    "\n",
    "**Key:** Each version builds on the previous one. We never start from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b510a7",
   "metadata": {},
   "source": [
    "## V2 Architecture\n",
    "\n",
    "Our V2 Planning Autonomy agent builds on V1's routing by adding retrieval and multi-step planning:\n",
    "\n",
    "![V2 Architecture](assets/diagrams/v2_architecture.png)\n",
    "\n",
    "![Data Flow Through System](assets/diagrams/v2_data_flow.png)\n",
    "\n",
    "**Key Points:**\n",
    "- **Green boxes** = V1 components (keep the 93% routing!)\n",
    "- **Orange boxes** = V2 new components (BM25 + Planning)\n",
    "- **Data flows** left-to-right: Message â†’ Routing â†’ Retrieval â†’ Planning â†’ Output\n",
    "\n",
    "**What's New in V2:**\n",
    "1. **BM25 Retriever**: Finds relevant SOPs using keyword matching\n",
    "2. **Plan Generator**: Creates multi-step plans using retrieved context\n",
    "3. **Custom Metrics**: SOP Recall + Plan Alignment (3-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf4799",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and set up environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c676349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q openai pandas python-dotenv rank-bm25\n",
    "!pip install -q 'arize-phoenix[evals]' openinference-instrumentation-openai\n",
    "\n",
    "print(\"Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Colab vs Local\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repository for data access\n",
    "    if not os.path.exists('awesome-generative-ai-guide'):\n",
    "        !git clone https://github.com/aishwaryanr/awesome-generative-ai-guide.git\n",
    "\n",
    "    # Navigate to notebooks directory\n",
    "    if os.path.exists('agentic-ai-course/notebooks'):\n",
    "        os.chdir('awesome-generative-ai-guide/resources/agentic_ai_course_lil')\n",
    "\n",
    "    # Get API key from Colab secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "    # Local environment - use .env file\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in Colab Secrets or .env file\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5954cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from rank_bm25 import BM25Okapi\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "# Arize Phoenix for observability\n",
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0ad91",
   "metadata": {},
   "source": [
    "## Load SOPs (Standard Operating Procedures)\n",
    "\n",
    "V2 uses a knowledge base of 9 SOPs covering different customer support scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756eaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sops(sops_directory=\"data/sops\"):\n",
    "    \"\"\"Load all SOP text files.\"\"\"\n",
    "    sops = {}\n",
    "    sop_files = glob.glob(f\"{sops_directory}/sop_*.txt\")\n",
    "\n",
    "    for filepath in sorted(sop_files):\n",
    "        filename = os.path.basename(filepath)\n",
    "        sop_id = filename.replace('.txt', '').upper()\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sops[sop_id] = {\n",
    "            'filename': filename,\n",
    "            'content': content,\n",
    "            'word_count': len(content.split())\n",
    "        }\n",
    "\n",
    "    return sops\n",
    "\n",
    "# Load SOPs\n",
    "sops_db = load_sops()\n",
    "print(f\"Loaded {len(sops_db)} SOPs\")\n",
    "print(f\"\\nSOP IDs: {list(sops_db.keys())}\")\n",
    "print(f\"\\nExample SOP (first 200 chars):\")\n",
    "first_sop = list(sops_db.keys())[0]\n",
    "print(f\"{first_sop}: {sops_db[first_sop]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d48d7",
   "metadata": {},
   "source": [
    "## Build BM25 Index\n",
    "\n",
    "BM25 is a keyword-based retrieval algorithm. We'll use it to find relevant SOPs given a customer message.\n",
    "\n",
    "![BM25 SOP Retrieval](assets/diagrams/v2_sop_retrieval.png)\n",
    "\n",
    "**How it works:**\n",
    "1. Combine message + department as query\n",
    "2. Score all 9 SOPs using BM25\n",
    "3. Return top K SOPs (K=2 for Prompt 1, K=4 for Prompt 2)\n",
    "\n",
    "**Key insight:** K=2 may miss relevant SOPs ranked #3-4, K=4 captures them â†’ better recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bm25_index(sops_db):\n",
    "    \"\"\"Build BM25 index over SOPs.\"\"\"\n",
    "    sop_ids = list(sops_db.keys())\n",
    "    sop_contents = [sops_db[sop_id]['content'] for sop_id in sop_ids]\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_corpus = [doc.lower().split() for doc in sop_contents]\n",
    "\n",
    "    # Build BM25\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    return bm25, sop_ids\n",
    "\n",
    "bm25_index, sop_ids = build_bm25_index(sops_db)\n",
    "print(f\"âœ“ BM25 index built over {len(sop_ids)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30f4ac",
   "metadata": {},
   "source": [
    "## Planning Agent - Prompt 1 (Baseline)\n",
    "\n",
    "**Configuration:**\n",
    "- **Routing**: V1's improved Prompt 2 (93% accuracy) - EXACT copy\n",
    "- **Retrieval**: K=2 (retrieve top 2 SOPs)\n",
    "- **Planning**: gpt-4o\n",
    "\n",
    "**Key: We use V1's EXACT department names and routing prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanningAgent:\n",
    "    def __init__(self, client, bm25_index, sop_ids, sops_db):\n",
    "        self.client = client\n",
    "        self.bm25_index = bm25_index\n",
    "        self.sop_ids = sop_ids\n",
    "        self.sops_db = sops_db\n",
    "\n",
    "        # Use EXACT same department names as V1's enum\n",
    "        self.departments = [\n",
    "            \"BILLING\",\n",
    "            \"RETURNS\",\n",
    "            \"TECHNICAL_SUPPORT\",\n",
    "            \"ORDER_STATUS\",\n",
    "            \"PRODUCT_INQUIRY\",\n",
    "            \"ACCOUNT_MANAGEMENT\",\n",
    "            \"ESCALATION\"\n",
    "        ]\n",
    "\n",
    "    def route_message(self, message):\n",
    "        \"\"\"\n",
    "        Route to department using V1's improved Prompt 2 (EXACT).\n",
    "\n",
    "        This builds on V1 Action Autonomy's best routing prompt (93% accuracy).\n",
    "        V2 adds planning on top of this solid routing foundation.\n",
    "        Uses EXACT department names from V1's enum.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Route customer messages to departments.\n",
    "\n",
    "Available departments:\n",
    "- BILLING: Payment issues, charges, refunds, refund status, account balances, fees\n",
    "- RETURNS: Return requests, exchanges, return status, return policies\n",
    "- TECHNICAL_SUPPORT: Login problems, password reset issues, website errors, checkout failures\n",
    "- ORDER_STATUS: Order tracking, shipping updates, delivery questions, missing items\n",
    "- PRODUCT_INQUIRY: Product questions, specifications, availability, pricing\n",
    "- ACCOUNT_MANAGEMENT: Profile updates, changing saved payment methods, preferences, address changes\n",
    "- ESCALATION: Very upset customers demanding managers, supervisor requests\n",
    "\n",
    "Important:\n",
    "- Login/password problems = TECHNICAL_SUPPORT (not ACCOUNT_MANAGEMENT)\n",
    "- Updating payment methods = ACCOUNT_MANAGEMENT (not BILLING)\n",
    "- Refund status = BILLING (not RETURNS)\n",
    "\n",
    "Message: \\\"{message}\\\"\n",
    "\n",
    "Respond with ONLY the department name, nothing else.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def retrieve_sops(self, message, department, top_k=2):\n",
    "        \"\"\"Retrieve relevant SOPs using BM25.\"\"\"\n",
    "        query = f\"{message} {department}\"\n",
    "        tokenized_query = query.lower().split()\n",
    "\n",
    "        scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "\n",
    "        retrieved_sops = []\n",
    "        for idx in top_indices:\n",
    "            sop_id = self.sop_ids[idx]\n",
    "            score = scores[idx]\n",
    "            content = self.sops_db[sop_id]['content']\n",
    "\n",
    "            # Use first 1500 words\n",
    "            words = content.split()[:1500]\n",
    "            excerpt = ' '.join(words)\n",
    "\n",
    "            retrieved_sops.append({\n",
    "                'sop_id': sop_id,\n",
    "                'score': score,\n",
    "                'excerpt': excerpt,\n",
    "                'full_content': content\n",
    "            })\n",
    "\n",
    "        return retrieved_sops\n",
    "\n",
    "    def generate_plan(self, message, department, retrieved_sops):\n",
    "        \"\"\"Generate action plan.\"\"\"\n",
    "        sops_context = \"\\n\\n\".join([\n",
    "            f\"--- {sop['sop_id']} (Relevance: {sop['score']:.2f}) ---\\n{sop['excerpt'][:2000]}...\"\n",
    "            for sop in retrieved_sops\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"You are a customer support agent planning assistant. Create a detailed, step-by-step action plan.\n",
    "\n",
    "**Customer Message:**\n",
    "\"{message}\"\n",
    "\n",
    "**Department:** {department}\n",
    "\n",
    "**Relevant Procedures (SOPs):**\n",
    "{sops_context}\n",
    "\n",
    "**Instructions:**\n",
    "Create a detailed action plan that:\n",
    "1. Lists specific steps the agent should take (in order)\n",
    "2. References relevant SOP procedures\n",
    "3. Includes verification or security steps\n",
    "4. Mentions escalation criteria if applicable\n",
    "5. Provides timeline expectations\n",
    "6. Notes any edge cases or system limitations\n",
    "\n",
    "Format as a numbered action plan. Be specific and actionable.\n",
    "\n",
    "**Action Plan:**\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def plan(self, message):\n",
    "        \"\"\"Full pipeline.\"\"\"\n",
    "        department = self.route_message(message)\n",
    "        retrieved_sops = self.retrieve_sops(message, department, top_k=2)\n",
    "        plan = self.generate_plan(message, department, retrieved_sops)\n",
    "\n",
    "        return {\n",
    "            'message': message,\n",
    "            'department': department,\n",
    "            'retrieved_sops': [\n",
    "                {'sop_id': sop['sop_id'], 'score': sop['score']}\n",
    "                for sop in retrieved_sops\n",
    "            ],\n",
    "            'plan': plan\n",
    "        }\n",
    "\n",
    "# Initialize agent\n",
    "agent = PlanningAgent(client, bm25_index, sop_ids, sops_db)\n",
    "print(\"âœ“ PlanningAgent initialized with V1's routing + BM25 + gpt-4o planning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41db90",
   "metadata": {},
   "source": [
    "## Demo: Generate a Plan\n",
    "\n",
    "Let's see the agent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a66cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_message = \"I bought a jacket last month, but it's too big. Can I return it?\"\n",
    "\n",
    "result = agent.plan(test_message)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PLANNING AGENT DEMO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCustomer Message: {result['message']}\")\n",
    "print(f\"\\nRouted Department: {result['department']}\")\n",
    "print(f\"\\nRetrieved SOPs:\")\n",
    "for sop in result['retrieved_sops']:\n",
    "    print(f\"  - {sop['sop_id']} (score: {sop['score']:.2f})\")\n",
    "print(f\"\\nGenerated Action Plan:\")\n",
    "print(result['plan'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92851e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¬ End of Chapter 1: Implementing Retrieval & Planning\n",
    "\n",
    "**Next: Chapter 2 - Continuous Calibration (CC)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f878d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Continuous Calibration (CC) Phase\n",
    "\n",
    "**Goal:** Observe failures, design custom metrics, and identify improvements.\n",
    "\n",
    "**In this phase:**\n",
    "- Enable Phoenix tracing to observe all LLM calls\n",
    "- Run systematic evaluation on test cases\n",
    "- Analyze errors in Phoenix UI\n",
    "- Design metrics from observed patterns (SOP Recall, Plan Alignment)\n",
    "- Compute metrics to quantify performance\n",
    "\n",
    "**Output:** Custom metrics that measure what matters + clear improvement targets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c87592",
   "metadata": {},
   "source": [
    "## Enable Phoenix Tracing\n",
    "\n",
    "Phoenix captures all LLM calls so we can observe what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Phoenix (Colab-compatible setup)\n",
    "import os\n",
    "\n",
    "# Configure Phoenix for Colab/local compatibility\n",
    "os.environ[\"PHOENIX_HOST\"] = \"0.0.0.0\"\n",
    "os.environ[\"PHOENIX_PORT\"] = \"6006\"\n",
    "\n",
    "import phoenix as px\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Starting Arize Phoenix...\")\n",
    "print(\"=\"*80)\n",
    "session = px.launch_app()  # don't pass port parameter\n",
    "print(\"Phoenix session url:\", session.url)\n",
    "\n",
    "# For Google Colab compatibility\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_window(6006)\n",
    "    print(\"âœ“ Phoenix running on Colab at port 6006\")\n",
    "except ImportError:\n",
    "    print(\"âœ“ Phoenix running locally at http://localhost:6006\")\n",
    "\n",
    "print(\"Open the URL above to view traces in real-time\\n\")\n",
    "\n",
    "# Enable OpenAI instrumentation for Prompt 1\n",
    "project_name = \"V2_planning_autonomy_prompt_1\"\n",
    "print(f\"Enabling tracing for project: {project_name}\")\n",
    "tracer_provider = register(project_name=project_name)\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "print(\"âœ“ Tracing enabled! All API calls will be captured in Phoenix.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be0e57",
   "metadata": {},
   "source": [
    "## Load Test Cases\n",
    "\n",
    "We have 22 grounded test cases with expected SOPs and procedure steps.\n",
    "\n",
    "**Each test case includes:**\n",
    "- Customer message\n",
    "- Complexity level (simple, medium, complex)\n",
    "- Expected SOPs (ground truth)\n",
    "- Expected procedure steps\n",
    "- Policy details to mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d58285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases\n",
    "test_cases = pd.read_csv('data/v2_test_cases.csv')\n",
    "print(f\"Loaded {len(test_cases)} test cases\")\n",
    "print(f\"\\nColumns: {list(test_cases.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(test_cases[['message', 'complexity', 'expected_sops']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006c8eb",
   "metadata": {},
   "source": [
    "## Run Prompt 1 Evaluation\n",
    "\n",
    "Let's evaluate the baseline and observe failures in Phoenix.\n",
    "\n",
    "**Note:** This will make ~66 OpenAI API calls (22 test cases Ã— 3 calls each):\n",
    "- 1 call for routing\n",
    "- 1 call for plan generation\n",
    "- Takes ~5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82121bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sop_name(sop):\n",
    "    \"\"\"Normalize SOP name to base format (e.g., SOP_001).\"\"\"\n",
    "    import re\n",
    "    sop = str(sop).upper()\n",
    "    sop = sop.replace('SOP-', 'SOP_').replace(' ', '_')\n",
    "    match = re.match(r'(SOP_\\d+)', sop)\n",
    "    return match.group(1) if match else sop\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "\n",
    "print(\"Running Prompt 1 evaluation...\")\n",
    "print(\"(Each plan generation is being traced in Phoenix)\\n\")\n",
    "\n",
    "for idx, row in test_cases.iterrows():\n",
    "    message = row['message']\n",
    "    expected_sops = row['expected_sops'].split(',') if pd.notna(row['expected_sops']) else []\n",
    "    expected_sops = [normalize_sop_name(s.strip()) for s in expected_sops]\n",
    "\n",
    "    print(f\"  [{idx+1}/{len(test_cases)}] Processing: {message[:60]}...\")\n",
    "\n",
    "    # Create Phoenix span for this test case\n",
    "    with tracer.start_as_current_span(f\"test_case_{idx}\") as span:\n",
    "        span.set_attribute(\"test.id\", idx)\n",
    "        span.set_attribute(\"test.message\", message)\n",
    "        span.set_attribute(\"test.complexity\", row['complexity'])\n",
    "        span.set_attribute(\"test.expected_sops\", str(expected_sops))\n",
    "\n",
    "        try:\n",
    "            result = agent.plan(message)\n",
    "\n",
    "            retrieved_sop_ids = [normalize_sop_name(sop['sop_id']) for sop in result['retrieved_sops']]\n",
    "\n",
    "            # Record in span\n",
    "            span.set_attribute(\"result.department\", result['department'])\n",
    "            span.set_attribute(\"result.retrieved_sops\", str(retrieved_sop_ids))\n",
    "            span.set_attribute(\"result.plan_length\", len(result['plan'].split()))\n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "            results.append({\n",
    "                'test_case_id': idx,\n",
    "                'message': message,\n",
    "                'complexity': row['complexity'],\n",
    "                'expected_sops': expected_sops,\n",
    "                'retrieved_sops': retrieved_sop_ids,\n",
    "                'department': result['department'],\n",
    "                'plan': result['plan']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR: {e}\")\n",
    "            span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nâœ“ Completed {len(results_df)} evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28bc25",
   "metadata": {},
   "source": [
    "## Observe Traces in Phoenix\n",
    "\n",
    "### The Trace-First Evaluation Workflow\n",
    "\n",
    "**Key workflow:** Observe â†’ Discover â†’ Measure â†’ Improve\n",
    "\n",
    "**Go to Phoenix UI:** http://localhost:6006/\n",
    "\n",
    "**What to observe:**\n",
    "1. Click on \"V2_planning_autonomy_prompt_1\" project\n",
    "2. See all test case traces\n",
    "3. Click on individual traces to see:\n",
    "   - Routing call (V1's prompt)\n",
    "   - Plan generation call (with SOPs)\n",
    "   - Retrieved SOPs vs Expected SOPs\n",
    "4. **Look for patterns:**\n",
    "   - Missing expected SOPs (K=2 limitation?)\n",
    "   - Plans missing critical steps\n",
    "   - Wrong SOPs retrieved\n",
    "\n",
    "**Exercise:** Find 3-5 failed cases and note what went wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a478fe",
   "metadata": {},
   "source": [
    "## Analyze Errors\n",
    "\n",
    "From observations, design metrics to measure failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23735b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple error analysis\n",
    "print(\"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "errors = []\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    expected_sops = set(row['expected_sops']) if isinstance(row['expected_sops'], list) else set()\n",
    "    retrieved_sops = set(row['retrieved_sops']) if isinstance(row['retrieved_sops'], list) else set()\n",
    "\n",
    "    # Missing expected SOPs\n",
    "    missing_sops = expected_sops - retrieved_sops\n",
    "    if missing_sops:\n",
    "        errors.append({\n",
    "            'test_case_id': idx,\n",
    "            'error_type': 'missing_sops',\n",
    "            'message': row['message'][:80],\n",
    "            'expected_sops': list(expected_sops),\n",
    "            'retrieved_sops': list(retrieved_sops),\n",
    "            'missing_sops': list(missing_sops)\n",
    "        })\n",
    "\n",
    "    # Extra/wrong SOPs\n",
    "    extra_sops = retrieved_sops - expected_sops\n",
    "    if extra_sops:\n",
    "        errors.append({\n",
    "            'test_case_id': idx,\n",
    "            'error_type': 'extra_sops',\n",
    "            'message': row['message'][:80],\n",
    "            'expected_sops': list(expected_sops),\n",
    "            'retrieved_sops': list(retrieved_sops),\n",
    "            'extra_sops': list(extra_sops)\n",
    "        })\n",
    "\n",
    "print(f\"\\nFound {len(errors)} error instances across {len(set(e['test_case_id'] for e in errors))} test cases\")\n",
    "\n",
    "missing_sops_errors = [e for e in errors if e['error_type'] == 'missing_sops']\n",
    "extra_sops_errors = [e for e in errors if e['error_type'] == 'extra_sops']\n",
    "\n",
    "print(f\"\\nMissing SOPs: {len(missing_sops_errors)} cases\")\n",
    "print(f\"Extra/Wrong SOPs: {len(extra_sops_errors)} cases\")\n",
    "\n",
    "if missing_sops_errors:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EXAMPLE: Missing Expected SOPs\")\n",
    "    print(\"-\"*80)\n",
    "    for i, error in enumerate(missing_sops_errors[:3]):\n",
    "        print(f\"\\nCase {i+1}:\")\n",
    "        print(f\"  Message: {error['message']}\")\n",
    "        print(f\"  Expected SOPs: {error['expected_sops']}\")\n",
    "        print(f\"  Retrieved SOPs: {error['retrieved_sops']}\")\n",
    "        print(f\"  Missing: {error['missing_sops']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0b161",
   "metadata": {},
   "source": [
    "## Design 2 Custom Metrics\n",
    "\n",
    "Based on observed failures, we design 2 metrics:\n",
    "\n",
    "### Metric 1: SOP Retrieval Recall @ K\n",
    "- **What:** % of expected SOPs actually retrieved\n",
    "- **Why:** Wrong SOPs â†’ wrong plan (garbage in, garbage out)\n",
    "- **Observed:** K=2 misses relevant SOPs ranked #3+\n",
    "- **Formula:** `recall = len(retrieved âˆ© expected) / len(expected)`\n",
    "\n",
    "### Metric 2: Plan-to-Steps Alignment (3-class)\n",
    "- **What:** Does plan cover expected procedure steps?\n",
    "- **Classes:** good (complete), partial (minor gaps), bad (major gaps)\n",
    "- **Why:** End-to-end quality check\n",
    "- **Observed:** Plans missing critical steps or policy details\n",
    "- **Judge:** GPT-4o evaluates with reasoning\n",
    "\n",
    "**Key:** Metrics emerged from observations, not predetermined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bd8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sop_recall(expected_sops, retrieved_sops):\n",
    "    \"\"\"What % of expected SOPs were retrieved?\"\"\"\n",
    "    if not expected_sops or len(expected_sops) == 0:\n",
    "        return 1.0\n",
    "\n",
    "    expected_set = set([normalize_sop_name(s) for s in expected_sops])\n",
    "    retrieved_set = set([normalize_sop_name(s) for s in retrieved_sops])\n",
    "\n",
    "    relevant_retrieved = expected_set & retrieved_set\n",
    "    recall = len(relevant_retrieved) / len(expected_set)\n",
    "\n",
    "    return recall\n",
    "\n",
    "def compute_plan_alignment(message, expected_steps, policy_details, generated_plan):\n",
    "    \"\"\"Does plan cover expected steps? (LLM-as-Judge with 3 classes)\"\"\"\n",
    "    judge_prompt = f\"\"\"You are evaluating if a customer support action plan adequately covers expected procedure steps.\n",
    "\n",
    "**Customer Message:**\n",
    "{message}\n",
    "\n",
    "**Expected Procedure Steps (from SOP):**\n",
    "{expected_steps}\n",
    "\n",
    "**Expected Policy Details:**\n",
    "{policy_details}\n",
    "\n",
    "**Generated Action Plan:**\n",
    "{generated_plan}\n",
    "\n",
    "**Evaluation Task:**\n",
    "Classify the plan quality into one of 3 classes:\n",
    "\n",
    "- **good**: All critical steps are covered, policy details are mentioned, plan is complete and actionable\n",
    "  Example: Plan includes all verification steps, mentions specific timelines, covers edge cases\n",
    "\n",
    "- **partial**: Most important steps are covered but missing some details or minor steps\n",
    "  Example: Plan has main actions but omits policy details like timelines or approval levels\n",
    "\n",
    "- **bad**: Plan is missing critical steps, has significant gaps, or is unrelated to the expected procedure\n",
    "  Example: Plan addresses wrong issue, skips mandatory verification steps, or completely misses the procedure\n",
    "\n",
    "Respond in this EXACT format:\n",
    "CLASS: <good|partial|bad>\n",
    "REASONING: <2-3 sentence explanation of what's covered and what's missing>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Parse class\n",
    "        class_line = [line for line in content.split('\\n') if line.startswith('CLASS:')]\n",
    "        reasoning_line = [line for line in content.split('\\n') if line.startswith('REASONING:')]\n",
    "\n",
    "        if class_line:\n",
    "            class_text = class_line[0].replace('CLASS:', '').strip().lower()\n",
    "            plan_class = class_text if class_text in ['good', 'partial', 'bad'] else 'partial'\n",
    "        else:\n",
    "            plan_class = 'partial'\n",
    "\n",
    "        if reasoning_line:\n",
    "            reasoning = reasoning_line[0].replace('REASONING:', '').strip()\n",
    "        else:\n",
    "            reasoning = content\n",
    "\n",
    "        return {\n",
    "            'class': plan_class,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  LLM judge error: {e}\")\n",
    "        return {'class': 'partial', 'reasoning': str(e)}\n",
    "\n",
    "print(\"âœ“ Metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49de9c7",
   "metadata": {},
   "source": [
    "## Compute Metrics for Prompt 1\n",
    "\n",
    "**Note:** This will make 22 more API calls (one per test case for LLM-as-Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = []\n",
    "\n",
    "print(\"Computing metrics for Prompt 1...\\n\")\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    # Get expected data from test cases\n",
    "    test_row = test_cases.iloc[idx]\n",
    "    expected_steps = test_row.get('expected_steps', '') if pd.notna(test_row.get('expected_steps')) else ''\n",
    "    policy_details = test_row.get('policy_details', '') if pd.notna(test_row.get('policy_details')) else ''\n",
    "\n",
    "    message = row['message']\n",
    "    expected_sops = row['expected_sops'] if isinstance(row['expected_sops'], list) else []\n",
    "    retrieved_sops = row['retrieved_sops'] if isinstance(row['retrieved_sops'], list) else []\n",
    "    plan = row['plan']\n",
    "\n",
    "    print(f\"[{idx+1}/{len(results_df)}] Evaluating: {message[:60]}...\")\n",
    "\n",
    "    # Metric 1: SOP Recall\n",
    "    recall = compute_sop_recall(expected_sops, retrieved_sops)\n",
    "    print(f\"  SOP Recall: {recall:.2f}\")\n",
    "\n",
    "    # Metric 2: Plan Alignment\n",
    "    alignment = compute_plan_alignment(message, expected_steps, policy_details, plan)\n",
    "    print(f\"  Plan Alignment: {alignment['class']}\")\n",
    "\n",
    "    metrics.append({\n",
    "        'test_case_id': idx,\n",
    "        'sop_recall': recall,\n",
    "        'plan_alignment_class': alignment['class'],\n",
    "        'plan_alignment_reasoning': alignment['reasoning']\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(f\"\\nâœ“ Metrics computed for {len(metrics_df)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3934d",
   "metadata": {},
   "source": [
    "## Summarize Prompt 1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb05c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 1 METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Metric 1: SOP Recall\n",
    "print(\"\\n1. SOP RETRIEVAL RECALL @ K\")\n",
    "print(\"-\" * 40)\n",
    "recall_mean = metrics_df['sop_recall'].mean()\n",
    "recall_perfect = (metrics_df['sop_recall'] == 1.0).sum()\n",
    "recall_zero = (metrics_df['sop_recall'] == 0.0).sum()\n",
    "print(f\"  Mean Recall: {recall_mean:.2%}\")\n",
    "print(f\"  Perfect (1.0): {recall_perfect}/{len(metrics_df)} cases\")\n",
    "print(f\"  Zero (0.0): {recall_zero}/{len(metrics_df)} cases\")\n",
    "print(f\"  â†’ Interpretation: On average, we retrieve {recall_mean:.0%} of expected SOPs\")\n",
    "\n",
    "# Metric 2: Plan Alignment\n",
    "print(\"\\n2. PLAN-TO-STEPS ALIGNMENT (3-class)\")\n",
    "print(\"-\" * 40)\n",
    "alignment_good = (metrics_df['plan_alignment_class'] == 'good').sum()\n",
    "alignment_partial = (metrics_df['plan_alignment_class'] == 'partial').sum()\n",
    "alignment_bad = (metrics_df['plan_alignment_class'] == 'bad').sum()\n",
    "\n",
    "print(f\"  Good: {alignment_good}/{len(metrics_df)} cases ({alignment_good/len(metrics_df):.1%})\")\n",
    "print(f\"  Partial: {alignment_partial}/{len(metrics_df)} cases ({alignment_partial/len(metrics_df):.1%})\")\n",
    "print(f\"  Bad: {alignment_bad}/{len(metrics_df)} cases ({alignment_bad/len(metrics_df):.1%})\")\n",
    "print(f\"  â†’ Interpretation: {alignment_good} plans are complete, {alignment_partial} need minor fixes, {alignment_bad} have major gaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb30e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¬ End of Chapter 2: Error Analysis & Metric Design (CC)\n",
    "\n",
    "**Next: Chapter 3 - Continuous Deployment (CD)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545b9a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸš€ Continuous Deployment (CD) Phase\n",
    "\n",
    "**Goal:** Make targeted improvements and measure impact.\n",
    "\n",
    "**In this phase:**\n",
    "- Identify root causes from CC metrics\n",
    "- Design Prompt 2 with targeted fixes (K=2â†’4, gpt-4oâ†’gpt-5)\n",
    "- Re-evaluate with same metrics\n",
    "- Compare Prompt 1 vs Prompt 2 performance\n",
    "- Validate improvements worked\n",
    "\n",
    "**Output:** Better system with measured improvements (SOP Recall: 54%â†’76%, Plan Alignment: 72%â†’100%).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371835b",
   "metadata": {},
   "source": [
    "## Identify Problems â†’ Design Improvements\n",
    "\n",
    "Based on metrics, what should we improve?\n",
    "\n",
    "**Problem 1: Low SOP Recall (53.79%)**\n",
    "- Root cause: K=2 is too restrictive\n",
    "- Many relevant SOPs ranked #3-4 but not retrieved\n",
    "- **Solution:** Increase K from 2 to 4\n",
    "\n",
    "**Problem 2: Plan Alignment not perfect (72% good)**\n",
    "- Root cause: gpt-4o has limitations\n",
    "- Some plans missing steps or policy details\n",
    "- **Solution:** Upgrade to gpt-5 (better reasoning)\n",
    "\n",
    "**Prompt 2 Improvements:**\n",
    "1. K=2 â†’ K=4 (targets SOP Recall)\n",
    "2. gpt-4o â†’ gpt-5 (targets Plan Alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a6095",
   "metadata": {},
   "source": [
    "## Prompt 2: Improved Agent\n",
    "\n",
    "Same architecture, but with targeted improvements.\n",
    "\n",
    "**Changes:**\n",
    "- âœ… K=2 â†’ K=4 (better SOP retrieval)\n",
    "- âœ… gpt-4o â†’ gpt-5 (better plan generation)\n",
    "- âœ… Same V1 routing (keep what works!)\n",
    "\n",
    "**Goal:** Improve both SOP Recall and Plan Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48120be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanningAgentPrompt2:\n",
    "    def __init__(self, client, bm25_index, sop_ids, sops_db):\n",
    "        self.client = client\n",
    "        self.bm25_index = bm25_index\n",
    "        self.sop_ids = sop_ids\n",
    "        self.sops_db = sops_db\n",
    "\n",
    "        # Use EXACT same department names as V1\n",
    "        self.departments = [\n",
    "            \"BILLING\",\n",
    "            \"RETURNS\",\n",
    "            \"TECHNICAL_SUPPORT\",\n",
    "            \"ORDER_STATUS\",\n",
    "            \"PRODUCT_INQUIRY\",\n",
    "            \"ACCOUNT_MANAGEMENT\",\n",
    "            \"ESCALATION\"\n",
    "        ]\n",
    "\n",
    "    def route_message(self, message):\n",
    "        \"\"\"Same V1 routing - don't regress!\"\"\"\n",
    "        prompt = f\"\"\"Route customer messages to departments.\n",
    "\n",
    "Available departments:\n",
    "- BILLING: Payment issues, charges, refunds, refund status, account balances, fees\n",
    "- RETURNS: Return requests, exchanges, return status, return policies\n",
    "- TECHNICAL_SUPPORT: Login problems, password reset issues, website errors, checkout failures\n",
    "- ORDER_STATUS: Order tracking, shipping updates, delivery questions, missing items\n",
    "- PRODUCT_INQUIRY: Product questions, specifications, availability, pricing\n",
    "- ACCOUNT_MANAGEMENT: Profile updates, changing saved payment methods, preferences, address changes\n",
    "- ESCALATION: Very upset customers demanding managers, supervisor requests\n",
    "\n",
    "Important:\n",
    "- Login/password problems = TECHNICAL_SUPPORT (not ACCOUNT_MANAGEMENT)\n",
    "- Updating payment methods = ACCOUNT_MANAGEMENT (not BILLING)\n",
    "- Refund status = BILLING (not RETURNS)\n",
    "\n",
    "Message: \\\"{message}\\\"\n",
    "\n",
    "Respond with ONLY the department name, nothing else.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def retrieve_sops(self, message, department, top_k=4):\n",
    "        \"\"\"\n",
    "        PROMPT 2 IMPROVEMENT: Increased top_k from 2 to 4\n",
    "        Rationale: Prompt 1 had low recall, missing SOPs ranked #3-4\n",
    "        \"\"\"\n",
    "        query = f\"{message} {department}\"\n",
    "        tokenized_query = query.lower().split()\n",
    "\n",
    "        scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "\n",
    "        retrieved_sops = []\n",
    "        for idx in top_indices:\n",
    "            sop_id = self.sop_ids[idx]\n",
    "            score = scores[idx]\n",
    "            content = self.sops_db[sop_id]['content']\n",
    "\n",
    "            words = content.split()[:1500]\n",
    "            excerpt = ' '.join(words)\n",
    "\n",
    "            retrieved_sops.append({\n",
    "                'sop_id': sop_id,\n",
    "                'score': score,\n",
    "                'excerpt': excerpt,\n",
    "                'full_content': content\n",
    "            })\n",
    "\n",
    "        return retrieved_sops\n",
    "\n",
    "    def generate_plan(self, message, department, retrieved_sops):\n",
    "        \"\"\"\n",
    "        PROMPT 2 IMPROVEMENT: Upgraded from gpt-4o to gpt-5\n",
    "        Rationale: gpt-5 has better reasoning, should improve plan quality\n",
    "        \"\"\"\n",
    "        sops_context = \"\\n\\n\".join([\n",
    "            f\"--- {sop['sop_id']} (Relevance: {sop['score']:.2f}) ---\\n{sop['excerpt'][:2000]}...\"\n",
    "            for sop in retrieved_sops\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"You are a customer support agent planning assistant. Create a detailed, step-by-step action plan.\n",
    "\n",
    "**Customer Message:**\n",
    "\"{message}\"\n",
    "\n",
    "**Department:** {department}\n",
    "\n",
    "**Relevant Procedures (SOPs):**\n",
    "{sops_context}\n",
    "\n",
    "**Instructions:**\n",
    "Create a detailed action plan that:\n",
    "1. Lists specific steps the agent should take (in order)\n",
    "2. References relevant SOP procedures\n",
    "3. Includes verification or security steps\n",
    "4. Mentions escalation criteria if applicable\n",
    "5. Provides timeline expectations\n",
    "6. Notes any edge cases or system limitations\n",
    "\n",
    "Format as a numbered action plan. Be specific and actionable.\n",
    "\n",
    "**Action Plan:**\"\"\"\n",
    "\n",
    "        # PROMPT 2: Use gpt-5 instead of gpt-4o\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def plan(self, message):\n",
    "        \"\"\"Full pipeline with K=4 and gpt-5.\"\"\"\n",
    "        department = self.route_message(message)\n",
    "        retrieved_sops = self.retrieve_sops(message, department, top_k=4)\n",
    "        plan = self.generate_plan(message, department, retrieved_sops)\n",
    "\n",
    "        return {\n",
    "            'message': message,\n",
    "            'department': department,\n",
    "            'retrieved_sops': [\n",
    "                {'sop_id': sop['sop_id'], 'score': sop['score']}\n",
    "                for sop in retrieved_sops\n",
    "            ],\n",
    "            'plan': plan\n",
    "        }\n",
    "\n",
    "# Initialize improved agent\n",
    "agent_p2 = PlanningAgentPrompt2(client, bm25_index, sop_ids, sops_db)\n",
    "print(\"âœ“ PlanningAgentPrompt2 initialized (K=4, gpt-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd62087",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What We Built:**\n",
    "- V2 Planning Agent that generates multi-step plans\n",
    "- Builds on V1's 93% routing (EXACT department names)\n",
    "- Uses BM25 to retrieve relevant SOPs\n",
    "- Uses LLM to generate detailed action plans\n",
    "\n",
    "**What We Learned:**\n",
    "1. **Incremental Building:** V2 = V1's routing + new capabilities\n",
    "2. **Trace-First:** Observe failures â†’ Design metrics â†’ Improve\n",
    "3. **Custom Metrics:** SOP Recall + Plan Alignment (3-class)\n",
    "4. **Targeted Improvements:** K=2â†’4, gpt-4oâ†’gpt-5\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run Prompt 2 evaluation\n",
    "2. Compute Prompt 2 metrics\n",
    "3. Compare Prompt 1 vs Prompt 2\n",
    "4. Verify improvements worked!\n",
    "\n",
    "**V2 Planning Autonomy: Complete!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

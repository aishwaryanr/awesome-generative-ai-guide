{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e243e6e",
   "metadata": {},
   "source": [
    "# V2: Planning Autonomy\n",
    "\n",
    "## From Single Actions to Multi-Step Plans\n",
    "\n",
    "In V1, we built an **action autonomy** agent that performed a single classification: routing customer messages to departments.\n",
    "\n",
    "Now we move up the autonomy ladder to **planning autonomy**: generating multi-step action plans by retrieving relevant procedures and reasoning over them.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **RAG Systems**: Use BM25 to retrieve relevant Standard Operating Procedures (SOPs)\n",
    "2. **Multi-Step Planning**: Generate detailed action plans instead of single actions\n",
    "3. **Custom Metrics**: Design evaluation metrics from observed failures\n",
    "4. **LLM-as-Judge**: Use GPT-4o to evaluate GPT-5 outputs\n",
    "5. **Trace-First Evaluation**: Observe â†’ Discover â†’ Measure â†’ Improve\n",
    "\n",
    "### The Incremental Building Story\n",
    "\n",
    "**V1 Achievement:**\n",
    "- Built routing from 73% â†’ 93% accuracy\n",
    "- Prompt 1 (baseline) â†’ Prompt 2 (improved with descriptions)\n",
    "\n",
    "**V2 Builds On V1:**\n",
    "- **KEEPS** V1's 93% routing (don't regress!)\n",
    "- **ADDS** BM25 retrieval to find relevant SOPs\n",
    "- **ADDS** multi-step plan generation\n",
    "\n",
    "**Key:** Each version builds on the previous one. We never start from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b510a7",
   "metadata": {},
   "source": [
    "## V2 Architecture\n",
    "\n",
    "Our V2 Planning Autonomy agent builds on V1's routing by adding retrieval and multi-step planning:\n",
    "\n",
    "<img src=\"assets/diagrams/v2_architecture.png\" alt=\"V2 Architecture\" width=\"400\"/>\n",
    "\n",
    "<img src=\"assets/diagrams/v2_data_flow.png\" alt=\"Data Flow Through System\" width=\"400\"/>\n",
    "\n",
    "**Key Points:**\n",
    "- **Green boxes** = V1 components (keep the 93% routing!)\n",
    "- **Orange boxes** = V2 new components (BM25 + Planning)\n",
    "- **Data flows** left-to-right: Message â†’ Routing â†’ Retrieval â†’ Planning â†’ Output\n",
    "\n",
    "**What's New in V2:**\n",
    "1. **BM25 Retriever**: Finds relevant SOPs using keyword matching\n",
    "2. **Plan Generator**: Creates multi-step plans using retrieved context\n",
    "3. **Custom Metrics**: SOP Recall + Plan Alignment (3-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf4799",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and set up environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c676349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q openai pandas python-dotenv rank-bm25\n",
    "!pip install -q 'arize-phoenix[evals]' openinference-instrumentation-openai\n",
    "\n",
    "print(\"Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Colab vs Local\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repository for data access\n",
    "    if not os.path.exists('awesome-generative-ai-guide'):\n",
    "        !git clone https://github.com/aishwaryanr/awesome-generative-ai-guide.git\n",
    "\n",
    "    # Navigate to course directory\n",
    "    os.chdir('awesome-generative-ai-guide/resources/agentic_ai_course_lil')\n",
    "\n",
    "    # Get API key from Colab secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "    # Local environment - use .env file\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in Colab Secrets or .env file\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5954cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from rank_bm25 import BM25Okapi\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "# Arize Phoenix for observability\n",
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0ad91",
   "metadata": {},
   "source": [
    "## Load SOPs (Standard Operating Procedures)\n",
    "\n",
    "V2 uses a knowledge base of 9 SOPs covering different customer support scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756eaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sops(sops_directory=\"data/sops\"):\n",
    "    \"\"\"Load all SOP text files.\"\"\"\n",
    "    sops = {}\n",
    "    sop_files = glob.glob(f\"{sops_directory}/sop_*.txt\")\n",
    "\n",
    "    for filepath in sorted(sop_files):\n",
    "        filename = os.path.basename(filepath)\n",
    "        sop_id = filename.replace('.txt', '').upper()\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sops[sop_id] = {\n",
    "            'filename': filename,\n",
    "            'content': content,\n",
    "            'word_count': len(content.split())\n",
    "        }\n",
    "\n",
    "    return sops\n",
    "\n",
    "# Load SOPs\n",
    "sops_db = load_sops()\n",
    "print(f\"Loaded {len(sops_db)} SOPs\")\n",
    "print(f\"\\nSOP IDs: {list(sops_db.keys())}\")\n",
    "print(f\"\\nExample SOP (first 200 chars):\")\n",
    "first_sop = list(sops_db.keys())[0]\n",
    "print(f\"{first_sop}: {sops_db[first_sop]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d48d7",
   "metadata": {},
   "source": [
    "## Build BM25 Index\n",
    "\n",
    "BM25 is a keyword-based retrieval algorithm. We'll use it to find relevant SOPs given a customer message.\n",
    "\n",
    "**How it works:**\n",
    "1. Combine message + department as query\n",
    "2. Score all 9 SOPs using BM25\n",
    "3. Return top K SOPs (K=2 for Prompt 1, K=4 for Prompt 2)\n",
    "\n",
    "**Key insight:** K=2 may miss relevant SOPs ranked #3-4, K=4 captures them â†’ better recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bm25_index(sops_db):\n",
    "    \"\"\"Build BM25 index over SOPs.\"\"\"\n",
    "    sop_ids = list(sops_db.keys())\n",
    "    sop_contents = [sops_db[sop_id]['content'] for sop_id in sop_ids]\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_corpus = [doc.lower().split() for doc in sop_contents]\n",
    "\n",
    "    # Build BM25\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    return bm25, sop_ids\n",
    "\n",
    "bm25_index, sop_ids = build_bm25_index(sops_db)\n",
    "print(f\"âœ“ BM25 index built over {len(sop_ids)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30f4ac",
   "metadata": {},
   "source": [
    "## Planning Agent - Prompt 1 (Baseline)\n",
    "\n",
    "**Configuration:**\n",
    "- **Routing**: V1's improved Prompt 2 (93% accuracy) - EXACT copy\n",
    "- **Retrieval**: K=2 (retrieve top 2 SOPs)\n",
    "- **Planning**: gpt-4o\n",
    "\n",
    "**Key: We use V1's EXACT department names and routing prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanningAgent:\n",
    "    def __init__(self, client, bm25_index, sop_ids, sops_db, top_k=2, model=\"gpt-4o\"):\n",
    "        \"\"\"\n",
    "        Unified Planning Agent that works for both Prompt 1 and Prompt 2.\n",
    "\n",
    "        Args:\n",
    "            top_k: Number of SOPs to retrieve (Prompt 1: 2, Prompt 2: 4)\n",
    "            model: LLM model to use (Prompt 1: gpt-4o, Prompt 2: gpt-5)\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.bm25_index = bm25_index\n",
    "        self.sop_ids = sop_ids\n",
    "        self.sops_db = sops_db\n",
    "        self.top_k = top_k\n",
    "        self.model = model\n",
    "\n",
    "        # Use EXACT same department names as V1's enum\n",
    "        self.departments = [\n",
    "            \"BILLING\",\n",
    "            \"RETURNS\",\n",
    "            \"TECHNICAL_SUPPORT\",\n",
    "            \"ORDER_STATUS\",\n",
    "            \"PRODUCT_INQUIRY\",\n",
    "            \"ACCOUNT_MANAGEMENT\",\n",
    "            \"ESCALATION\"\n",
    "        ]\n",
    "\n",
    "    def route_message(self, message):\n",
    "        \"\"\"Route to department using V1's improved Prompt 2 (93% accuracy).\"\"\"\n",
    "        prompt = f\"\"\"Route customer messages to departments.\n",
    "\n",
    "Available departments:\n",
    "- BILLING: Payment issues, charges, refunds, refund status, account balances, fees\n",
    "- RETURNS: Return requests, exchanges, return status, return policies\n",
    "- TECHNICAL_SUPPORT: Login problems, password reset issues, website errors, checkout failures\n",
    "- ORDER_STATUS: Order tracking, shipping updates, delivery questions, missing items\n",
    "- PRODUCT_INQUIRY: Product questions, specifications, availability, pricing\n",
    "- ACCOUNT_MANAGEMENT: Profile updates, changing saved payment methods, preferences, address changes\n",
    "- ESCALATION: Very upset customers demanding managers, supervisor requests\n",
    "\n",
    "Important:\n",
    "- Login/password problems = TECHNICAL_SUPPORT (not ACCOUNT_MANAGEMENT)\n",
    "- Updating payment methods = ACCOUNT_MANAGEMENT (not BILLING)\n",
    "- Refund status = BILLING (not RETURNS)\n",
    "\n",
    "Message: \\\"{message}\\\"\n",
    "\n",
    "Respond with ONLY the department name, nothing else.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def retrieve_sops(self, message, department):\n",
    "        \"\"\"Retrieve relevant SOPs using BM25.\"\"\"\n",
    "        query = f\"{message} {department}\"\n",
    "        tokenized_query = query.lower().split()\n",
    "\n",
    "        scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k]\n",
    "\n",
    "        retrieved_sops = []\n",
    "        for idx in top_indices:\n",
    "            sop_id = self.sop_ids[idx]\n",
    "            score = scores[idx]\n",
    "            content = self.sops_db[sop_id]['content']\n",
    "\n",
    "            words = content.split()[:1500]\n",
    "            excerpt = ' '.join(words)\n",
    "\n",
    "            retrieved_sops.append({\n",
    "                'sop_id': sop_id,\n",
    "                'score': score,\n",
    "                'excerpt': excerpt,\n",
    "                'full_content': content\n",
    "            })\n",
    "\n",
    "        return retrieved_sops\n",
    "\n",
    "    def generate_plan(self, message, department, retrieved_sops):\n",
    "        \"\"\"Generate action plan using configured model.\"\"\"\n",
    "        sops_context = \"\\n\\n\".join([\n",
    "            f\"--- {sop['sop_id']} (Relevance: {sop['score']:.2f}) ---\\n{sop['excerpt'][:2000]}...\"\n",
    "            for sop in retrieved_sops\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"You are a customer support agent planning assistant. Create a detailed, step-by-step action plan.\n",
    "\n",
    "**Customer Message:**\n",
    "\"{message}\"\n",
    "\n",
    "**Department:** {department}\n",
    "\n",
    "**Relevant Procedures (SOPs):**\n",
    "{sops_context}\n",
    "\n",
    "**Instructions:**\n",
    "Create a detailed action plan that:\n",
    "1. Lists specific steps the agent should take (in order)\n",
    "2. References relevant SOP procedures\n",
    "3. Includes verification or security steps\n",
    "4. Mentions escalation criteria if applicable\n",
    "5. Provides timeline expectations\n",
    "6. Notes any edge cases or system limitations\n",
    "\n",
    "Format as a numbered action plan. Be specific and actionable.\n",
    "\n",
    "**Action Plan:**\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def plan(self, message):\n",
    "        \"\"\"Full pipeline.\"\"\"\n",
    "        department = self.route_message(message)\n",
    "        retrieved_sops = self.retrieve_sops(message, department)\n",
    "        plan = self.generate_plan(message, department, retrieved_sops)\n",
    "\n",
    "        return {\n",
    "            'message': message,\n",
    "            'department': department,\n",
    "            'retrieved_sops': [\n",
    "                {'sop_id': sop['sop_id'], 'score': sop['score']}\n",
    "                for sop in retrieved_sops\n",
    "            ],\n",
    "            'plan': plan\n",
    "        }\n",
    "\n",
    "# Initialize Prompt 1 agent (baseline)\n",
    "agent = PlanningAgent(client, bm25_index, sop_ids, sops_db, top_k=2, model=\"gpt-4o\")\n",
    "print(\"âœ“ PlanningAgent initialized (Prompt 1: K=2, gpt-4o)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41db90",
   "metadata": {},
   "source": [
    "## Demo: Generate a Plan\n",
    "\n",
    "Let's see the agent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a66cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_message = \"I bought a jacket last month, but it's too big. Can I return it?\"\n",
    "\n",
    "result = agent.plan(test_message)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PLANNING AGENT DEMO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCustomer Message: {result['message']}\")\n",
    "print(f\"\\nRouted Department: {result['department']}\")\n",
    "print(f\"\\nRetrieved SOPs:\")\n",
    "for sop in result['retrieved_sops']:\n",
    "    print(f\"  - {sop['sop_id']} (score: {sop['score']:.2f})\")\n",
    "print(f\"\\nGenerated Action Plan:\")\n",
    "print(result['plan'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92851e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¬ End of Chapter\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f878d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Continuous Calibration (CC) Phase\n",
    "\n",
    "**Goal:** Observe failures, design custom metrics, and identify improvements.\n",
    "\n",
    "**In this phase:**\n",
    "- Enable Phoenix tracing to observe all LLM calls\n",
    "- Run systematic evaluation on test cases\n",
    "- Analyze errors in Phoenix UI\n",
    "- Design metrics from observed patterns (SOP Recall, Plan Alignment)\n",
    "- Compute metrics to quantify performance\n",
    "\n",
    "**Output:** Custom metrics that measure what matters + clear improvement targets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c87592",
   "metadata": {},
   "source": [
    "## Enable Phoenix Tracing\n",
    "\n",
    "Phoenix captures all LLM calls so we can observe what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Phoenix (Colab-compatible setup)\n",
    "import os\n",
    "\n",
    "# Configure Phoenix for Colab/local compatibility\n",
    "os.environ[\"PHOENIX_HOST\"] = \"0.0.0.0\"\n",
    "os.environ[\"PHOENIX_PORT\"] = \"6006\"\n",
    "\n",
    "import phoenix as px\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Starting Arize Phoenix...\")\n",
    "print(\"=\"*80)\n",
    "session = px.launch_app()  # don't pass port parameter\n",
    "print(\"Phoenix session url:\", session.url)\n",
    "\n",
    "# For Google Colab compatibility\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.serve_kernel_port_as_window(6006)\n",
    "    print(\"âœ“ Phoenix running on Colab at port 6006\")\n",
    "except ImportError:\n",
    "    print(\"âœ“ Phoenix running locally at http://localhost:6006\")\n",
    "\n",
    "print(\"Open the URL above to view traces in real-time\\n\")\n",
    "\n",
    "# Enable OpenAI instrumentation for Prompt 1\n",
    "project_name = \"V2_planning_autonomy_prompt_1\"\n",
    "print(f\"Enabling tracing for project: {project_name}\")\n",
    "tracer_provider = register(project_name=project_name)\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "print(\"âœ“ Tracing enabled! All API calls will be captured in Phoenix.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be0e57",
   "metadata": {},
   "source": [
    "## Load Test Cases\n",
    "\n",
    "We have 22 grounded test cases with expected SOPs and procedure steps.\n",
    "\n",
    "**Each test case includes:**\n",
    "- Customer message\n",
    "- Complexity level (simple, medium, complex)\n",
    "- Expected SOPs (ground truth)\n",
    "- Expected procedure steps\n",
    "- Policy details to mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d58285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases\n",
    "test_cases = pd.read_csv('data/v2_test_cases.csv')\n",
    "print(f\"Loaded {len(test_cases)} test cases\")\n",
    "print(f\"\\nColumns: {list(test_cases.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(test_cases[['message', 'complexity', 'expected_sops']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006c8eb",
   "metadata": {},
   "source": [
    "## Run Prompt 1 Evaluation\n",
    "\n",
    "Let's evaluate the baseline and observe failures in Phoenix.\n",
    "\n",
    "**Note:** This will make ~66 OpenAI API calls (22 test cases Ã— 3 calls each):\n",
    "- 1 call for routing\n",
    "- 1 call for plan generation\n",
    "- Takes ~5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82121bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sop_name(sop):\n",
    "    \"\"\"Normalize SOP name to base format (e.g., SOP_001).\"\"\"\n",
    "    import re\n",
    "    sop = str(sop).upper()\n",
    "    sop = sop.replace('SOP-', 'SOP_').replace(' ', '_')\n",
    "    match = re.match(r'(SOP_\\d+)', sop)\n",
    "    return match.group(1) if match else sop\n",
    "\n",
    "def evaluate_agent(agent, test_cases, tracer, description):\n",
    "    \"\"\"Run evaluation with Phoenix tracing.\"\"\"\n",
    "    results = []\n",
    "    total = len(test_cases)\n",
    "\n",
    "    print(f\"Running {description} evaluation on {total} test cases...\\n\")\n",
    "\n",
    "    for idx, row in test_cases.iterrows():\n",
    "        message = row['message']\n",
    "        expected_sops = row['expected_sops'].split(',') if pd.notna(row['expected_sops']) else []\n",
    "        expected_sops = [normalize_sop_name(s.strip()) for s in expected_sops]\n",
    "\n",
    "        print(f\"[{idx+1}/{total}] {message[:40]}...\", end=' ')\n",
    "\n",
    "        with tracer.start_as_current_span(f\"test_case_{idx}\") as span:\n",
    "            span.set_attribute(\"test.id\", idx)\n",
    "            span.set_attribute(\"test.message\", message)\n",
    "            span.set_attribute(\"test.expected_sops\", str(expected_sops))\n",
    "\n",
    "            try:\n",
    "                result = agent.plan(message)\n",
    "                retrieved_sop_ids = [normalize_sop_name(sop['sop_id']) for sop in result['retrieved_sops']]\n",
    "\n",
    "                span.set_attribute(\"result.department\", result['department'])\n",
    "                span.set_attribute(\"result.retrieved_sops\", str(retrieved_sop_ids))\n",
    "                span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "                results.append({\n",
    "                    'test_case_id': idx,\n",
    "                    'message': message,\n",
    "                    'complexity': row['complexity'],\n",
    "                    'expected_sops': expected_sops,\n",
    "                    'retrieved_sops': retrieved_sop_ids,\n",
    "                    'department': result['department'],\n",
    "                    'plan': result['plan']\n",
    "                })\n",
    "                print(\"âœ“\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run Prompt 1 evaluation\n",
    "results_df = evaluate_agent(agent, test_cases, tracer, \"Prompt 1\")\n",
    "print(f\"\\nâœ“ Completed {len(results_df)} evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28bc25",
   "metadata": {},
   "source": [
    "## Observe Traces in Phoenix\n",
    "\n",
    "### The Trace-First Evaluation Workflow\n",
    "\n",
    "**Key workflow:** Observe â†’ Discover â†’ Measure â†’ Improve\n",
    "\n",
    "**Go to Phoenix UI:** http://localhost:6006/\n",
    "\n",
    "**What to observe:**\n",
    "1. Click on \"V2_planning_autonomy_prompt_1\" project\n",
    "2. See all test case traces\n",
    "3. Click on individual traces to see:\n",
    "   - Routing call (V1's prompt)\n",
    "   - Plan generation call (with SOPs)\n",
    "   - Retrieved SOPs vs Expected SOPs\n",
    "4. **Look for patterns:**\n",
    "   - Missing expected SOPs (K=2 limitation?)\n",
    "   - Plans missing critical steps\n",
    "   - Wrong SOPs retrieved\n",
    "\n",
    "**Exercise:** Find 3-5 failed cases and note what went wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a478fe",
   "metadata": {},
   "source": [
    "## Analyze Errors\n",
    "\n",
    "From observations, design metrics to measure failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23735b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(results_df):\n",
    "    \"\"\"Analyze retrieval errors.\"\"\"\n",
    "    errors = {'missing': [], 'extra': []}\n",
    "\n",
    "    for idx, row in results_df.iterrows():\n",
    "        expected = set(row['expected_sops']) if isinstance(row['expected_sops'], list) else set()\n",
    "        retrieved = set(row['retrieved_sops']) if isinstance(row['retrieved_sops'], list) else set()\n",
    "\n",
    "        missing = expected - retrieved\n",
    "        extra = retrieved - expected\n",
    "\n",
    "        if missing:\n",
    "            errors['missing'].append({\n",
    "                'id': idx,\n",
    "                'message': row['message'][:60],\n",
    "                'missing': list(missing)\n",
    "            })\n",
    "        if extra:\n",
    "            errors['extra'].append({\n",
    "                'id': idx,\n",
    "                'message': row['message'][:60],\n",
    "                'extra': list(extra)\n",
    "            })\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nMissing SOPs: {len(errors['missing'])} cases\")\n",
    "    print(f\"Extra SOPs: {len(errors['extra'])} cases\")\n",
    "\n",
    "    if errors['missing']:\n",
    "        print(f\"\\nExample missing SOPs (first 3):\")\n",
    "        for e in errors['missing'][:3]:\n",
    "            print(f\"  {e['message']}... Missing: {e['missing']}\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "# Analyze errors\n",
    "errors = analyze_errors(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0b161",
   "metadata": {},
   "source": [
    "## Design 2 Custom Metrics\n",
    "\n",
    "Based on observed failures, we design 2 metrics:\n",
    "\n",
    "### Metric 1: SOP Retrieval Recall @ K\n",
    "- **What:** % of expected SOPs actually retrieved\n",
    "- **Why:** Wrong SOPs â†’ wrong plan (garbage in, garbage out)\n",
    "- **Observed:** K=2 misses relevant SOPs ranked #3+\n",
    "- **Formula:** `recall = len(retrieved âˆ© expected) / len(expected)`\n",
    "\n",
    "### Metric 2: Plan-to-Steps Alignment (3-class)\n",
    "- **What:** Does plan cover expected procedure steps?\n",
    "- **Classes:** good (complete), partial (minor gaps), bad (major gaps)\n",
    "- **Why:** End-to-end quality check\n",
    "- **Observed:** Plans missing critical steps or policy details\n",
    "- **Judge:** GPT-4o evaluates with reasoning\n",
    "\n",
    "**Key:** Metrics emerged from observations, not predetermined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bd8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sop_recall(expected_sops, retrieved_sops):\n",
    "    \"\"\"Calculate % of expected SOPs retrieved.\"\"\"\n",
    "    if not expected_sops:\n",
    "        return 1.0\n",
    "    expected_set = set([normalize_sop_name(s) for s in expected_sops])\n",
    "    retrieved_set = set([normalize_sop_name(s) for s in retrieved_sops])\n",
    "    relevant = expected_set & retrieved_set\n",
    "    return len(relevant) / len(expected_set)\n",
    "\n",
    "def judge_plan_quality(message, expected_steps, policy_details, generated_plan):\n",
    "    \"\"\"LLM judge: returns 'good', 'partial', or 'bad'.\"\"\"\n",
    "    prompt = f\"\"\"Evaluate if this action plan covers expected procedure steps.\n",
    "\n",
    "**Message:** {message}\n",
    "**Expected Steps:** {expected_steps}\n",
    "**Expected Policy:** {policy_details}\n",
    "**Generated Plan:** {generated_plan}\n",
    "\n",
    "Classify as:\n",
    "- good: All critical steps covered, complete and actionable\n",
    "- partial: Main steps covered but missing some details\n",
    "- bad: Missing critical steps or significant gaps\n",
    "\n",
    "Respond: CLASS: <good|partial|bad>\n",
    "REASONING: <brief explanation>\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Parse class\n",
    "        for line in content.split('\\n'):\n",
    "            if line.startswith('CLASS:'):\n",
    "                class_text = line.replace('CLASS:', '').strip().lower()\n",
    "                return class_text if class_text in ['good', 'partial', 'bad'] else 'partial'\n",
    "        return 'partial'\n",
    "    except:\n",
    "        return 'partial'\n",
    "\n",
    "def compute_metrics(results_df, test_cases):\n",
    "    \"\"\"Compute both SOP recall and plan alignment metrics.\"\"\"\n",
    "    metrics = []\n",
    "    total = len(results_df)\n",
    "\n",
    "    print(f\"Computing metrics for {total} test cases...\\n\")\n",
    "\n",
    "    for idx, row in results_df.iterrows():\n",
    "        test_row = test_cases.iloc[idx]\n",
    "        expected_steps = test_row.get('expected_steps', '') if pd.notna(test_row.get('expected_steps')) else ''\n",
    "        policy_details = test_row.get('policy_details', '') if pd.notna(test_row.get('policy_details')) else ''\n",
    "\n",
    "        message = row['message']\n",
    "        expected_sops = row['expected_sops'] if isinstance(row['expected_sops'], list) else []\n",
    "        retrieved_sops = row['retrieved_sops'] if isinstance(row['retrieved_sops'], list) else []\n",
    "        plan = row['plan']\n",
    "\n",
    "        # Compute metrics\n",
    "        recall = compute_sop_recall(expected_sops, retrieved_sops)\n",
    "        alignment = judge_plan_quality(message, expected_steps, policy_details, plan)\n",
    "\n",
    "        print(f\"[{idx+1}/{total}] Recall: {recall:.0%}, Alignment: {alignment}\")\n",
    "\n",
    "        metrics.append({\n",
    "            'test_case_id': idx,\n",
    "            'sop_recall': recall,\n",
    "            'plan_alignment': alignment\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "print(\"âœ“ Metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49de9c7",
   "metadata": {},
   "source": [
    "## Compute Metrics for Prompt 1\n",
    "\n",
    "**Note:** This will make 22 more API calls (one per test case for LLM-as-Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for Prompt 1\n",
    "metrics_df = compute_metrics(results_df, test_cases)\n",
    "print(f\"\\nâœ“ Metrics computed for {len(metrics_df)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3934d",
   "metadata": {},
   "source": [
    "## Summarize Prompt 1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb05c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 1 METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Metric 1: SOP Recall\n",
    "recall_mean = metrics_df['sop_recall'].mean()\n",
    "print(f\"\\n1. SOP Retrieval Recall: {recall_mean:.1%}\")\n",
    "print(f\"   â†’ We retrieve {recall_mean:.0%} of expected SOPs on average\")\n",
    "\n",
    "# Metric 2: Plan Alignment\n",
    "alignment_counts = metrics_df['plan_alignment'].value_counts()\n",
    "good = alignment_counts.get('good', 0)\n",
    "partial = alignment_counts.get('partial', 0)\n",
    "bad = alignment_counts.get('bad', 0)\n",
    "total = len(metrics_df)\n",
    "\n",
    "print(f\"\\n2. Plan Alignment:\")\n",
    "print(f\"   Good: {good}/{total} ({good/total:.0%})\")\n",
    "print(f\"   Partial: {partial}/{total} ({partial/total:.0%})\")\n",
    "print(f\"   Bad: {bad}/{total} ({bad/total:.0%})\")\n",
    "print(f\"   â†’ {good} complete plans, {partial} need minor fixes, {bad} have gaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb30e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¬ End of Chapter\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545b9a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸš€ Continuous Deployment (CD) Phase\n",
    "\n",
    "**Goal:** Make targeted improvements and measure impact.\n",
    "\n",
    "**In this phase:**\n",
    "- Identify root causes from CC metrics\n",
    "- Design Prompt 2 with targeted fixes (K=2â†’4, gpt-4oâ†’gpt-5)\n",
    "- Re-evaluate with same metrics\n",
    "- Compare Prompt 1 vs Prompt 2 performance\n",
    "- Validate improvements worked\n",
    "\n",
    "**Output:** Better system with measured improvements (SOP Recall: 54%â†’76%, Plan Alignment: 72%â†’100%).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371835b",
   "metadata": {},
   "source": [
    "## Identify Problems â†’ Design Improvements\n",
    "\n",
    "Based on metrics, what should we improve?\n",
    "\n",
    "**Problem 1: Low SOP Recall (53.79%)**\n",
    "- Root cause: K=2 is too restrictive\n",
    "- Many relevant SOPs ranked #3-4 but not retrieved\n",
    "- **Solution:** Increase K from 2 to 4\n",
    "\n",
    "**Problem 2: Plan Alignment not perfect (72% good)**\n",
    "- Root cause: gpt-4o has limitations\n",
    "- Some plans missing steps or policy details\n",
    "- **Solution:** Upgrade to gpt-5 (better reasoning)\n",
    "\n",
    "**Prompt 2 Improvements:**\n",
    "1. K=2 â†’ K=4 (targets SOP Recall)\n",
    "2. gpt-4o â†’ gpt-5 (targets Plan Alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a6095",
   "metadata": {},
   "source": [
    "## Prompt 2: Improved Agent\n",
    "\n",
    "Same architecture, but with targeted improvements.\n",
    "\n",
    "**Changes:**\n",
    "- âœ… K=2 â†’ K=4 (better SOP retrieval)\n",
    "- âœ… gpt-4o â†’ gpt-5 (better plan generation)\n",
    "- âœ… Same V1 routing (keep what works!)\n",
    "\n",
    "**Goal:** Improve both SOP Recall and Plan Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48120be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Prompt 2 agent (improved)\n",
    "agent_p2 = PlanningAgent(client, bm25_index, sop_ids, sops_db, top_k=4, model=\"gpt-5\")\n",
    "print(\"âœ“ PlanningAgent initialized (Prompt 2: K=4, gpt-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd62087",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What We Built:**\n",
    "- V2 Planning Agent that generates multi-step plans\n",
    "- Builds on V1's 93% routing (EXACT department names)\n",
    "- Uses BM25 to retrieve relevant SOPs\n",
    "- Uses LLM to generate detailed action plans\n",
    "\n",
    "**What We Learned:**\n",
    "1. **Incremental Building:** V2 = V1's routing + new capabilities\n",
    "2. **Trace-First:** Observe failures â†’ Design metrics â†’ Improve\n",
    "3. **Custom Metrics:** SOP Recall + Plan Alignment (3-class)\n",
    "4. **Targeted Improvements:** K=2â†’4, gpt-4oâ†’gpt-5\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run Prompt 2 evaluation\n",
    "2. Compute Prompt 2 metrics\n",
    "3. Compare Prompt 1 vs Prompt 2\n",
    "4. Verify improvements worked!\n",
    "\n",
    "**V2 Planning Autonomy: Complete!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

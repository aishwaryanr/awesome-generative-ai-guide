# Partie 12 : Annexes techniques

Cette partie regroupe des ressources de r√©f√©rence pratiques pour l'entra√Ænement et le d√©ploiement de LLM.

---

## 12.1 Glossaire complet LLM

### A

**Adapter** : Module l√©ger fine-tun√© sur un mod√®le base fig√© (ex: LoRA).

**Alignment** : Processus pour aligner le comportement du mod√®le avec les intentions humaines (RLHF, DPO).

**Attention** : M√©canisme permettant au mod√®le de pond√©rer l'importance de diff√©rentes parties de l'input.

**Autoregressive** : G√©n√©ration token par token, chaque token d√©pendant des pr√©c√©dents.

### B

**Batch size** : Nombre d'exemples trait√©s simultan√©ment.

**Beam search** : Strat√©gie de d√©codage maintenant plusieurs hypoth√®ses.

**BLEU** : M√©trique d'√©valuation pour g√©n√©ration de texte (comparaison n-grammes).

**BPE (Byte-Pair Encoding)** : Algorithme de tokenisation subword.

### C

**Causal masking** : Masque emp√™chant l'attention sur les tokens futurs.

**Checkpoint** : Snapshot des poids du mod√®le √† un moment donn√©.

**Context window** : Longueur maximale de s√©quence que le mod√®le peut traiter.

**Cross-entropy** : Fonction de loss mesurant la diff√©rence entre distributions.

### D

**Decoding** : Processus de g√©n√©ration de texte depuis les logits.

**Distillation** : Entra√Æner un petit mod√®le √† imiter un grand.

**DPO (Direct Preference Optimization)** : M√©thode d'alignement sans RL explicite.

**Dropout** : Technique de r√©gularisation d√©sactivant al√©atoirement des neurones.

### E

**Embedding** : Repr√©sentation vectorielle dense d'un token.

**Epoch** : Une passe compl√®te sur tout le dataset d'entra√Ænement.

### F

**Fine-tuning** : Adapter un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cifique.

**FlashAttention** : Impl√©mentation optimis√©e de l'attention r√©duisant la m√©moire.

**FSDP (Fully Sharded Data Parallel)** : Parall√©lisme distribu√© shardant les param√®tres.

### G

**Gradient accumulation** : Accumuler gradients sur plusieurs mini-batchs avant update.

**Greedy decoding** : Toujours choisir le token le plus probable.

### H

**Hallucination** : G√©n√©ration de contenu factuellement incorrect mais fluide.

**Hyperparameter** : Param√®tre de configuration de l'entra√Ænement (learning rate, batch size, etc.).

### I

**Inference** : Utilisation du mod√®le entra√Æn√© pour faire des pr√©dictions.

**Instruction tuning** : Fine-tuning sur paires instruction-r√©ponse.

### K

**KL divergence** : Mesure de diff√©rence entre deux distributions.

**KV cache** : Cache des cl√©s et valeurs d'attention pour acc√©l√©rer l'inf√©rence.

### L

**Learning rate** : Taux d'ajustement des poids pendant l'entra√Ænement.

**Logits** : Scores bruts avant softmax.

**LoRA (Low-Rank Adaptation)** : Fine-tuning efficient via matrices low-rank.

### M

**Masking** : Cacher certains tokens (pour causal LM ou padding).

**MoE (Mixture of Experts)** : Architecture avec plusieurs FFN, routage dynamique.

**Multi-head attention** : Attention avec plusieurs t√™tes parall√®les.

### N

**NTP (Next Token Prediction)** : Objectif d'entra√Ænement des LLM.

**Nucleus sampling** : Top-p sampling bas√© sur probabilit√© cumulative.

### P

**Perplexity** : M√©trique mesurant l'incertitude du mod√®le (exp de la loss).

**Prompt** : Texte d'entr√©e donn√© au mod√®le.

**PPO (Proximal Policy Optimization)** : Algorithme RL utilis√© dans RLHF.

### Q

**Quantization** : R√©duction de la pr√©cision des poids (float16 ‚Üí int8/int4).

### R

**RAG (Retrieval-Augmented Generation)** : Combiner retrieval et g√©n√©ration.

**Reward model** : Mod√®le apprenant √† scorer les r√©ponses selon pr√©f√©rences.

**RLHF (RL from Human Feedback)** : M√©thode d'alignement via RL.

### S

**Sampling** : S√©lection stochastique du prochain token.

**Scaling laws** : Relations empiriques entre taille mod√®le/donn√©es et performance.

**SFT (Supervised Fine-Tuning)** : Fine-tuning supervis√© sur paires input-output.

**Softmax** : Fonction transformant logits en probabilit√©s.

### T

**Temperature** : Param√®tre contr√¥lant la randomness du sampling.

**Tokenization** : D√©coupage du texte en unit√©s (tokens).

**Top-k sampling** : Sampler parmi les k tokens les plus probables.

**Transformer** : Architecture de r√©seau bas√©e sur l'attention.

### W

**Warmup** : Augmentation progressive du learning rate en d√©but d'entra√Ænement.

**Weight decay** : R√©gularisation L2 sur les poids.

### Z

**ZeRO** : Optimisation m√©moire (sharding optimizer states, gradients, param√®tres).

---

## 12.2 Recettes de configuration standard

### 12.2.1 Mod√®les p√©dagogiques (125M - 1B params)

**Configuration 125M (GPT-2 small)** :

```python
config_125M = {
    "vocab_size": 50257,
    "n_positions": 1024,
    "n_embd": 768,
    "n_layer": 12,
    "n_head": 12,
    "n_inner": 3072,  # 4 √ó n_embd
    "activation": "gelu",
    "dropout": 0.1,
}

training_config_125M = {
    "batch_size": 512,
    "learning_rate": 6e-4,
    "warmup_steps": 2000,
    "max_steps": 100000,
    "weight_decay": 0.1,
    "grad_clip": 1.0,
}
```

**Configuration 1.3B** :

```python
config_1.3B = {
    "vocab_size": 50257,
    "n_positions": 2048,
    "n_embd": 1536,
    "n_layer": 36,
    "n_head": 16,
    "n_inner": 6144,
    "activation": "gelu",
    "dropout": 0.1,
}

training_config_1.3B = {
    "batch_size": 256,
    "learning_rate": 2e-4,
    "warmup_steps": 5000,
    "max_steps": 300000,
    "weight_decay": 0.1,
    "grad_clip": 1.0,
}
```

### 12.2.2 Recette pr√©-training base (7B params)

**Inspir√© de LLaMA/Mistral** :

```python
config_7B = {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "intermediate_size": 11008,  # ~2.7 √ó hidden
    "num_hidden_layers": 32,
    "num_attention_heads": 32,
    "num_key_value_heads": 32,  # GQA: peut r√©duire (ex: 8)
    "max_position_embeddings": 4096,
    "rope_theta": 10000.0,  # RoPE
    "rms_norm_eps": 1e-5,
    "attention_dropout": 0.0,
    "use_cache": True,
}

training_7B = {
    "global_batch_size": 4_000_000,  # tokens
    "micro_batch_size": 4,
    "gradient_accumulation_steps": "auto",  # d√©pend du nombre de GPUs
    "learning_rate": 3e-4,
    "min_lr": 3e-5,
    "warmup_steps": 2000,
    "total_steps": 100_000,
    "lr_schedule": "cosine",
    "weight_decay": 0.1,
    "grad_clip": 1.0,
    "bf16": True,
}

# Donn√©es
data_mix_7B = {
    "web_crawl": 0.45,
    "books": 0.15,
    "code": 0.10,
    "wikipedia": 0.05,
    "papers": 0.05,
    "conversations": 0.10,
    "other": 0.10,
}
```

### 12.2.3 Recette SFT

```python
sft_config = {
    "base_model": "meta-llama/Llama-2-7b-hf",
    "dataset_size": 50_000,  # paires instruction-r√©ponse
    "epochs": 3,
    "batch_size": 128,
    "learning_rate": 2e-5,
    "lr_schedule": "cosine",
    "warmup_ratio": 0.03,
    "max_seq_length": 2048,
    "packing": True,  # Packer plusieurs exemples par s√©quence
    "use_flash_attn": True,
}

# Mix de donn√©es SFT
sft_mix = {
    "general_qa": 0.30,
    "reasoning": 0.20,
    "code": 0.20,
    "creative": 0.15,
    "math": 0.10,
    "safety": 0.05,
}
```

### 12.2.4 Recette DPO

```python
dpo_config = {
    "base_model": "llama-2-7b-chat",  # Partir du mod√®le SFT
    "preference_dataset_size": 10_000,
    "epochs": 1,
    "batch_size": 64,
    "learning_rate": 5e-7,  # Beaucoup plus faible que SFT
    "beta": 0.1,  # Coefficient KL
    "max_seq_length": 2048,
    "max_prompt_length": 1024,
}
```

### 12.2.5 Recette RLHF (PPO)

```python
rlhf_config = {
    "base_model": "llama-2-7b-sft",
    "reward_model": "llama-2-7b-reward",
    "ppo_epochs": 4,
    "batch_size": 128,
    "mini_batch_size": 32,
    "learning_rate": 1.41e-5,
    "init_kl_coef": 0.2,
    "target_kl": 6.0,
    "gamma": 1.0,
    "lam": 0.95,
    "cliprange": 0.2,
}
```

---

## 12.3 Checklists op√©rationnelles

### 12.3.1 Checklist avant grand training

**Donn√©es** :
- [ ] Dataset nettoy√© et d√©dupliqu√©
- [ ] Validation split s√©par√© (min 1000 exemples)
- [ ] Statistiques de base calcul√©es (longueur moyenne, distribution)
- [ ] PII supprim√©es ou anonymis√©es
- [ ] Licences v√©rifi√©es

**Infrastructure** :
- [ ] GPUs disponibles et test√©s
- [ ] Stockage suffisant (3√ó taille des donn√©es minimum)
- [ ] Monitoring configur√© (W&B, TensorBoard, Prometheus)
- [ ] Checkpointing automatique activ√©
- [ ] Backup/recovery plan d√©fini

**Code** :
- [ ] Reproductibilit√© assur√©e (seeds, versions fig√©es)
- [ ] Tests unitaires pass√©s
- [ ] Configuration versionn√©e (git)
- [ ] Script de reprise apr√®s crash test√©

**Validation** :
- [ ] Benchmarks de r√©f√©rence d√©finis
- [ ] Validation manuelle pr√©vue (√©chantillon √† √©valuer humainement)
- [ ] Crit√®res d'arr√™t d√©finis (perplexit√©, accuracy, early stopping)

### 12.3.2 Checklist avant mise en production

**Mod√®le** :
- [ ] √âvalu√© sur benchmarks standard
- [ ] Test√© sur cas edge/adversariaux
- [ ] Red teaming effectu√© (s√©curit√©, jailbreaks)
- [ ] Taille optimis√©e (quantization si pertinent)
- [ ] Comportement sur prompts vides/longs v√©rifi√©

**Infrastructure** :
- [ ] Load testing effectu√© (latence, throughput)
- [ ] Autoscaling configur√© et test√©
- [ ] Fallback/redundancy en place
- [ ] Monitoring et alertes actifs
- [ ] Logs centralis√©s

**API** :
- [ ] Authentification s√©curis√©e
- [ ] Rate limiting configur√©
- [ ] Documentation API √† jour
- [ ] Versioning en place (v1, v2...)
- [ ] Quotas et billing configur√©s

**L√©gal & Compliance** :
- [ ] Conditions d'utilisation valid√©es
- [ ] RGPD : Droit √† l'oubli impl√©ment√©
- [ ] Audit trail en place
- [ ] PII masqu√©es dans les logs
- [ ] Modalit√©s de support d√©finies

### 12.3.3 Checklist s√©curit√©

**Mod√®le** :
- [ ] Pas de donn√©es sensibles dans les poids (membership inference test√©)
- [ ] Refus appropri√©s impl√©ment√©s
- [ ] Garde-fous contre g√©n√©ration de contenu dangereux
- [ ] Watermarking (si applicable)

**API** :
- [ ] HTTPS uniquement
- [ ] CORS correctement configur√©
- [ ] Input sanitization (limite taille, format)
- [ ] Output filtering (d√©tection contenu probl√©matique)
- [ ] Rate limiting anti-abus

**Infrastructure** :
- [ ] Secrets stock√©s s√©curis√©s (vault, secrets manager)
- [ ] Acc√®s bas√© sur moindre privil√®ge
- [ ] Logs d'acc√®s et audits activ√©s
- [ ] Sauvegarde chiffr√©e
- [ ] Plan de r√©ponse aux incidents d√©fini

---

## 12.4 Pistes de recherche et √©volutions futures

### 12.4.1 Architectures post-Transformer

**SSM/Mamba** :
- Complexit√© lin√©aire en longueur de s√©quence
- Inf√©rence en temps constant par token
- Challenge : Atteindre la qualit√© des Transformers

**Architectures hybrides** :
- Combiner Transformer (raisonnement) + SSM (efficacit√©)
- Exemple : Attention locale + SSM global
- Potentiel : Meilleur compromis qualit√©/co√ªt

**√Ä surveiller** :
- RWKV : RNN avec parall√©lisation
- Retentive Networks
- √âvolutions de Mamba (Mamba-2, etc.)

### 12.4.2 LLM pour math√©matiques et raisonnement

**D√©fis actuels** :
- Hallucinations sur calculs complexes
- Difficult√© avec raisonnement multi-√©tapes
- Manque de v√©rification formelle

**Approches prometteuses** :
- Tool use syst√©matique (calculateurs, provers)
- Reward models sp√©cialis√©s en maths
- G√©n√©ration de preuves formelles (Lean, Coq)
- Chain-of-thought renforc√©

### 12.4.3 Int√©gration symbolique et neuronale

**Neurosymbolic AI** :
- Combiner LLM (flexibilit√©) + syst√®mes symboliques (exactitude)
- Exemple : LLM g√©n√®re code ‚Üí moteur de r√®gles valide
- Applications : Planification, v√©rification, contraintes

**Knowledge graphs + LLM** :
- RAG enrichi avec graphes de connaissances
- Raisonnement relationnel explicite
- Tra√ßabilit√© et explicabilit√© am√©lior√©es

### 12.4.4 Efficacit√© et green AI

**Challenges** :
- Co√ªt carbone de l'entra√Ænement (GPT-3 : ~500 tonnes CO2)
- Consommation √©nerg√©tique de l'inf√©rence √† grande √©chelle

**Directions** :
- Mod√®les small mais capables (distillation avanc√©e)
- Sparsit√© et pruning
- Optimisations mat√©rielles (TPU, ASICs d√©di√©s)
- Entra√Ænement distribu√© efficient (moins de redondance)

### 12.4.5 Multimodalit√© riche

**Au-del√† de texte + image** :
- Audio, vid√©o, capteurs, robotique
- G√©n√©ration vid√©o coh√©rente longue
- Contr√¥le fin et √©dition

**Challenges** :
- Alignement cross-modal robuste
- G√©n√©ration haute r√©solution
- Coh√©rence temporelle (vid√©o, audio)

---

## 12.5 Ressources compl√©mentaires

### 12.5.1 Courses et tutoriels

- [Stanford CS224N](http://web.stanford.edu/class/cs224n/) - NLP with Deep Learning
- [Hugging Face NLP Course](https://huggingface.co/course)
- [Fast.ai](https://www.fast.ai/) - Practical Deep Learning
- [DeepLearning.AI courses](https://www.deeplearning.ai/)

### 12.5.2 Frameworks et biblioth√®ques

**Entra√Ænement** :
- PyTorch, JAX/Flax
- HuggingFace Transformers
- DeepSpeed, FSDP
- Megatron-LM

**Inf√©rence** :
- vLLM, TGI, SGLang
- Ollama (local)
- ONNX Runtime, TensorRT

**Outils** :
- LangChain, LlamaIndex (orchestration)
- Weights & Biases (tracking)
- Prometheus + Grafana (monitoring)

### 12.5.3 Papers foundationnels

Voir [`REFERENCES.md`](../REFERENCES.md) pour la liste compl√®te et annot√©e.

### 12.5.4 Communaut√©s et conf√©rences

**Conf√©rences** :
- NeurIPS, ICML, ICLR (ML g√©n√©ral)
- ACL, EMNLP (NLP)
- MLSys (syst√®mes ML)

**Communaut√©s** :
- Hugging Face Forums
- EleutherAI Discord
- Reddit r/MachineLearning, r/LocalLLaMA

---

## Conclusion du livre

F√©licitations ! Vous avez parcouru un chemin complet de **z√©ro au LLM de production**.

**Vous √™tes maintenant capable de** :
- Concevoir et entra√Æner des LLM from scratch
- Fine-tuner et aligner avec les meilleures pratiques
- Int√©grer outils, agents et RAG
- Optimiser pour l'inf√©rence √† grande √©chelle
- D√©ployer en production avec observabilit√© et s√©curit√©
- Mesurer l'impact et it√©rer en continu

**Le voyage ne s'arr√™te pas ici** : Le domaine des LLM √©volue rapidement. Continuez √† :
- Exp√©rimenter avec les nouvelles architectures et techniques
- Partager vos apprentissages avec la communaut√©
- Mesurer l'impact r√©el de vos d√©ploiements
- Rester vigilant sur les aspects √©thiques et soci√©taux

**Bonne construction de LLM ! üöÄ**

---

## Index des concepts cl√©s

*(Index alphab√©tique pointant vers les sections pertinentes dans chaque partie)*

- **Alignement** ‚Üí Partie 7
- **Attention mechanism** ‚Üí Partie 4.1
- **Batching continu** ‚Üí Partie 9.2
- **DPO** ‚Üí Partie 7.4
- **FlashAttention** ‚Üí Partie 4.2
- **FSDP** ‚Üí Partie 6.4
- **KV cache** ‚Üí Partie 9.2
- **LoRA** ‚Üí Partie 9.3
- **MoE** ‚Üí Partie 4.3
- **Quantization** ‚Üí Partie 9.3
- **RAG** ‚Üí Partie 8.2
- **RLHF** ‚Üí Partie 7.2
- **Scaling laws** ‚Üí Partie 2.5
- **SFT** ‚Üí Partie 7.1
- **Tokenization** ‚Üí Partie 3.4
- **Transformer** ‚Üí Partie 4.1
- **vLLM** ‚Üí Partie 9.4

---

**Fin des annexes**

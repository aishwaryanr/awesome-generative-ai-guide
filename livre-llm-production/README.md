# De z√©ro au LLM de production

**Guide complet pour la conception, l'entra√Ænement et le d√©ploiement de Large Language Models**

---

## √Ä propos de ce livre

Ce livre vous accompagne dans un parcours complet, de la compr√©hension th√©orique des LLM jusqu'√† leur mise en production. Il couvre l'ensemble de la cha√Æne de valeur : fondations math√©matiques, architectures modernes, donn√©es, entra√Ænement, alignement, optimisation, d√©ploiement et op√©rations.

### Public cible

- Ing√©nieurs ML souhaitant ma√Ætriser les LLM de bout en bout
- Data scientists d√©sireux de passer de la recherche √† la production
- Architectes techniques concevant des syst√®mes √† base de LLM
- Chercheurs en IA voulant acqu√©rir une vision pratique et industrielle

### Pr√©requis

- Python de base
- Notions de probabilit√©s et d'alg√®bre lin√©aire (niveau licence)
- Familiarit√© avec les concepts de machine learning (souhaitable mais pas obligatoire)

---

## Table des mati√®res

### [Partie 1. Vision, panorama et mod√®les mentaux](./partie-01/README.md)
- Historique du NLP et du Deep Learning
- De RNN aux Transformers
- Cas d'usage des LLM (chatbots, agents, code, copilotes, recherche)
- Mod√®les mentaux d'ing√©nierie
- **Labs** : Formulation de cas d'usage et d√©finition de m√©triques

### [Partie 2. Fondations math√©matiques pour LLM](./partie-02/README.md)
- Espaces vectoriels et tenseurs
- Probabilit√©s, entropie et divergence KL
- Optimisation et descente de gradient
- Lois d'√©chelle (scaling laws)
- **Labs** : Impl√©mentation de cross-entropy et visualisation de paysages de perte

### [Partie 3. Bases de deep learning appliqu√©es au texte](./partie-03/README.md)
- Graphes computationnels et backpropagation
- Couches fondamentales (lin√©aires, normalisation, r√©sidus, dropout)
- Mod√®les s√©quentiels (N-gram, RNN, LSTM, GRU)
- Tokenisation (BPE, Unigram, SentencePiece)
- **Labs** : Construction d'un LM char-level puis subword

### [Partie 4. Architectures de LLM modernes](./partie-04/README.md)
- Transformer : Multi-Head Attention, encodages positionnels
- Optimisations : FlashAttention, sparse attention, long-context
- Mixture of Experts (MoE)
- Post-Transformers : SSM/Mamba et hybrides
- Mod√®les multimodaux
- **Labs** : Comparaison dense vs MoE, analyse latence/m√©moire

### [Partie 5. Donn√©es : collecte, nettoyage et pr√©paration](./partie-05/README.md)
- Sources de donn√©es (publiques, priv√©es, synth√©tiques)
- Droits, licences et PII
- D√©duplication et filtrage
- Construction du m√©lange final
- **Labs** : Pipeline de d√©duplication et analyse de distributions

### [Partie 6. Pr√©-training : entra√Æner un LLM de z√©ro](./partie-06/README.md)
- Objectif Next Token Prediction (NTP)
- Configuration mod√®le et hyperparam√®tres
- Parall√©lisme distribu√© (data, tensor, pipeline, FSDP, ZeRO)
- Monitoring et checkpointing
- **Labs** : Entra√Ænement d'un mod√®le small en FSDP

### [Partie 7. Post-training : SFT, alignement et pr√©f√©rences](./partie-07/README.md)
- Supervised Fine-Tuning (SFT)
- RLHF classique (reward model, PPO)
- RLAIF et pr√©f√©rences synth√©tiques
- M√©thodes sans RL (DPO et variantes)
- S√©curit√© et refus utiles
- **Labs** : Dataset de pr√©f√©rences, comparaison DPO vs PPO, politique de refus

### [Partie 8. Outils, agents et int√©gration avanc√©e](./partie-08/README.md)
- Tool use et function calling
- RAG fiable (indexation, chunking, reranking)
- Agents et orchestration
- M√©moires prolong√©es et personnalisation
- **Labs** : Pipeline RAG de bout en bout, agent outill√©

### [Partie 9. Inference et optimisation mod√®le](./partie-09/README.md)
- Strat√©gies de d√©codage
- Acc√©l√©ration (KV cache, batching dynamique, sp√©culation)
- Compression et adaptation (quantization, LoRA, distillation)
- Serving (vLLM, TGI, SGLang, Ollama, Triton)
- **Labs** : Benchmarks latence/throughput, pipeline de sp√©culation

### [Partie 10. D√©ploiement en production et LLMOps](./partie-10/README.md)
- Architecture API (gateway, auth, rate limiting)
- Observabilit√© et monitoring
- D√©tection de d√©rive et r√©-entra√Ænement
- Co√ªts et optimisation
- S√©curit√©, privacy et conformit√©
- **Labs** : API de service avec observabilit√©, tableau de bord de d√©rive

### [Partie 11. √âtude de cas fil rouge](./partie-11/README.md)
- Projet complet : assistant technique / copilote dev
- De la sp√©cification au d√©ploiement
- Prototype ‚Üí √âchelle ‚Üí Industrialisation
- A/B testing et roadmap d'√©volution

### [Partie 12. Annexes techniques](./partie-12/README.md)
- Glossaire complet LLM
- Recettes de configuration standard
- Checklists (training, production, s√©curit√©)
- Pistes de recherche

---

## Encadr√©s th√©matiques

Le livre contient des encadr√©s approfondis sur des sujets cl√©s :

- **Comparatif DPO vs PPO** : avantages, limites et contextes d'usage
- **Alignment automatis√© √† l'√©chelle** : juges IA, boucles critique-refine
- **Inference engines** : crit√®res de choix et patterns de d√©ploiement
- **Training-free alignment** : contr√¥les au d√©codage sans fine-tuning
- **RAG** : m√©triques, pi√®ges et bonnes pratiques

---

## Labs et exercices pratiques

Chaque partie comprend des exercices pratiques et des labs pour mettre en ≈ìuvre les concepts. Les notebooks et scripts sont disponibles dans le dossier [`labs/`](./labs/).

---

## R√©f√©rences bibliographiques

Une bibliographie annot√©e compl√®te est disponible dans [`REFERENCES.md`](./REFERENCES.md), avec les ancrages principaux :

- **Alignement** : Wang et al., Cao et al., Pan et al.
- **Serving/Inference** : Park et al.
- **RAG et contexte Transformer** : Gupta et al.
- **Outils et agents** : Watson et al.
- **SSM/Mamba** : (√† compl√©ter avec papers originaux)

---

## Structure du d√©p√¥t

```
livre-llm-production/
‚îú‚îÄ‚îÄ README.md                 # Ce fichier
‚îú‚îÄ‚îÄ REFERENCES.md             # Bibliographie annot√©e
‚îú‚îÄ‚îÄ partie-01/                # Vision et panorama
‚îú‚îÄ‚îÄ partie-02/                # Fondations math√©matiques
‚îú‚îÄ‚îÄ partie-03/                # Bases deep learning
‚îú‚îÄ‚îÄ partie-04/                # Architectures modernes
‚îú‚îÄ‚îÄ partie-05/                # Donn√©es
‚îú‚îÄ‚îÄ partie-06/                # Pr√©-training
‚îú‚îÄ‚îÄ partie-07/                # Post-training et alignement
‚îú‚îÄ‚îÄ partie-08/                # Outils et agents
‚îú‚îÄ‚îÄ partie-09/                # Inference et optimisation
‚îú‚îÄ‚îÄ partie-10/                # D√©ploiement et LLMOps
‚îú‚îÄ‚îÄ partie-11/                # √âtude de cas fil rouge
‚îú‚îÄ‚îÄ partie-12/                # Annexes
‚îú‚îÄ‚îÄ labs/                     # Exercices et notebooks
‚îî‚îÄ‚îÄ assets/                   # Images et ressources
```

---

## Licence

Ce livre est mis √† disposition √† des fins √©ducatives. Tous droits r√©serv√©s.

---

## Contributions et feedback

Pour signaler des erreurs, sugg√©rer des am√©liorations ou poser des questions, veuillez ouvrir une issue sur le d√©p√¥t GitHub.

---

**Bonne lecture et bon apprentissage !** üöÄ

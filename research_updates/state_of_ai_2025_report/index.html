<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>State of Applied AI in 2025</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="progress-bar" id="progress"></div>

    <div class="presentation" id="presentation">

        <!-- SLIDE 1: Title -->
        <div class="slide active slide-title">
            <h1>State of Applied AI<br><span class="muted" style="font-size: 0.7em;">in 2025</span></h1>
            <p class="subtitle">2025 Trends, Applied AI Challenges, and What to Look Forward to in 2026</p>
        </div>
        
        <!-- SLIDE 2: The Big Question -->
        <div class="slide slide-statement">
            <h2>What was the real breakthrough of 2025?</h2>
        </div>
        
        <!-- SLIDE 3: Not What You Think -->
        <div class="slide slide-statement">
            <h2>It wasn't just new model releases.</h2>
            <p class="small">Models got better, but that wasn't what moved the needle for teams actually shipping AI.</p>
        </div>
        
        <!-- SLIDE 4: The Answer -->
        <div class="slide slide-statement">
            <h2>It was <span class="blue">plumbing</span>.</h2>
            <p class="small">Standards emerged. Integration got easier. The boring work of making agents actually work finally started paying off. The unglamorous infrastructure work became the competitive advantage.</p>
        </div>
        
        <!-- SLIDE 5: What This Means -->
        <div class="slide slide-statement">
            <h2>The teams that shipped weren't the ones with the best models.</h2>
            <p class="small">They weren't stuck contemplating which model to use. They knew how to connect everything together:and that's what mattered.</p>
        </div>
        
        <!-- SLIDE 6: The Practitioner Reality -->
        <div class="slide slide-list">
            <h2>A Few Honest Lessons from 2025</h2>
            <ul>
                <li class="blue"><strong>Most of your time goes to integration.</strong> Not prompts, not model selection:connecting systems and handling edge cases.</li>
                <li class="green"><strong>Reliability beats capability.</strong> A predictable system is far better than something accurate but chaotic.</li>
                <li class="orange"><strong>The model is the easy part.</strong> The hard part is everything around it:context, tools, evaluation, deployment.</li>
                <li class="purple"><strong>Start narrower than you think.</strong> Build up to complex agents: making 10-step agents on day one only makes debugging harder.</li>
            </ul>
        </div>
        
        <!-- SLIDE 7: The Gap -->
        <div class="slide slide-vs">
            <div class="vs-box old">
                <span class="tag">What Most Teams Build</span>
                <h3>Impressive demos</h3>
                <p>Works in notebooks, fails in production. 95% never ship.</p>
            </div>
            <span class="vs-arrow">→</span>
            <div class="vs-box new">
                <span class="tag">What Actually Ships</span>
                <h3>Boring reliability</h3>
                <p>Predictable, observable, recoverable. Does less, works always.</p>
            </div>
        </div>
        
        <!-- SLIDE 9: Framework Visual -->
        <div class="slide slide-visual">
            <h2>The Applied AI Stack</h2>
            <svg class="framework-svg" viewBox="0 0 950 480" xmlns="http://www.w3.org/2000/svg" style="max-width: 1100px;">
                <g class="framework-layer">
                    <rect x="20" y="15" width="750" height="95" rx="8" fill="#3b82f6" opacity="0.15" stroke="#3b82f6" stroke-width="2"/>
                    <text x="30" y="38" fill="#3b82f6" font-weight="700" font-size="14">INPUT LAYER</text>
                    <rect x="30" y="50" width="170" height="50" rx="6" fill="#3b82f6"/><text x="115" y="80" text-anchor="middle" fill="white" font-size="11" font-weight="500">Multimodal Inputs</text>
                    <rect x="210" y="50" width="180" height="50" rx="6" fill="#3b82f6"/><text x="300" y="80" text-anchor="middle" fill="white" font-size="11" font-weight="500">Context Engineering</text>
                    <rect x="400" y="50" width="170" height="50" rx="6" fill="#3b82f6"/><text x="485" y="80" text-anchor="middle" fill="white" font-size="11" font-weight="500">Meta Prompting</text>
                    <rect x="580" y="50" width="180" height="50" rx="6" fill="#3b82f6"/><text x="670" y="80" text-anchor="middle" fill="white" font-size="10" font-weight="500">Auto Prompt Optimization</text>
                </g>
                <g class="framework-layer">
                    <rect x="20" y="120" width="750" height="95" rx="8" fill="#10b981" opacity="0.15" stroke="#10b981" stroke-width="2"/>
                    <text x="30" y="143" fill="#10b981" font-weight="700" font-size="14">DATA AND MODEL LAYER</text>
                    <rect x="30" y="155" width="115" height="50" rx="6" fill="#10b981"/><text x="87" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Foundation Models</text>
                    <rect x="155" y="155" width="110" height="50" rx="6" fill="#10b981"/><text x="210" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Long Context</text>
                    <rect x="275" y="155" width="90" height="50" rx="6" fill="#10b981"/><text x="320" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">RL + RLVR</text>
                    <rect x="375" y="155" width="110" height="50" rx="6" fill="#10b981"/><text x="430" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Fine-Tuning</text>
                    <rect x="495" y="155" width="130" height="50" rx="6" fill="#10b981"/><text x="560" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Hybrid Reasoning</text>
                    <rect x="635" y="155" width="125" height="50" rx="6" fill="#10b981"/><text x="697" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Quantization</text>
                </g>
                <g class="framework-layer">
                    <rect x="20" y="225" width="750" height="95" rx="8" fill="#f97316" opacity="0.15" stroke="#f97316" stroke-width="2"/>
                    <text x="30" y="248" fill="#f97316" font-weight="700" font-size="14">APPLICATION LAYER</text>
                    <rect x="30" y="260" width="140" height="50" rx="6" fill="#f97316"/><text x="100" y="290" text-anchor="middle" fill="white" font-size="11" font-weight="500">RAG</text>
                    <rect x="180" y="260" width="140" height="50" rx="6" fill="#f97316"/><text x="250" y="290" text-anchor="middle" fill="white" font-size="11" font-weight="500">Agents</text>
                    <rect x="330" y="260" width="140" height="50" rx="6" fill="#f97316"/><text x="400" y="290" text-anchor="middle" fill="white" font-size="11" font-weight="500">Tool Calling</text>
                    <rect x="480" y="260" width="140" height="50" rx="6" fill="#f97316"/><text x="550" y="290" text-anchor="middle" fill="white" font-size="11" font-weight="500">Skills</text>
                    <rect x="630" y="260" width="130" height="50" rx="6" fill="#f97316"/><text x="695" y="290" text-anchor="middle" fill="white" font-size="10" font-weight="500">Agentic Frameworks</text>
                </g>
                <g class="framework-layer">
                    <rect x="20" y="330" width="750" height="95" rx="8" fill="#8b5cf6" opacity="0.15" stroke="#8b5cf6" stroke-width="2"/>
                    <text x="30" y="353" fill="#8b5cf6" font-weight="700" font-size="14">OUTPUT LAYER</text>
                    <rect x="30" y="365" width="360" height="50" rx="6" fill="#8b5cf6"/><text x="210" y="395" text-anchor="middle" fill="white" font-size="11" font-weight="500">Evals</text>
                    <rect x="400" y="365" width="360" height="50" rx="6" fill="#8b5cf6"/><text x="580" y="395" text-anchor="middle" fill="white" font-size="11" font-weight="500">Production Monitoring</text>
                </g>
                <g class="framework-layer">
                    <rect x="790" y="15" width="145" height="410" rx="8" fill="#ef4444" opacity="0.15" stroke="#ef4444" stroke-width="2"/>
                    <text x="862" y="42" text-anchor="middle" fill="#ef4444" font-weight="700" font-size="13">CHALLENGES</text>
                    <rect x="800" y="55" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="80" text-anchor="middle" fill="white" font-size="10" font-weight="500">Hallucinations</text>
                    <rect x="800" y="100" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="125" text-anchor="middle" fill="white" font-size="10" font-weight="500">Inconsistent Reasoning</text>
                    <rect x="800" y="145" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="170" text-anchor="middle" fill="white" font-size="10" font-weight="500">Over-Autonomy</text>
                    <rect x="800" y="190" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="215" text-anchor="middle" fill="white" font-size="10" font-weight="500">Poor Tool Grounding</text>
                    <rect x="800" y="235" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="260" text-anchor="middle" fill="white" font-size="10" font-weight="500">Long Context Drift</text>
                    <rect x="800" y="280" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="305" text-anchor="middle" fill="white" font-size="10" font-weight="500">Retrieval Issues</text>
                    <rect x="800" y="325" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="350" text-anchor="middle" fill="white" font-size="10" font-weight="500">Multi-Agent Errors</text>
                    <rect x="800" y="370" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="395" text-anchor="middle" fill="white" font-size="10" font-weight="500">Debugging</text>
                </g>
            </svg>
            <p class="caption">Four layers of the stack, plus the challenges that cut across all of them</p>
        </div>
        
        <!-- SLIDE 10: Agenda -->
        <div class="slide slide-list">
            <h2>What We'll Cover</h2>
            <ul>
                <li class="blue"><strong>Input Layer</strong> : From prompts to context engineering, meta-prompting, and multimodal</li>
                <li class="green"><strong>Model Layer</strong> : Foundation models, long context, RLVR, fine-tuning, and hybrid reasoning</li>
                <li class="orange"><strong>Application Layer</strong> : Agents that actually ship, tool calling, and patterns that work</li>
                <li class="purple"><strong>Output Layer</strong> : Trust as engineering, reliability math, and security frameworks</li>
                <li class="red"><strong>What's Still Broken</strong> : Hallucinations, RAG stagnation, and the production gap</li>
                <li class="purple"><strong>Road Ahead</strong> : What 2026 looks like and how to prepare</li>
            </ul>
        </div>
        


        <!-- ============================================ -->
        <!-- SECTION 1: INPUT LAYER -->
        <!-- ============================================ -->

        <!-- SLIDE: Section Break -->
        <div class="slide slide-section">
            <p class="label blue">Section 1</p>
            <h2>Input Layer</h2>
            <p class="sub">From Prompt Craft to Context Engineering</p>
            <div class="framework-container">
                <svg class="framework-svg" viewBox="0 0 950 480" xmlns="http://www.w3.org/2000/svg" style="max-width: 1100px;">
                    <g class="framework-layer">
                        <rect x="20" y="15" width="750" height="95" rx="8" fill="#3b82f6" opacity="0.15" stroke="#3b82f6" stroke-width="2"/>
                        <text x="30" y="38" fill="#3b82f6" font-weight="700" font-size="14">INPUT LAYER</text>
                        <rect x="30" y="50" width="170" height="50" rx="6" fill="#3b82f6"/><text x="115" y="80" text-anchor="middle" fill="white" font-size="11" font-weight="500">Multimodal Inputs</text>
                        <rect x="210" y="50" width="180" height="50" rx="6" fill="#3b82f6"/><text x="300" y="80" text-anchor="middle" fill="white" font-size="11" font-weight="500">Context Engineering</text>
                        <rect x="400" y="50" width="170" height="50" rx="6" fill="#3b82f6"/><text x="485" y="80" text-anchor="middle" fill="white" font-size="11" font-weight="500">Meta Prompting</text>
                        <rect x="580" y="50" width="180" height="50" rx="6" fill="#3b82f6"/><text x="670" y="80" text-anchor="middle" fill="white" font-size="10" font-weight="500">Auto Prompt Optimization</text>
                    </g>
                    <rect class="highlight-ring active" x="15" y="10" width="760" height="105" rx="12"/>
                    <g class="framework-layer dimmed"><rect x="20" y="120" width="750" height="95" rx="8" fill="#10b981" opacity="0.15" stroke="#10b981" stroke-width="2"/><text x="30" y="143" fill="#10b981" font-weight="700" font-size="14">DATA AND MODEL LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="225" width="750" height="95" rx="8" fill="#f97316" opacity="0.15" stroke="#f97316" stroke-width="2"/><text x="30" y="248" fill="#f97316" font-weight="700" font-size="14">APPLICATION LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="330" width="750" height="95" rx="8" fill="#8b5cf6" opacity="0.15" stroke="#8b5cf6" stroke-width="2"/><text x="30" y="353" fill="#8b5cf6" font-weight="700" font-size="14">OUTPUT LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="790" y="15" width="145" height="410" rx="8" fill="#ef4444" opacity="0.15" stroke="#ef4444" stroke-width="2"/><text x="862" y="220" text-anchor="middle" fill="#ef4444" font-weight="700" font-size="13">CHALLENGES</text></g>
                </svg>
            </div>
        </div>

        <!-- SLIDE: Section Overview -->
        <div class="slide slide-list">
            <h2>What Changed in the Input Layer</h2>
            <ul>
                <li class="blue"><strong>Prompt engineering evolved.</strong> From brittle skill-based craft to automated optimization.</li>
                <li class="blue"><strong>Meta-prompting emerged.</strong> Models now generate and refine prompts automatically.</li>
                <li class="blue"><strong>Automatic prompt optimization.</strong> Tools that iterate and improve prompts without human intervention.</li>
                <li class="blue"><strong>Context engineering matters more.</strong> What you put in the prompt matters more than how you phrase it.</li>
                <li class="blue"><strong>Multimodal became table stakes.</strong> Images, audio, and video as inputs moved from experimental to expected.</li>
            </ul>
        </div>

        <!-- ============================================ -->
        <!-- PART 1: PROMPTING IN 2024 -->
        <!-- ============================================ -->

        <!-- SLIDE: Prompting in 2024 -->
        <div class="slide slide-statement">
            <span class="topic-tag blue">Prompting 2024</span>
            <h2>In 2024, prompting was a <span class="orange">craft</span>.</h2>
            <p class="small">Models were sensitive. Small changes in wording produced wildly different outputs. Prompt engineering was a skill that took months to master.</p>
        </div>

        <!-- SLIDE: 2024 Prompting Characteristics -->
        <div class="slide slide-cards">
            <span class="topic-tag blue">Prompting 2024</span>
            <h2>Prompting in 2024: A Fragile Art</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--orange);">
                    <h3>Brittle & Model-Specific</h3>
                    <p>Prompts that worked on GPT-4 failed on Claude. Minor updates broke production systems. Every model needed different phrasing.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>Skill-Based Techniques</h3>
                    <p>Chain-of-Thought, Tree-of-Thought, ReAct patterns. Researchers published papers on prompting techniques. It was a specialized skill.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Manual Iteration</h3>
                    <p>Teams spent weeks A/B testing prompts. Small word changes = big output differences. Prompt engineering was expensive and slow.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Famous Techniques -->
        <div class="slide slide-visual">
            <span class="topic-tag blue">Prompting 2024</span>
            <h2>Some Research Papers That Defined 2024</h2>
            <img src="images/Prompting_Techniques_2024.png" alt="Prompting Techniques: CoT, ToT, ReAct, Self-Consistency">
            <p class="caption">These techniques worked, but required expertise to implement correctly. Most teams struggled to replicate paper results.</p>
        </div>

        <!-- ============================================ -->
        <!-- PART 2: THE 2025 SHIFT -->
        <!-- ============================================ -->

        <!-- SLIDE: 2025 Shift Statement -->
        <div class="slide slide-statement">
            <span class="topic-tag blue">2025 Shift</span>
            <h2>Then models got <span class="green">smarter</span>.</h2>
            <p class="small">2025 models are less brittle. They understand intent better. Careful phrasing matters less. And we found ways to automate the optimization.</p>
        </div>

        <!-- SLIDE: What Changed -->
        <div class="slide slide-vs">
            <span class="topic-tag blue">2025 Shift</span>
            <div class="vs-box old">
                <span class="tag">2024 Approach</span>
                <h3>"How do I phrase this?"</h3>
                <p>Manually crafting prompts, testing variations, hoping it works across models</p>
            </div>
            <span class="vs-arrow">→</span>
            <div class="vs-box new">
                <span class="tag">2025 Approach</span>
                <h3>"Let the model write it"</h3>
                <p>Meta-prompting and automated optimization. Models generate better prompts than humans.</p>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- PART 3: META-PROMPTING -->
        <!-- ============================================ -->

        <!-- SLIDE: Meta-Prompting Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag blue">Meta-Prompting</span>
            <h2>What is <span class="blue">Meta-Prompting</span>?</h2>
            <p class="small">A meta-prompt instructs the model to create a good prompt based on your task description. Instead of writing prompts yourself, you describe what you want and the model generates an optimized prompt.</p>
        </div>

        <!-- SLIDE: Meta-Prompting How It Works -->
        <div class="slide slide-content">
            <span class="topic-tag blue">Meta-Prompting</span>
            <h2>Meta-Prompting: How It Works</h2>
            <div class="body">
                <div class="text">
                    <p><strong>The idea is simple:</strong> Use a prompt to generate prompts.</p>
                    <p>OpenAI's Playground uses meta-prompts behind the "Generate" button. You describe your task, and it creates a complete, optimized prompt.</p>
                    <p class="muted" style="font-size: 0.9rem; margin-top: 1rem;">The meta-prompt includes best practices:</p>
                    <ul>
                        <li>Understand the task objectives and constraints</li>
                        <li>Encourage reasoning before conclusions</li>
                        <li>Include high-quality examples with placeholders</li>
                        <li>Specify output format explicitly</li>
                        <li>Add edge cases and important notes</li>
                    </ul>
                </div>
                <div class="visual" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
                    <div class="equation" style="width: 100%;">
                        <div class="formula" style="font-size: 1.5rem;">Task Description → Meta-Prompt → Optimized Prompt</div>
                        <p class="explain">Models generate better prompts than most humans can write manually</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SLIDE: Meta-Prompting Example -->
        <div class="slide slide-two-col">
            <span class="topic-tag blue">Meta-Prompting</span>
            <h2>Meta-Prompting: Before & After</h2>
            <div class="cols">
                <div class="col">
                    <h3 class="red">What You Write</h3>
                    <div style="background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 16px; margin-top: 12px;">
                        <p style="font-family: monospace; font-size: 0.9rem; color: var(--text-muted);">"I need a prompt for sentiment analysis of customer reviews"</p>
                    </div>
                    <p style="font-size: 0.85rem; color: var(--text-dim); margin-top: 12px;">Just describe your task in plain language. No prompt engineering expertise required.</p>
                </div>
                <div class="col">
                    <h3 class="green">What the Model Generates</h3>
                    <div style="background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 16px; margin-top: 12px; font-size: 0.8rem;">
                        <p style="font-family: monospace; color: var(--text-muted); line-height: 1.5;">Analyze customer review sentiment.<br><br># Steps<br>1. Read the review carefully<br>2. Identify emotional indicators<br>3. Consider context and nuance<br>4. Classify as positive/negative/neutral<br><br># Output Format<br>JSON with sentiment and confidence score<br><br># Examples<br>[Detailed examples with edge cases...]</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SLIDE: OpenAI Meta-Prompt Structure -->
        <div class="slide slide-list">
            <span class="topic-tag blue">Meta-Prompting</span>
            <h2>What OpenAI's Meta-Prompt Does</h2>
            <ul>
                <li class="blue"><strong>Understands the task:</strong> Grasps objectives, requirements, constraints, and expected output.</li>
                <li class="green"><strong>Enforces reasoning order:</strong> Reasoning steps before conclusions. Never start examples with answers.</li>
                <li class="orange"><strong>Includes examples:</strong> High-quality examples with placeholders for complex elements.</li>
                <li class="purple"><strong>Specifies output format:</strong> Explicit length, syntax (JSON, markdown, etc.), structure.</li>
                <li class="blue"><strong>Preserves user content:</strong> Keeps any details, guidelines, or examples you provide.</li>
            </ul>
            <p class="subtext">Source: OpenAI Prompt Generation Guide — the meta-prompt behind their Playground's Generate button.</p>
        </div>

        <!-- SLIDE: Why Meta-Prompting Matters -->
        <div class="slide slide-cards">
            <span class="topic-tag blue">Meta-Prompting</span>
            <h2>Why Meta Prompting is Super Valuable</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>Faster Iteration</h3>
                    <p>Generate 10 prompt variations in seconds. Test all of them. Pick the winner. What took days now takes minutes.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>Best Practices Built-In</h3>
                    <p>Meta-prompts encode years of prompt engineering research. You get chain-of-thought, examples, and structure automatically.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Democratized Expertise</h3>
                    <p>You don't need to be a prompt engineer. Describe what you want in plain English. The model handles the craft.</p>
                </div>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- PART 4: AUTOMATIC PROMPT OPTIMIZATION -->
        <!-- ============================================ -->

        <!-- SLIDE: Auto Prompt Optimization Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag blue">Auto Optimization</span>
            <h2>Beyond meta-prompting: <span class="purple">Automatic Optimization</span></h2>
            <p class="small">Meta-prompting generates prompts. But what if you could automatically iterate and improve them based on actual performance? That's automatic prompt optimization.</p>
        </div>

        <!-- SLIDE: What is DSPy -->
        <div class="slide slide-statement">
            <span class="topic-tag blue">Auto Optimization</span>
            <h2>DSPy: <span class="purple">Automated A/B Testing</span> for Prompts</h2>
            <p class="small">Instead of manually tweaking prompts and hoping they work, DSPy automatically tries different variations, measures which ones perform best, and keeps the winners. It's like having a tireless intern who tests thousands of prompt variations for you.</p>
        </div>

        <!-- SLIDE: The Problem DSPy Solves -->
        <div class="slide slide-vs">
            <span class="topic-tag blue">Auto Optimization</span>
            <div class="vs-box old">
                <span class="tag">Manual Prompting</span>
                <h3>Guess and Check</h3>
                <p>Write a prompt. Test it. Doesn't work well? Tweak it. Test again. Repeat for hours. Still breaks on edge cases.</p>
            </div>
            <span class="vs-arrow">→</span>
            <div class="vs-box new">
                <span class="tag">DSPy</span>
                <h3>Automatic Optimization</h3>
                <p>Give examples of what "good" looks like. DSPy tries hundreds of prompt variations automatically and finds what works best.</p>
            </div>
        </div>

        <!-- SLIDE: How DSPy Optimization Works -->
        <div class="slide slide-visual">
            <span class="topic-tag blue">Auto Optimization</span>
            <h2>How DSPy Finds the Best Prompt</h2>
            <img src="images/dspy_process.png" alt="DSPy Optimization Process">
            <p class="caption">You provide task + data. DSPy generates prompt variations. The loop scores, selects best, and repeats until optimized.</p>
        </div>

        <!-- SLIDE: DSPy Example -->
        <div class="slide slide-two-col">
            <span class="topic-tag blue">Auto Optimization</span>
            <h2>DSPy in Action</h2>
            <div class="cols">
                <div class="col">
                    <h3 class="blue">What You Write</h3>
                    <div style="background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 16px; margin-top: 12px;">
                        <p style="font-family: monospace; font-size: 0.8rem; color: var(--text-muted); line-height: 1.6;">
                            <span style="color: var(--blue);"># Define: question in, answer out</span><br>
                            qa = dspy.ChainOfThought("question -> answer")<br><br>
                            <span style="color: var(--blue);"># Give 10-20 examples</span><br>
                            examples = [...]<br><br>
                            <span style="color: var(--blue);"># Let DSPy optimize</span><br>
                            optimized = dspy.compile(qa, examples)
                        </p>
                    </div>
                </div>
                <div class="col">
                    <h3 class="green">What DSPy Figures Out</h3>
                    <div style="background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 16px; margin-top: 12px;">
                        <p style="font-family: monospace; font-size: 0.8rem; color: var(--text-muted); line-height: 1.6;">
                            "Given the question, reason step-by-step. First identify the key concepts. Then consider relevant facts. Finally, synthesize into a clear answer. Format: <br><br>
                            Reasoning: [your reasoning]<br>
                            Answer: [concise answer]"
                        </p>
                    </div>
                    <p style="font-size: 0.85rem; color: var(--text-dim); margin-top: 12px;">DSPy discovered this works better than simpler prompts.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Why DSPy Matters -->
        <div class="slide slide-cards">
            <span class="topic-tag blue">Auto Optimization</span>
            <h2>Why This Matters</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>No More Prompt Guessing</h3>
                    <p>Stop spending hours tweaking wording. Give examples of what "good" looks like, and let the machine find the best way to ask for it.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>Gets Better Over Time</h3>
                    <p>Collected more examples? Re-run optimization. Found edge cases? Add them and re-compile. Your prompts improve as your data grows.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Works Across Models</h3>
                    <p>Switching from GPT-4 to Claude? Re-optimize with the same examples. DSPy finds what works best for each model automatically.</p>
                </div>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- PART 5: CONTEXT ENGINEERING -->
        <!-- ============================================ -->

        <!-- SLIDE: Context Engineering Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag blue">Context Engineering</span>
            <h2>Prompting skills matter. But <span class="blue">context</span> matters more.</h2>
            <p class="small">For agentic systems, the clever phrasing is less important than what information you provide. This is context engineering.</p>
        </div>

        <!-- SLIDE: Context Engineering Visual -->
        <div class="slide slide-visual">
            <span class="topic-tag blue">Context Engineering</span>
            <h2>Context Engineering: What Goes Into the Prompt</h2>
            <img src="images/context_engineering.png" alt="Context Engineering Diagram">
            <p class="caption">Source: <a href="https://x.com/toaboricua/status/1903478376592859633" target="_blank" style="color: #aaa;">@toaboricua on X</a></p>
        </div>

        <!-- SLIDE: Context Engineering Definition -->
        <div class="slide slide-content">
            <span class="topic-tag blue">Context Engineering</span>
            <h2>Context Engineering: The New Discipline</h2>
            <div class="body">
                <div class="text">
                    <p><strong>"The art and science of filling the context window with just the right information at each step."</strong></p>
                    <p>Not about clever phrasing — it's about what information the model needs and when it needs it.</p>
                    <p class="muted" style="font-size: 0.85rem; margin-top: 1rem;">Three types of context matter:</p>
                    <ul>
                        <li><strong>Instructions:</strong> Prompts, memories, examples</li>
                        <li><strong>Knowledge:</strong> Facts, retrieved information</li>
                        <li><strong>Tools:</strong> Feedback from tool calls and actions</li>
                    </ul>
                </div>
                <div class="visual" style="display: flex; flex-direction: column; align-items: center;">
                    <img src="https://blog.langchain.com/content/images/size/w1000/2025/07/image-1.png" alt="Context Engineering">
                    <p style="font-size: 0.7rem; color: #888; margin-top: 0.5rem;">Source: <a href="https://blog.langchain.com/context-engineering-for-agents/" target="_blank" style="color: #aaa;">LangChain Blog</a></p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Context Engineering Strategies -->
        <div class="slide slide-content">
            <span class="topic-tag blue">Context Engineering</span>
            <h2>Four Strategies for Managing Context</h2>
            <div class="body">
                <div class="text">
                    <ul>
                        <li><strong>Write:</strong> Save information outside the context window. Use scratchpads and memories to persist across sessions.</li>
                        <li><strong>Select:</strong> Pull only relevant context in. Use embeddings, knowledge graphs, and careful filtering.</li>
                        <li><strong>Compress:</strong> Reduce tokens through summarization and trimming. Prevent context overload.</li>
                        <li><strong>Isolate:</strong> Split context across multiple agents or sandboxed environments.</li>
                    </ul>
                    <p class="muted" style="font-size: 0.85rem; margin-top: 1rem;">The goal: give agents exactly what they need, nothing more.</p>
                </div>
                <div class="visual" style="display: flex; flex-direction: column; align-items: center;">
                    <img src="https://blog.langchain.com/content/images/size/w1000/2025/07/image.png" alt="Context Engineering Strategies">
                    <p style="font-size: 0.7rem; color: #888; margin-top: 0.5rem;">Source: <a href="https://blog.langchain.com/context-engineering-for-agents/" target="_blank" style="color: #aaa;">LangChain Blog</a></p>
                </div>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- PART 6: MULTIMODAL INPUTS -->
        <!-- (Placeholder for McKinsey content) -->
        <!-- ============================================ -->

        <!-- SLIDE: Multimodal Statement -->
        <div class="slide slide-statement">
            <span class="topic-tag blue">Multimodal</span>
            <h2>Text-only AI systems are <span class="red">legacy</span>.</h2>
            <p class="small">In 2024, processing images alongside text was a differentiator. In 2025, it's table stakes. Systems that only handle text are increasingly inadequate for real-world use cases.</p>
        </div>

        <!-- SLIDE: Multimodal Use Cases -->
        <div class="slide slide-cards">
            <span class="topic-tag blue">Multimodal</span>
            <h2>What Multimodal Inputs Enable</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>Customer Service</h3>
                    <p>User sends a screenshot of an error message with their complaint. The model sees both, understands the context, and provides a relevant solution. No more "please describe what you see."</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>Code & Development</h3>
                    <p>Share a photo of a whiteboard diagram and ask "implement this architecture." Upload a UI mockup and get working code. The model understands visual intent, not just text descriptions.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Document Processing</h3>
                    <p>Feed invoices, receipts, contracts — the model reads text, understands layout, interprets signatures and stamps. No need to extract text first; it sees the whole document.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Why Now -->
        <div class="slide slide-content">
            <span class="topic-tag blue">Multimodal</span>
            <h2>Why Multimodal Works Now</h2>
            <div class="body">
                <div class="text">
                    <p><strong>2024 models could see images. 2025 models understand them.</strong></p>
                    <p>The latest models (GPT-5.2, Claude Opus 4.5, Gemini 3) have native multimodal understanding — images, audio, and video are first-class inputs, not bolted-on features.</p>
                    <ul>
                        <li><strong>Better accuracy:</strong> Models reason about visual and text context together, reducing hallucinations</li>
                        <li><strong>Lower latency:</strong> No separate OCR or vision pipeline needed — one model handles everything</li>
                        <li><strong>Richer context:</strong> A picture is worth a thousand tokens of description you don't have to write</li>
                    </ul>
                </div>
                <div class="visual" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
                    <div class="equation" style="width: 100%;">
                        <div class="formula" style="font-size: 1.3rem;">Image + Text → Understanding</div>
                        <p class="explain">Not image-to-text + text-to-understanding anymore</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- INPUT LAYER TAKEAWAYS -->
        <!-- ============================================ -->

        <!-- SLIDE: Input Layer Takeaways -->
        <div class="slide slide-list">
            <h2 class="blue">Input Layer: Key Takeaways</h2>
            <ul>
                <li class="green"><strong>Let models write your prompts.</strong> Meta-prompting generates better prompts than manual crafting. Use it.</li>
                <li class="green"><strong>Automate prompt optimization.</strong> Tools like DSPy iterate faster than humans. Stop manual A/B testing.</li>
                <li class="green"><strong>Focus on context, not phrasing.</strong> What you put in the prompt matters more than how you say it.</li>
                <li class="green"><strong>Plan for multimodal now.</strong> If your AI system only handles text, you're building technical debt.</li>
            </ul>
        </div>



        <!-- ============================================ -->
        <!-- SECTION 2: MODEL LAYER -->
        <!-- ============================================ -->

        <!-- SLIDE: Section Break -->
        <div class="slide slide-section">
            <p class="label green">Section 2</p>
            <h2>Model & Data Layer</h2>
            <p class="sub">From "bigger is better" to "think before you speak"</p>
            <div class="framework-container">
                <svg class="framework-svg" viewBox="0 0 950 480" xmlns="http://www.w3.org/2000/svg" style="max-width: 1100px;">
                    <g class="framework-layer dimmed"><rect x="20" y="15" width="750" height="95" rx="8" fill="#3b82f6" opacity="0.15" stroke="#3b82f6" stroke-width="2"/><text x="30" y="38" fill="#3b82f6" font-weight="700" font-size="14">INPUT LAYER</text></g>
                    <g class="framework-layer">
                        <rect x="20" y="120" width="750" height="95" rx="8" fill="#10b981" opacity="0.15" stroke="#10b981" stroke-width="2"/>
                        <text x="30" y="143" fill="#10b981" font-weight="700" font-size="14">DATA AND MODEL LAYER</text>
                        <rect x="30" y="155" width="140" height="50" rx="6" fill="#10b981"/><text x="100" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">System 2 Reasoning</text>
                        <rect x="180" y="155" width="100" height="50" rx="6" fill="#10b981"/><text x="230" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">RLVR</text>
                        <rect x="290" y="155" width="120" height="50" rx="6" fill="#10b981"/><text x="350" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Long Context</text>
                        <rect x="420" y="155" width="120" height="50" rx="6" fill="#10b981"/><text x="480" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Quantization</text>
                        <rect x="550" y="155" width="210" height="50" rx="6" fill="#10b981"/><text x="655" y="185" text-anchor="middle" fill="white" font-size="10" font-weight="500">Fine-Tuning & Distillation</text>
                    </g>
                    <rect class="highlight-ring active" x="15" y="115" width="760" height="105" rx="12"/>
                    <g class="framework-layer dimmed"><rect x="20" y="225" width="750" height="95" rx="8" fill="#f97316" opacity="0.15" stroke="#f97316" stroke-width="2"/><text x="30" y="248" fill="#f97316" font-weight="700" font-size="14">APPLICATION LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="330" width="750" height="95" rx="8" fill="#8b5cf6" opacity="0.15" stroke="#8b5cf6" stroke-width="2"/><text x="30" y="353" fill="#8b5cf6" font-weight="700" font-size="14">OUTPUT LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="790" y="15" width="145" height="410" rx="8" fill="#ef4444" opacity="0.15" stroke="#ef4444" stroke-width="2"/><text x="862" y="220" text-anchor="middle" fill="#ef4444" font-weight="700" font-size="13">CHALLENGES</text></g>
                </svg>
            </div>
        </div>

        <!-- SLIDE: Section Overview -->
        <div class="slide slide-list">
            <h2>What Changed in the Model Layer</h2>
            <ul>
                <li class="green"><strong>Models learned to think.</strong> System 2 reasoning emerged: models that allocate compute dynamically based on problem difficulty.</li>
                <li class="green"><strong>RLVR changed training.</strong> Reinforcement Learning with Verifiable Rewards proved you can train reasoning without human labels.</li>
                <li class="green"><strong>Context windows hit 1M tokens.</strong> But effective use of long context requires more than just bigger windows.</li>
                <li class="green"><strong>Efficiency became a priority.</strong> Quantization and distillation made frontier capabilities accessible on consumer hardware.</li>
            </ul>
        </div>

        <!-- ============================================ -->
        <!-- SYSTEM 1 → SYSTEM 2 REASONING -->
        <!-- ============================================ -->

        <!-- SLIDE: System 2 Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag green">System 2 Reasoning</span>
            <h2>The biggest shift in 2025: models that <span class="green">think before they speak</span>.</h2>
            <p class="small">Instead of generating tokens as fast as possible, these models allocate more compute to harder problems. The result: dramatically better reasoning on complex tasks.</p>
        </div>

        <!-- SLIDE: System 1 vs System 2 -->
        <div class="slide slide-vs">
            <span class="topic-tag green">System 2 Reasoning</span>
            <div class="vs-box old">
                <span class="tag">System 1</span>
                <h3>Fast, Intuitive</h3>
                <p>Immediate responses. Pattern matching. Great for simple queries, but prone to confident errors on hard problems.</p>
            </div>
            <span class="vs-arrow">→</span>
            <div class="vs-box new">
                <span class="tag">System 2</span>
                <h3>Slow, Deliberate</h3>
                <p>Models allocate thinking time proportional to difficulty. More reliable on complex reasoning, but 3-5x slower.</p>
            </div>
        </div>

        <!-- SLIDE: Why System 2 Matters -->
        <div class="slide slide-cards">
            <span class="topic-tag green">System 2 Reasoning</span>
            <h2>Why System 2 Reasoning Matters</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>Dynamic Compute Allocation</h3>
                    <p>Simple questions get quick answers. Complex problems trigger extended reasoning chains. The model decides how hard to think based on the task.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>Visible Thinking Process</h3>
                    <p>You can see the model's reasoning in its "thinking" tokens. This makes debugging easier and helps identify where reasoning goes wrong.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Trade Speed for Accuracy</h3>
                    <p>For tasks where correctness matters more than latency—code generation, complex analysis, multi-step reasoning—the tradeoff is worth it.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Test-Time Compute -->
        <div class="slide slide-content" style="justify-content: center;">
            <span class="topic-tag green">System 2 Reasoning</span>
            <div class="equation" style="max-width: 850px; margin: 0 auto;">
                <p class="formula">Test-Time Compute = <span class="green">Thinking Time</span> × <span class="blue">Tokens</span></p>
                <p class="explain">The new scaling law: you can improve outputs by letting models think longer</p>
            </div>
            <div style="text-align: center; margin-top: 1.5rem; color: var(--text-muted); font-size: 1.05rem; max-width: 750px; margin-left: auto; margin-right: auto;">
                <p>2024's scaling law was about training compute. 2025's insight: <strong>inference compute matters too</strong>.</p>
                <p style="margin-top: 0.5rem;">Models can solve harder problems by spending more compute at inference time, not just at training time.</p>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- RLVR -->
        <!-- ============================================ -->

        <!-- SLIDE: RLHF Context -->
        <div class="slide slide-statement">
            <span class="topic-tag green">RLVR</span>
            <h2>2024 was the year of RLHF.</h2>
            <p class="small">Reinforcement Learning from Human Feedback. Humans rank model outputs. The model learns what humans prefer. This gave us helpful, harmless assistants—but it doesn't scale, and "sounds good" isn't the same as "is correct."</p>
        </div>

        <!-- SLIDE: RLVR Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag green">RLVR</span>
            <h2>2025 introduced RLVR: rewards you can verify automatically.</h2>
            <p class="small">Reinforcement Learning with Verifiable Rewards. Give the model problems with checkable answers—math proofs, code that compiles, logic puzzles. Tell it only right or wrong. No human labelers needed. Scales with compute, not headcount.</p>
        </div>

        <!-- SLIDE: RLHF vs RLVR Visual -->
        <div class="slide slide-visual">
            <span class="topic-tag green">RLVR</span>
            <h2>RLHF vs RLVR: The Key Difference</h2>
            <img src="images/rlhf_rlvr.png" alt="RLHF vs RLVR Comparison">
            <p class="caption">RLHF asks "which sounds better?" RLVR asks "is this correct?" One requires humans. One requires only a verifier.</p>
        </div>

        <!-- SLIDE: RLVR Insight -->
        <div class="slide slide-statement">
            <span class="topic-tag green">RLVR</span>
            <h2>RLVR compresses search into intuition.</h2>
            <p class="small">What looks like "reasoning" is actually learned search patterns. The model isn't thinking step-by-step—it's pattern matching on solution strategies it learned during training.</p>
        </div>

        <!-- SLIDE: Self-Correction -->
        <div class="slide slide-content">
            <span class="topic-tag green">RLVR</span>
            <h2>The Self-Correction Breakthrough</h2>
            <div class="body">
                <div class="text">
                    <p>RLVR-trained models learned something unexpected: <strong>how to catch and correct their own mistakes</strong>.</p>
                    <ul>
                        <li>Models detect when reasoning is going wrong</li>
                        <li>They backtrack and try different approaches</li>
                        <li>This emerged naturally from the training process</li>
                    </ul>
                    <p style="margin-top: 1rem;"><strong>The results:</strong></p>
                    <ul>
                        <li>40-60% fewer hallucinations in trained domains</li>
                        <li>Models express uncertainty instead of fabricating</li>
                        <li>Graceful degradation on hard problems</li>
                    </ul>
                </div>
                <div class="visual" style="flex-direction: column; gap: 16px;">
                    <div class="card" style="background: rgba(16, 185, 129, 0.1); border-color: rgba(16, 185, 129, 0.3); text-align: center; width: 100%;">
                        <p style="font-size: 0.85rem; color: var(--green); margin-bottom: 0.5rem;">RLVR excels at</p>
                        <p style="font-size: 1.05rem;">Code • Math • Logic • Structured Tasks</p>
                    </div>
                    <div class="card" style="background: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.3); text-align: center; width: 100%;">
                        <p style="font-size: 0.85rem; color: var(--red); margin-bottom: 0.5rem;">RLVR struggles with</p>
                        <p style="font-size: 1.05rem;">Creative Writing • Subjective Tasks</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- LONG CONTEXT -->
        <!-- ============================================ -->

        <!-- SLIDE: Long Context Intro -->
        <div class="slide slide-stat">
            <span class="topic-tag green">Long Context</span>
            <p class="num green">1M</p>
            <p class="label">tokens in a single context window</p>
            <p class="context">That's ~700 pages. Entire codebases. Full research papers with all citations. But there's a catch.</p>
        </div>

        <!-- SLIDE: Context Windows -->
        <div class="slide slide-cards">
            <span class="topic-tag green">Long Context</span>
            <h2>Context Windows Exploded in 2025</h2>
            <div class="cards-grid">
                <div class="card" style="text-align: center;">
                    <p class="num green">1M</p>
                    <p style="font-weight: 600;">Gemini 3 Pro</p>
                    <p style="font-size: 0.85rem; color: var(--muted);">~700 pages input</p>
                </div>
                <div class="card" style="text-align: center;">
                    <p class="num blue">400K</p>
                    <p style="font-weight: 600;">GPT-5.2</p>
                    <p style="font-size: 0.85rem; color: var(--muted);">~128K output cap</p>
                </div>
                <div class="card" style="text-align: center;">
                    <p class="num purple">200K</p>
                    <p style="font-weight: 600;">Claude Opus 4.5</p>
                    <p style="font-size: 0.85rem; color: var(--muted);">Up to 1M enterprise</p>
                </div>
            </div>
            <p class="subtext muted" style="text-align: center; margin-top: 1.5rem;">Entire codebases in context. Multi-document analysis without chunking. Complex reasoning across long dependencies.</p>
        </div>

        <!-- SLIDE: Effective vs Claimed -->
        <div class="slide slide-statement">
            <span class="topic-tag green">Long Context</span>
            <h2>Claimed context ≠ effective context.</h2>
            <p class="small">Models can accept 1M tokens. That doesn't mean they use them well. Information in the middle gets lost. Retrieval quality degrades with distance. Test your specific use case.</p>
        </div>

        <!-- SLIDE: Long Context Challenges -->
        <div class="slide slide-list">
            <span class="topic-tag green">Long Context</span>
            <h2>The Long Context Reality Check</h2>
            <ul>
                <li class="orange"><strong>"Lost in the middle" problem persists.</strong> Models remember beginnings and ends better than middles. Structure your context accordingly.</li>
                <li class="blue"><strong>Costs scale linearly.</strong> 10x more context = 10x higher cost. Strategic context management still matters.</li>
                <li class="purple"><strong>Latency increases.</strong> Longer context means slower first-token response. Plan for user experience.</li>
                <li class="green"><strong>Quality varies by model.</strong> Some models handle 1M well. Others degrade at 100K. Benchmark your specific tasks.</li>
            </ul>
        </div>

        <!-- ============================================ -->
        <!-- EFFICIENCY & QUANTIZATION -->
        <!-- ============================================ -->

        <!-- SLIDE: Efficiency Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag green">Efficiency</span>
            <h2>2025's hidden story: frontier capabilities on consumer hardware.</h2>
            <p class="small">Quantization, distillation, and mixture-of-experts made models 10x more accessible.</p>
        </div>

        <!-- SLIDE: Quantization -->
        <div class="slide slide-visual">
            <span class="topic-tag green">Efficiency</span>
            <h2>Quantization: Smaller Without Losing Quality</h2>
            <img src="images/quantization.png" alt="Quantization comparison showing 32-bit, 8-bit, and 4-bit models">
            <p class="caption">Reduce precision from 32-bit to 4-bit. Same model, 8x smaller, runs on consumer hardware. Quality loss is minimal for most production tasks.</p>
        </div>

        <!-- ============================================ -->
        <!-- FINE-TUNING & DISTILLATION -->
        <!-- ============================================ -->

        <!-- SLIDE: Fine-tuning Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag green">Fine-Tuning</span>
            <h2>Fine-tuning: training a model on your specific data.</h2>
            <p class="small">Take a general-purpose model. Train it further on domain-specific examples. The result: a model that speaks your industry's language, follows your formats, and understands your context—often matching larger models at a fraction of the cost.</p>
        </div>

        <!-- SLIDE: Fine-tuning in Practice -->
        <div class="slide slide-list">
            <span class="topic-tag green">Fine-Tuning</span>
            <h2>Where Fine-Tuning Made the Difference in 2025</h2>
            <ul>
                <li class="green"><strong>Healthcare:</strong> Harvard researchers fine-tuned smaller LLMs to scan medical records for social determinants of health—outperforming larger general models with less bias.</li>
                <li class="blue"><strong>Finance:</strong> Banks fine-tuned models on earnings reports and risk assessments using internal terminology that general models couldn't understand.</li>
                <li class="purple"><strong>Legal:</strong> LegiLM, fine-tuned for data privacy regulations, interprets compliance requirements that general models miss.</li>
                <li class="orange"><strong>Chemistry:</strong> LlaSMol, a Mistral-based model fine-tuned for molecular science, substantially outperformed non-fine-tuned models on chemistry tasks.</li>
            </ul>
        </div>

        <!-- SLIDE: Fine-tuning Guidance -->
        <div class="slide slide-statement">
            <span class="topic-tag green">Fine-Tuning</span>
            <h2>But always start with prompting. Fine-tune only when you have to.</h2>
            <p class="small">Prompting is faster to iterate, requires no training data, and works for most use cases. Fine-tune when you're running the same task at massive scale, need consistent output formats, or require domain knowledge the base model lacks.</p>
        </div>

        <!-- SLIDE: Distillation Trend -->
        <div class="slide slide-statement">
            <span class="topic-tag green">Distillation</span>
            <h2>Distillation became the default deployment strategy.</h2>
            <p class="small">Use a large model to generate training data. Train a smaller model on that data. Deploy the small model at 10x lower cost. This pattern—70B teacher to 7B student—drove most production cost optimizations in 2025.</p>
        </div>

        <!-- SLIDE: Domain Experts -->
        <div class="slide slide-cards">
            <span class="topic-tag green">Distillation</span>
            <h2>Where Domain-Specific Models Shine</h2>
            <div class="cards-grid two">
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>Healthcare</h3>
                    <p>Medical coding from clinical notes. Drug interaction checking. Radiology report generation. Anywhere regulatory precision matters.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>Legal</h3>
                    <p>Contract clause extraction. Case law research. Compliance document review. Tasks requiring jurisdiction-specific knowledge.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Finance</h3>
                    <p>Earnings call summarization. Risk factor analysis. Regulatory filing generation. Domain jargon and format requirements.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--orange);">
                    <h3>Code</h3>
                    <p>Repository-specific assistants. Internal API documentation. Company coding standards enforcement. Codebase-aware refactoring.</p>
                </div>
            </div>
            <p class="subtext muted" style="text-align: center; margin-top: 1rem;">The pattern: General models for exploration, specialized models for production.</p>
        </div>

        <!-- ============================================ -->
        <!-- TAKEAWAYS -->
        <!-- ============================================ -->

        <!-- SLIDE: Model Takeaways -->
        <div class="slide slide-list">
            <h2 class="green">Model Layer: Key Takeaways</h2>
            <ul>
                <li class="green"><strong>System 2 reasoning trades speed for accuracy.</strong> Use thinking models for complex tasks where correctness matters more than latency.</li>
                <li class="green"><strong>RLVR enables self-correction.</strong> Models trained with verifiable rewards catch their own mistakes on structured tasks.</li>
                <li class="green"><strong>Long context ≠ infinite context.</strong> Test effective context length for your use case. The middle gets lost.</li>
                <li class="green"><strong>Small + specialized beats large + general.</strong> Fine-tuned 7B often outperforms 70B at 10% the cost.</li>
            </ul>
        </div>



        <!-- ============================================ -->
        <!-- SECTION 3: APPLICATION LAYER -->
        <!-- ============================================ -->

        <!-- SLIDE: Section Break -->
        <div class="slide slide-section">
            <p class="label orange">Section 3</p>
            <h2>Application Layer</h2>
            <p class="sub">From "Which model?" to "Can it do real work?"</p>
            <div class="framework-container">
                <svg class="framework-svg" viewBox="0 0 950 480" xmlns="http://www.w3.org/2000/svg" style="max-width: 1100px;">
                    <g class="framework-layer dimmed"><rect x="20" y="15" width="750" height="95" rx="8" fill="#3b82f6" opacity="0.15" stroke="#3b82f6" stroke-width="2"/><text x="30" y="38" fill="#3b82f6" font-weight="700" font-size="14">INPUT LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="120" width="750" height="95" rx="8" fill="#10b981" opacity="0.15" stroke="#10b981" stroke-width="2"/><text x="30" y="143" fill="#10b981" font-weight="700" font-size="14">DATA AND MODEL LAYER</text></g>
                    <g class="framework-layer">
                        <rect x="20" y="225" width="750" height="95" rx="8" fill="#f97316" opacity="0.15" stroke="#f97316" stroke-width="2"/>
                        <text x="30" y="248" fill="#f97316" font-weight="700" font-size="14">APPLICATION LAYER</text>
                        <rect x="30" y="260" width="100" height="50" rx="6" fill="#f97316"/><text x="80" y="290" text-anchor="middle" fill="white" font-size="10" font-weight="500">RAG</text>
                        <rect x="140" y="260" width="100" height="50" rx="6" fill="#f97316"/><text x="190" y="290" text-anchor="middle" fill="white" font-size="10" font-weight="500">Agents</text>
                        <rect x="250" y="260" width="100" height="50" rx="6" fill="#f97316"/><text x="300" y="290" text-anchor="middle" fill="white" font-size="10" font-weight="500">Tool Calling</text>
                        <rect x="360" y="260" width="100" height="50" rx="6" fill="#f97316"/><text x="410" y="290" text-anchor="middle" fill="white" font-size="10" font-weight="500">Skills</text>
                        <rect x="470" y="260" width="140" height="50" rx="6" fill="#f97316"/><text x="540" y="290" text-anchor="middle" fill="white" font-size="10" font-weight="500">Agentic Frameworks</text>
                        <rect x="620" y="260" width="140" height="50" rx="6" fill="#f97316"/><text x="690" y="290" text-anchor="middle" fill="white" font-size="10" font-weight="500">Standards (MCP/A2A)</text>
                    </g>
                    <rect class="highlight-ring active" x="15" y="220" width="760" height="105" rx="12"/>
                    <g class="framework-layer dimmed"><rect x="20" y="330" width="750" height="95" rx="8" fill="#8b5cf6" opacity="0.15" stroke="#8b5cf6" stroke-width="2"/><text x="30" y="353" fill="#8b5cf6" font-weight="700" font-size="14">OUTPUT LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="790" y="15" width="145" height="410" rx="8" fill="#ef4444" opacity="0.15" stroke="#ef4444" stroke-width="2"/><text x="862" y="220" text-anchor="middle" fill="#ef4444" font-weight="700" font-size="13">CHALLENGES</text></g>
                </svg>
            </div>
        </div>

        <!-- SLIDE: Section Overview -->
        <div class="slide slide-list">
            <h2>What Changed in the Application Layer</h2>
            <ul>
                <li class="orange"><strong>Delegation replaced answers.</strong> Book the meeting, pull the numbers, draft the doc. The success metric moved from good responses to completed outcomes.</li>
                <li class="orange"><strong>RAG matured into a real subsystem.</strong> Naive "add embeddings and pray" died. Hybrid retrieval, reranking, and agentic workflows became the norm.</li>
                <li class="orange"><strong>Agent types diversified.</strong> Deep agents for long-horizon work. Ambient agents for always-on automation. Browser agents for UI control.</li>
                <li class="orange"><strong>Standards emerged from protocol wars.</strong> MCP and A2A moved to open governance. Fragmentation started consolidating.</li>
            </ul>
        </div>

        <!-- SLIDE: The Shift -->
        <div class="slide slide-statement">
            <h2>People wanted delegation, not answers.</h2>
            <p class="small">Book the meeting. Pull the numbers. Draft the doc. Update the ticket. Reconcile the spreadsheet. The success metric moved from "a good response" to "a completed outcome."</p>
        </div>

        <!-- ============================================ -->
        <!-- RAG IN 2025 -->
        <!-- ============================================ -->

        <!-- SLIDE: RAG Didn't Die -->
        <div class="slide slide-statement">
            <span class="topic-tag orange">RAG</span>
            <h2>RAG didn't die. Naive RAG did.</h2>
            <p class="small">The shift was from "add embeddings and pray" to "engineer retrieval like a real subsystem." What showed up in practice: hybrid retrieval, serious chunking strategies, metadata filtering, and access control.</p>
        </div>

        <!-- SLIDE: What Changed in RAG -->
        <div class="slide slide-cards">
            <span class="topic-tag orange">RAG</span>
            <h2>How RAG Matured in 2025</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>Hybrid Retrieval Became Normal</h3>
                    <p>Sparse retrieval (BM25) for exact matches. Dense embeddings for semantic matches. Reranking to pick the final context. Better retrieval wasn't just better embeddings—it was better input to retrieval.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--orange);">
                    <h3>RAG Became a Workflow, Not a Single Call</h3>
                    <p>The system rewrites the query, does multi-step retrieval, picks sources, verifies results, then answers. One-pass retrieval fails silently. The workflow approach makes failure visible and recoverable.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Graph-Based Retrieval Got Attention</h3>
                    <p>Traditional RAG is good at "find me the paragraph." It struggles at "summarize the themes" or "how do these concepts relate." GraphRAG builds structure over your data for corpus-level questions.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Long Context + RAG -->
        <div class="slide slide-statement">
            <span class="topic-tag orange">RAG</span>
            <h2>Long context is a bigger desk. RAG is choosing the right papers to put on it.</h2>
            <p class="small">Long context reduces complexity for small, stable datasets. But enterprise needs—cost control, freshness, access control, auditability—kept retrieval relevant. Most teams landed on hybrid: use long context as a reasoning workspace, but still retrieve curated slices into it.</p>
        </div>

        <!-- SLIDE: Why RAG Stayed -->
        <div class="slide slide-list">
            <span class="topic-tag orange">RAG</span>
            <h2>Why Retrieval Stayed Relevant</h2>
            <ul>
                <li class="blue"><strong>Cost control.</strong> Passing huge context is expensive and often wasteful. Retrieval lets you pay only for what you need.</li>
                <li class="green"><strong>Freshness.</strong> If data changes daily, you don't want to keep repacking massive context. Fetch what's current.</li>
                <li class="purple"><strong>Access control.</strong> "Put it all in the prompt" breaks down when different users have different permissions.</li>
                <li class="orange"><strong>Auditability.</strong> Retrieval makes it easier to show what sources were used and why.</li>
            </ul>
        </div>

        <!-- ============================================ -->
        <!-- AGENT TYPES -->
        <!-- ============================================ -->

        <!-- SLIDE: Agent Types Intro -->
        <div class="slide slide-statement">
            <span class="topic-tag orange">Agents</span>
            <h2>2025 was the year agent work split into distinct categories.</h2>
            <p class="small">"Agent" stopped meaning "LLM that can call a tool" and started meaning "a system that can complete work across many steps, over time, with integration, and with guardrails."</p>
        </div>

        <!-- SLIDE: Deep Agents -->
        <div class="slide slide-content">
            <span class="topic-tag orange">Agents</span>
            <h2>Deep Agents: Long-Horizon Work</h2>
            <div class="body">
                <div class="text">
                    <p>Deep agents handle tasks that take minutes to hours, with many steps, context management, and delegation.</p>
                    <p style="margin-top: 1rem;"><strong>What they do:</strong></p>
                    <ul>
                        <li>Hold state and artifacts, not just chat history</li>
                        <li>Decompose goals into sub-tasks</li>
                        <li>Recover from failure, retry safely, continue</li>
                        <li>Use memory and filesystems to avoid context collapse</li>
                    </ul>
                    <p style="margin-top: 1rem;"><strong>Example:</strong> OpenAI shipped "deep research" as an agentic capability—multi-step research that browses and synthesizes, returning a report with citations.</p>
                </div>
                <div class="visual" style="flex-direction: column; gap: 16px; justify-content: center;">
                    <div class="card" style="background: rgba(249, 115, 22, 0.1); border-color: rgba(249, 115, 22, 0.3); text-align: center;">
                        <p style="font-weight: 700; color: var(--orange); font-size: 1.1rem;">A shallow agent answers.</p>
                        <p style="font-weight: 700; color: var(--orange); font-size: 1.1rem; margin-top: 0.5rem;">A deep agent produces.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SLIDE: Ambient Agents -->
        <div class="slide slide-content">
            <span class="topic-tag orange">Agents</span>
            <h2>Ambient Agents: Always-On Automation</h2>
            <div class="body">
                <div class="text">
                    <p>Ambient agents run in the background and act on events rather than waiting for prompts.</p>
                    <p style="margin-top: 1rem;"><strong>They respond to:</strong></p>
                    <ul>
                        <li>Event streams, logs, monitoring alerts</li>
                        <li>Tickets breaching SLA, churn signals spiking</li>
                        <li>Build failures, incident starts, contract renewals</li>
                    </ul>
                    <p style="margin-top: 1rem;"><strong>Core ingredients:</strong></p>
                    <ul>
                        <li>Triggers: event streams, schedules, webhooks</li>
                        <li>Policies: what it can do automatically vs. what needs approval</li>
                        <li>Memory of ongoing state: what's already handled</li>
                    </ul>
                </div>
                <div class="visual" style="flex-direction: column; gap: 16px; justify-content: center;">
                    <div class="card" style="background: rgba(139, 92, 246, 0.1); border-color: rgba(139, 92, 246, 0.3); text-align: center;">
                        <p style="font-size: 0.9rem; color: var(--purple); margin-bottom: 0.5rem;">Where they win</p>
                        <p style="font-size: 1rem;">IT ops triage • Security alert routing • SLA management • Compliance checks</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SLIDE: Coding Agents -->
        <div class="slide slide-statement">
            <span class="topic-tag orange">Agents</span>
            <h2>Coding agents became the first place many teams experienced real agents.</h2>
            <p class="small">Why? Software work has the perfect control surface: repos, tests, CI, and pull requests. Every step is visible. Humans steer via PR review. Governance is built in.</p>
        </div>

        <!-- SLIDE: Coding Agent Types -->
        <div class="slide slide-cards">
            <span class="topic-tag orange">Agents</span>
            <h2>Three Surfaces for Coding Agents</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>IDE Agents</h3>
                    <p>Multi-step work inside your editor. Finds relevant files, proposes edits across multiple files, runs terminal commands and tests, watches errors, fixes and retries in a loop. Cursor and Copilot agent mode run multiple agents in parallel.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>Repo & PR Agents</h3>
                    <p>Work through issues and pull requests in a GitHub Actions environment. Produce PRs, logs, and reviewable commits. This is where autonomy becomes governable—every step visible in commits, humans steer via PR review.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Terminal Agents</h3>
                    <p>Agentic coding from the command line. Understands your codebase, helps with routine tasks and git workflows through natural language. Works with existing bash environment and can act as both MCP server and client.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Browser Agents -->
        <div class="slide slide-content">
            <span class="topic-tag orange">Agents</span>
            <h2>Browser Agents: UI Control</h2>
            <div class="body">
                <div class="text">
                    <p>Agents that operate the real surface area people use: browsers and SaaS UIs.</p>
                    <p style="margin-top: 1rem;"><strong>OpenAI Operator</strong> (January 2025)</p>
                    <ul>
                        <li>Uses screenshots to "see" and virtual mouse/keyboard to interact</li>
                        <li>Books reservations, fills forms, places orders</li>
                        <li>Later integrated into ChatGPT as "agent mode"</li>
                    </ul>
                    <p style="margin-top: 1rem;"><strong>Google Project Mariner</strong></p>
                    <ul>
                        <li>Chrome extension with sidebar interface</li>
                        <li>Runs up to 10 parallel task streams</li>
                        <li>"Teach & Repeat" learns demonstrated workflows</li>
                    </ul>
                </div>
                <div class="visual" style="flex-direction: column; gap: 16px; justify-content: center;">
                    <div class="card" style="background: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.3); text-align: center;">
                        <p style="font-size: 0.9rem; color: var(--red); margin-bottom: 0.5rem;">The risk</p>
                        <p style="font-size: 1rem;">UI brittleness, broad access requirements, irreversible actions. Require approvals for anything permanent.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SLIDE: Multi-Agent -->
        <div class="slide slide-two-col">
            <span class="topic-tag orange">Agents</span>
            <h2>Multi-Agent: When It Helps, When It Hurts</h2>
            <div class="cols">
                <div class="col">
                    <h3 class="green">When It Helps</h3>
                    <ul>
                        <li><strong>Parallel research:</strong> Multiple agents gather evidence simultaneously, then consolidate</li>
                        <li><strong>Role separation:</strong> Planner, executor, reviewer as distinct agents</li>
                        <li><strong>Tool specialization:</strong> Different agents with different permissions or domains</li>
                    </ul>
                </div>
                <div class="col">
                    <h3 class="red">When It Hurts</h3>
                    <ul>
                        <li><strong>Non-determinism:</strong> Outcomes vary more with multiple agents</li>
                        <li><strong>Coordination overhead:</strong> Agents disagree or duplicate work</li>
                        <li><strong>Error amplification:</strong> One agent's wrong assumption spreads</li>
                        <li><strong>Cost:</strong> You pay for parallel runs</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- SKILLS -->
        <!-- ============================================ -->

        <!-- SLIDE: Skills Intro -->
        <div class="slide slide-two-col">
            <span class="topic-tag orange">Skills</span>
            <h2>Skills: Packaged Expertise for Agents</h2>
            <div class="cols">
                <div class="col">
                    <h3 class="orange">Agents execute.</h3>
                    <p>They reason, call tools, handle multi-step workflows. But agents are general-purpose—they don't inherently know your domain.</p>
                </div>
                <div class="col">
                    <h3 class="blue">Skills equip.</h3>
                    <p>Folders of instructions, scripts, and resources that agents load on demand. BigQuery queries. NDA review procedures. PDF extraction. Domain expertise, packaged and portable.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Skills Visual -->
        <div class="slide slide-visual">
            <span class="topic-tag orange">Skills</span>
            <h2>How Skills Fit Into Agent Architecture</h2>
            <img src="images/skills_architecture.webp" alt="Agent + Skills + Virtual Machine">
            <p class="caption">Agent configuration includes equipped skills and MCP servers. Skills live as directories in the agent's file system—loaded when relevant. Source: Anthropic</p>
        </div>

        <!-- ============================================ -->
        <!-- AGENTIC FRAMEWORKS -->
        <!-- ============================================ -->

        <!-- SLIDE: Agentic Frameworks -->
        <div class="slide slide-statement">
            <span class="topic-tag orange">Frameworks</span>
            <h2>Frameworks moved from "loops" to explicit workflows.</h2>
            <p class="small">Early agent setups: plan, call tool, read output, repeat. In 2025, frameworks moved toward explicit graphs with named steps, clear transitions, and predictable control flow. Debugging a loop is painful. Debugging a workflow is tractable.</p>
        </div>

        <!-- SLIDE: What Changed in Frameworks -->
        <div class="slide slide-cards">
            <span class="topic-tag orange">Frameworks</span>
            <h2>What Changed in Agentic Frameworks</h2>
            <div class="cards-grid two">
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>State Became First-Class</h3>
                    <p>Checkpointing progress so you can resume after failures. Persisting intermediate artifacts. Supporting "pause and wait" for humans or slow systems. Real tasks span hours or days.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>Human-in-Loop: Designed In</h3>
                    <p>Approval gates, escalation paths, and fallbacks became built-in concepts. Better support for partial automation. Enterprises don't want full autonomy—they want safe delegation.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Tooling Got Structured</h3>
                    <p>Clear contracts for tools: inputs, outputs, errors. Better handling of failures and partial results. Moves toward portable tool ecosystems so apps don't rewrite integrations.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--orange);">
                    <h3>Multi-Agent: Supported, Not Default</h3>
                    <p>Better coordination primitives, message passing, role boundaries. More emphasis on controlling non-determinism. Multi-agent as an advanced pattern, not the starting point.</p>
                </div>
            </div>
        </div>

        <!-- ============================================ -->
        <!-- STANDARDS -->
        <!-- ============================================ -->

        <!-- SLIDE: Why Standards Matter -->
        <div class="slide slide-statement">
            <span class="topic-tag orange">Standards</span>
            <h2>2025 was the year everyone realized fragmentation would kill adoption.</h2>
            <p class="small">Two different interoperability problems emerged. How do AI apps talk to tools and data sources? How do agents coordinate with other agents? The industry started racing toward standard rails.</p>
        </div>

        <!-- SLIDE: MCP and A2A -->
        <div class="slide slide-two-col">
            <span class="topic-tag orange">Standards</span>
            <h2>Two Standards, Two Problems</h2>
            <div class="cols">
                <div class="col">
                    <h3 class="blue">MCP: Tool Connectivity</h3>
                    <p>A universal connector contract so AI apps can talk to tools and data sources consistently.</p>
                    <ul>
                        <li>Standardized tool descriptions</li>
                        <li>Any agent can understand any MCP-compatible tool</li>
                        <li>Reduces integration cost over time</li>
                    </ul>
                </div>
                <div class="col">
                    <h3 class="purple">A2A: Agent-to-Agent</h3>
                    <p>A protocol for agents coordinating with other agents across systems.</p>
                    <ul>
                        <li>Different layer than MCP</li>
                        <li>Enables multi-vendor agent collaboration</li>
                        <li>Critical for complex enterprise workflows</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- SLIDE: Protocol Wars Timeline -->
        <div class="slide slide-list">
            <span class="topic-tag orange">Standards</span>
            <h2>The Protocol Wars: From Fragmentation to Open Governance</h2>
            <ul>
                <li class="blue"><strong>April 2025:</strong> Google announces Agent2Agent (A2A) protocol. The agent-to-agent communication problem gets a proposed standard.</li>
                <li class="green"><strong>June 2025:</strong> Linux Foundation launches Agent2Agent protocol project. Microsoft publicly backs the open protocol.</li>
                <li class="purple"><strong>December 2025:</strong> Anthropic donates MCP to Linux Foundation's "Agentic AI Foundation." Positioned explicitly around neutrality and open governance.</li>
                <li class="orange"><strong>The pattern:</strong> Corporate-controlled APIs lose to open standards. Governance determines whether a standard becomes real infrastructure.</li>
            </ul>
        </div>

        <!-- SLIDE: Why This Matters -->
        <div class="slide slide-statement">
            <span class="topic-tag orange">Standards</span>
            <h2>Standards reduce vendor lock-in. Standards reduce integration cost. Open governance wins.</h2>
            <p class="small">The move to Linux Foundation wasn't just about technology. It was about trust. Enterprises adopt standards they can rely on outlasting any single vendor's strategy.</p>
        </div>

        <!-- ============================================ -->
        <!-- TAKEAWAYS -->
        <!-- ============================================ -->

        <!-- SLIDE: Application Takeaways -->
        <div class="slide slide-list">
            <h2 class="orange">Application Layer: Key Takeaways</h2>
            <ul>
                <li class="orange"><strong>RAG survived because enterprises need grounding, freshness, and control.</strong> What changed: RAG became an engineered system, not a trick.</li>
                <li class="orange"><strong>Agent types diversified.</strong> Deep for long-horizon work, ambient for always-on automation, browser for UI control, coding as the breakthrough success.</li>
                <li class="orange"><strong>Skills and frameworks matured.</strong> Reusable capabilities, explicit workflows, state management, and human-in-loop by design.</li>
                <li class="orange"><strong>Standards moved to open governance.</strong> MCP and A2A under Linux Foundation. The protocol wars are consolidating.</li>
            </ul>
        </div>



        <!-- ============================================ -->
        <!-- SECTION 4: OUTPUT LAYER - Evals & Monitoring -->
        <!-- ============================================ -->

        <!-- SLIDE: Section Break -->
        <div class="slide slide-section">
            <p class="label purple">Section 4</p>
            <h2>Output Layer</h2>
            <p class="sub">Evals & Production Monitoring</p>
            <div class="framework-container">
                <svg class="framework-svg" viewBox="0 0 950 480" xmlns="http://www.w3.org/2000/svg" style="max-width: 1100px;">
                    <g class="framework-layer dimmed"><rect x="20" y="15" width="750" height="95" rx="8" fill="#3b82f6" opacity="0.15" stroke="#3b82f6" stroke-width="2"/><text x="30" y="38" fill="#3b82f6" font-weight="700" font-size="14">INPUT LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="120" width="750" height="95" rx="8" fill="#10b981" opacity="0.15" stroke="#10b981" stroke-width="2"/><text x="30" y="143" fill="#10b981" font-weight="700" font-size="14">DATA AND MODEL LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="225" width="750" height="95" rx="8" fill="#f97316" opacity="0.15" stroke="#f97316" stroke-width="2"/><text x="30" y="248" fill="#f97316" font-weight="700" font-size="14">APPLICATION LAYER</text></g>
                    <g class="framework-layer">
                        <rect x="20" y="330" width="750" height="95" rx="8" fill="#8b5cf6" opacity="0.15" stroke="#8b5cf6" stroke-width="2"/>
                        <text x="30" y="353" fill="#8b5cf6" font-weight="700" font-size="14">OUTPUT LAYER</text>
                        <rect x="30" y="365" width="360" height="50" rx="6" fill="#8b5cf6"/><text x="210" y="395" text-anchor="middle" fill="white" font-size="11" font-weight="500">Evals</text>
                        <rect x="400" y="365" width="360" height="50" rx="6" fill="#8b5cf6"/><text x="580" y="395" text-anchor="middle" fill="white" font-size="11" font-weight="500">Production Monitoring</text>
                    </g>
                    <rect class="highlight-ring active" x="15" y="325" width="760" height="105" rx="12"/>
                    <g class="framework-layer dimmed"><rect x="790" y="15" width="145" height="410" rx="8" fill="#ef4444" opacity="0.15" stroke="#ef4444" stroke-width="2"/><text x="862" y="220" text-anchor="middle" fill="#ef4444" font-weight="700" font-size="13">CHALLENGES</text></g>
                </svg>
            </div>
        </div>

        <!-- SLIDE: Section Overview -->
        <div class="slide slide-list">
            <h2>What's in the Output Layer</h2>
            <ul>
                <li class="purple"><strong>Evals became the bottleneck.</strong> Teams that shipped fast had evals running before features, not after.</li>
                <li class="purple"><strong>Model evals vs product evals.</strong> Benchmarks tell you about the model. Production metrics tell you about your system.</li>
                <li class="purple"><strong>Agent evaluation got structured.</strong> Four buckets emerged for evaluating agentic systems systematically.</li>
                <li class="purple"><strong>Monitoring closed the loop.</strong> Real-time signals feeding back into development. The flywheel that compounds quality.</li>
            </ul>
        </div>

        <!-- SLIDE: Quality Verification Bottleneck -->
        <div class="slide slide-statement">
            <span class="topic-tag purple">Quality Verification</span>
            <h2>Quality verification became the bottleneck for shipping AI.</h2>
        </div>

        <!-- SLIDE: Model vs Product Evals -->
        <div class="slide slide-visual">
            <span class="topic-tag purple">Evals</span>
            <h2>Model Evals vs Product Evals</h2>
            <img src="images/model_vs_product_evaluation.png" alt="Model Evals vs Product Evals">
            <p class="caption">Model evals test the underlying model (benchmarks, capabilities). Product evals test your system (task completion, user success). Both matter—but product evals determine if you ship.</p>
        </div>

        <!-- SLIDE: Evals vs Monitoring -->
        <div class="slide slide-vs">
            <span class="topic-tag purple">Evals</span>
            <div class="vs-box" style="background: rgba(139, 92, 246, 0.1); border-color: rgba(139, 92, 246, 0.3);">
                <span class="tag" style="color: var(--purple);">Evals</span>
                <h3>Offline Quality Assurance</h3>
                <p>Run before deployment. Test against known datasets. Catch regressions before users see them. Answer: "Is this change safe to ship?"</p>
            </div>
            <span class="vs-arrow">→</span>
            <div class="vs-box" style="background: rgba(16, 185, 129, 0.1); border-color: rgba(16, 185, 129, 0.3);">
                <span class="tag" style="color: var(--green);">Monitoring</span>
                <h3>Online Quality Assurance</h3>
                <p>Run in production. Track real user interactions. Catch issues evals missed. Answer: "Is this working for actual users?"</p>
            </div>
        </div>

        <!-- SLIDE: Online vs Offline Timing -->
        <div class="slide slide-visual">
            <span class="topic-tag purple">Evals</span>
            <h2>Evals vs Monitoring: When They Run</h2>
            <img src="images/online_vs_offline_timing.png" alt="Online vs Offline Timing">
            <p class="caption">Evals run offline before deployment. Monitoring runs online in production. Both essential—different timing, different purpose.</p>
        </div>

        <!-- SLIDE: Eval Mental Model -->
        <div class="slide slide-statement">
            <span class="topic-tag purple">Evals</span>
            <h2>The eval mental model: What are you actually testing?</h2>
            <p class="small">Every eval answers one of three questions. (1) Can it do the task at all? Capability. (2) Does it still do the task after changes? Regression. (3) Does it do the task the way we want? Alignment. Know which question you're asking.</p>
        </div>

        <!-- SLIDE: Agent Evaluation Buckets -->
        <div class="slide slide-cards">
            <span class="topic-tag purple">Evals</span>
            <h2>Four Buckets for Agent Evaluation</h2>
            <div class="cards-grid two">
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>1. Task Completion</h3>
                    <p>Did the agent achieve the goal? Binary success/failure on well-defined objectives. The baseline metric.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>2. Trajectory Quality</h3>
                    <p>How did it get there? Efficient tool use, sensible step ordering, recovery from errors. The path matters.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--orange);">
                    <h3>3. Safety & Boundaries</h3>
                    <p>Did it stay in bounds? No unauthorized actions, proper escalation, respecting guardrails. Trust requires limits.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>4. Resource Efficiency</h3>
                    <p>What did it cost? Tokens consumed, API calls made, time elapsed. Efficiency at scale.</p>
                </div>
            </div>
        </div>

        <!-- SLIDE: Grader Stack -->
        <div class="slide slide-visual">
            <span class="topic-tag purple">Evals</span>
            <h2>The Grader Stack: Who Evaluates?</h2>
            <img src="images/three_evaluation_approaches.png" alt="Three Evaluation Approaches">
            <p class="caption">Deterministic checks first (fast, cheap). LLM-as-judge for scale (the 2025 breakthrough). Human review for calibration and edge cases.</p>
        </div>

        <!-- SLIDE: Capability vs Regression -->
        <div class="slide slide-vs">
            <span class="topic-tag purple">Evals</span>
            <div class="vs-box" style="background: rgba(59, 130, 246, 0.1); border-color: rgba(59, 130, 246, 0.3);">
                <span class="tag" style="color: var(--blue);">Capability Evals</span>
                <h3>"Can it do new things?"</h3>
                <p>Testing new features. Expanding to new domains. Pushing boundaries. Run when adding capabilities.</p>
            </div>
            <span class="vs-arrow">↔</span>
            <div class="vs-box" style="background: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.3);">
                <span class="tag" style="color: var(--red);">Regression Evals</span>
                <h3>"Does it still work?"</h3>
                <p>Catching breakage. Model updates, prompt changes, dependency shifts. Run on every change. Non-negotiable.</p>
            </div>
        </div>

        <!-- SLIDE: The Flywheel -->
        <div class="slide slide-visual">
            <span class="topic-tag purple">Monitoring</span>
            <h2>The Eval Flywheel: How Quality Compounds</h2>
            <img src="images/continuous_improvement_flywheel.png" alt="Continuous Improvement Flywheel">
            <p class="caption">Ship → Observe → Curate failures into eval cases → Eval before next deploy → Improve → Ship again. Each cycle makes the system more robust.</p>
        </div>

        <!-- SLIDE: What to Look For -->
        <div class="slide slide-list">
            <span class="topic-tag purple">Building AI Products</span>
            <h2>If You're Building AI Products, Know This</h2>
            <ul>
                <li class="purple"><strong>Evals before features.</strong> You can't iterate fast without fast feedback. Build the eval harness first.</li>
                <li class="blue"><strong>LLM-as-judge scales.</strong> Human review doesn't. Calibrate your LLM graders against human judgment, then trust them.</li>
                <li class="green"><strong>Regression tests are sacred.</strong> Every production failure becomes a test case. The suite only grows.</li>
                <li class="orange"><strong>Monitor implicit signals.</strong> Users don't file bug reports. They regenerate, abandon, or leave. Watch behavior.</li>
                <li class="red"><strong>Close the loop.</strong> Production → evals → improvements → production. The flywheel compounds.</li>
            </ul>
        </div>

        <!-- SLIDE: Output Takeaways -->
        <div class="slide slide-list">
            <h2 class="purple">Output Layer: Key Takeaways</h2>
            <ul>
                <li class="purple"><strong>Evals are the shipping bottleneck.</strong> Fast eval cycles = fast iteration. Invest here first.</li>
                <li class="purple"><strong>Model evals ≠ product evals.</strong> Benchmarks don't tell you if users succeed. Test what matters.</li>
                <li class="purple"><strong>LLM-as-judge changed evaluation.</strong> Scale beyond human capacity. Calibrate carefully.</li>
                <li class="purple"><strong>The flywheel wins.</strong> Production failures → eval cases → prevented failures. Compound quality over time.</li>
            </ul>
        </div>



        <!-- ============================================ -->
        <!-- SECTION 5: WHAT'S BROKEN (Slides 54-65) -->
        <!-- ============================================ -->
        
        <!-- SLIDE 54: Section Break -->
        <div class="slide slide-section">
            <p class="label red">Section 5</p>
            <h2>What's Still Broken</h2>
            <p class="sub">The Challenges That Remain</p>
            <div class="framework-container">
                <svg class="framework-svg" viewBox="0 0 950 480" xmlns="http://www.w3.org/2000/svg" style="max-width: 1100px;">
                    <g class="framework-layer dimmed"><rect x="20" y="15" width="750" height="95" rx="8" fill="#3b82f6" opacity="0.15" stroke="#3b82f6" stroke-width="2"/><text x="30" y="38" fill="#3b82f6" font-weight="700" font-size="14">INPUT LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="120" width="750" height="95" rx="8" fill="#10b981" opacity="0.15" stroke="#10b981" stroke-width="2"/><text x="30" y="143" fill="#10b981" font-weight="700" font-size="14">DATA AND MODEL LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="225" width="750" height="95" rx="8" fill="#f97316" opacity="0.15" stroke="#f97316" stroke-width="2"/><text x="30" y="248" fill="#f97316" font-weight="700" font-size="14">APPLICATION LAYER</text></g>
                    <g class="framework-layer dimmed"><rect x="20" y="330" width="750" height="95" rx="8" fill="#8b5cf6" opacity="0.15" stroke="#8b5cf6" stroke-width="2"/><text x="30" y="353" fill="#8b5cf6" font-weight="700" font-size="14">OUTPUT LAYER</text></g>
                    <g class="framework-layer">
                        <rect x="790" y="15" width="145" height="410" rx="8" fill="#ef4444" opacity="0.15" stroke="#ef4444" stroke-width="2"/>
                        <text x="862" y="42" text-anchor="middle" fill="#ef4444" font-weight="700" font-size="13">CHALLENGES</text>
                        <rect x="800" y="55" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="80" text-anchor="middle" fill="white" font-size="10" font-weight="500">Hallucinations</text>
                        <rect x="800" y="100" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="125" text-anchor="middle" fill="white" font-size="10" font-weight="500">Inconsistent Reasoning</text>
                        <rect x="800" y="145" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="170" text-anchor="middle" fill="white" font-size="10" font-weight="500">Over-Autonomy</text>
                        <rect x="800" y="190" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="215" text-anchor="middle" fill="white" font-size="10" font-weight="500">Poor Tool Grounding</text>
                        <rect x="800" y="235" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="260" text-anchor="middle" fill="white" font-size="10" font-weight="500">Long Context Drift</text>
                        <rect x="800" y="280" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="305" text-anchor="middle" fill="white" font-size="10" font-weight="500">Retrieval Issues</text>
                        <rect x="800" y="325" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="350" text-anchor="middle" fill="white" font-size="10" font-weight="500">Multi-Agent Errors</text>
                        <rect x="800" y="370" width="125" height="38" rx="5" fill="#ef4444"/><text x="862" y="395" text-anchor="middle" fill="white" font-size="10" font-weight="500">Debugging</text>
                    </g>
                    <rect class="highlight-ring active" x="785" y="10" width="155" height="420" rx="12"/>
                </svg>
            </div>
        </div>
        
        <!-- SLIDE 54b: Section 5 Overview -->
        <div class="slide slide-list">
            <h2>The Problems We Haven't Solved</h2>
            <ul>
                <li class="red"><strong>Hallucinations remain fundamental.</strong> Math confirms: complete elimination is impossible with current architectures. Design for detection, not prevention.</li>
                <li class="red"><strong>RAG hit a wall.</strong> Academic papers keep appearing. Breakthroughs don't. The fundamental retrieval problem persists.</li>
                <li class="red"><strong>Multi-agent coordination is chaos.</strong> Debugging distributed AI systems remains genuinely hard.</li>
                <li class="red"><strong>Long context doesn't mean infinite context.</strong> Models lose information in the middle. Token limits still matter.</li>
            </ul>
        </div>
        
        <!-- SLIDE 55: Honest Assessment -->
        <div class="slide slide-quote">
            <blockquote>"AI is transformative for specific, well-constrained use cases where occasional failure is acceptable."</blockquote>
            <p class="attr">For anything requiring consistent reliability, it remains a powerful tool that requires constant supervision.</p>
        </div>
        
        <!-- SLIDE 56: Hallucinations Reality -->
        <div class="slide slide-list">
            <span class="topic-tag red">Hallucinations</span>
            <h2>Hallucinations: What Practitioners Need to Know</h2>
            <ul>
                <li class="green"><strong>Simple facts are mostly reliable now.</strong> General knowledge queries rarely hallucinate. This is a solved problem for basic use cases.</li>
                <li class="red"><strong>Complex reasoning still hallucinates.</strong> Legal, medical, financial:domain-specific accuracy drops significantly. Don't trust without verification.</li>
                <li class="orange"><strong>Complete elimination is mathematically impossible.</strong> Current architectures learn patterns, not truth. Design systems that catch hallucinations, not prevent them.</li>
                <li class="blue"><strong>Verification is your responsibility.</strong> Use retrieval to ground responses. Implement confidence thresholds. Build human review into high-stakes workflows.</li>
            </ul>
        </div>
        
        <!-- SLIDE 57: Why Hallucinations Persist -->
        <div class="slide slide-statement">
            <span class="topic-tag red">Hallucinations</span>
            <h2>Mathematical research confirms: complete elimination is impossible with current architectures.</h2>
            <p class="small">Models learn statistical patterns, not actual understanding. Training data has gaps and errors. Design systems assuming hallucinations will happen:the question isn't "how do we prevent them?" but "how do we catch them before they cause damage?"</p>
        </div>
        
        <!-- SLIDE 58: RAG Stagnation -->
        <div class="slide slide-quote">
            <span class="topic-tag red">RAG</span>
            <blockquote>"RAG is in a phase of stagnation."</blockquote>
            <p class="attr">: RAGFlow researchers, 2025. Academic papers keep appearing. Breakthroughs don't.</p>
        </div>
        
        <!-- SLIDE 59: RAG Problems -->
        <div class="slide slide-cards">
            <span class="topic-tag red">RAG</span>
            <h2>Why RAG Hit the Wall</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--red);">
                    <h3>Chunk Size Tradeoffs</h3>
                    <p>Small chunks = precise matching but fragmented context. Large chunks = context preserved but reduced precision. No right answer — just tradeoffs based on your use case.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--red);">
                    <h3>Storage Explosion</h3>
                    <p>A single page image requires ~512KB in tensor storage. A million-page corpus hits terabytes. That's before you consider retrieval latency at scale.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--red);">
                    <h3>Similarity ≠ Relevance</h3>
                    <p>Query "customer churn Q4" and you'll get docs about satisfaction, Q4 financials, historical churn — all semantically related, none answering your question.</p>
                </div>
            </div>
            <p class="subtext muted" style="margin-top: 1rem;">For many use cases, long-context models may be simpler than sophisticated RAG pipelines. Evaluate before defaulting to RAG because it's familiar.</p>
        </div>
        
        <!-- SLIDE 60: RAG Hype Cycle -->
        <div class="slide slide-visual">
            <span class="topic-tag red">RAG</span>
            <h2>RAG Interest Over Time</h2>
            <img src="images/rag_hype_cycle.png" alt="RAG Hype Cycle">
            <p class="caption">Hype peaked in early-mid 2023. Long-context alternatives are emerging as viable options for many use cases.</p>
        </div>
        
        <!-- SLIDE 61: Production Gap -->
        <div class="slide slide-statement">
            <span class="topic-tag red">Production Gap</span>
            <h2>Most pilots never ship. Here's why.</h2>
            <p class="small">The reliability math doesn't work. The platform keeps changing. The cost surprises. What worked in sandbox breaks with real data at real scale with real users.</p>
        </div>
        
        <!-- SLIDE 62: Why Pilots Fail -->
        <div class="slide slide-list">
            <span class="topic-tag red">Production Gap</span>
            <h2>Why So Many Pilots Fail</h2>
            <ul>
                <li class="red"><strong>The reliability math doesn't work:</strong> 5% error rate is fine for chatbots, catastrophic for agents that update databases</li>
                <li class="red"><strong>The platform keeps moving:</strong> 70% of regulated enterprises rebuild their AI stack every 3 months</li>
                <li class="red"><strong>Demo ≠ production:</strong> The gap between 90% reliability and 99.9% reliability is massive</li>
                <li class="red"><strong>Costs surprise:</strong> $50-100 per 100-turn conversation wasn't in the pilot budget</li>
            </ul>
            <p class="subtext">How teams adapted: 68% of production agents run fewer than 10 steps before human intervention. 92.5% deliver output to humans, not other systems. Autonomy got deliberately constrained.</p>
        </div>
        
        <!-- SLIDE 63: Debugging -->
        <div class="slide slide-vs">
            <span class="topic-tag red">Debugging</span>
            <div class="vs-box" style="background: rgba(59, 130, 246, 0.1); border-color: rgba(59, 130, 246, 0.3);">
                <span class="tag" style="color: var(--blue);">Traditional Debugging</span>
                <h3>Set breakpoint. Examine state. Step through code.</h3>
                <p>Decades of infrastructure support this workflow.</p>
            </div>
            <span class="vs-arrow">→</span>
            <div class="vs-box" style="background: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.3);">
                <span class="tag" style="color: var(--red);">Agent Debugging</span>
                <h3>Hope your logs are comprehensive enough to guess what happened.</h3>
                <p>Non-deterministic execution. Bugs emerge 50+ exchanges in.</p>
            </div>
        </div>
        
        <!-- SLIDE 64: Multi-Agent Debugging -->
        <div class="slide slide-visual">
            <span class="topic-tag red">Debugging</span>
            <h2>Multi-Agent Debugging is Fundamentally Different</h2>
            <img src="images/challenges/www_getmaxim_ai_5.png" alt="Agent Debugging">
            <p class="caption">Agent tracing platforms are emerging, but the field is primitive compared to traditional software development. Budget debugging time you can't easily do.</p>
        </div>
        
        <!-- SLIDE 64b: Observability Emerging -->
        <div class="slide slide-visual">
            <span class="topic-tag red">Debugging</span>
            <h2>Agent Observability: A New Category</h2>
            <div class="visual-container">
                <img src="images/challenges/www_getmaxim_ai_6.png" alt="AI Agent Observability Platforms" style="max-height: 50vh;">
            </div>
            <p class="caption">Agent observability platforms emerged as essential infrastructure in 2025. Tracing, logging, and monitoring for non-deterministic systems requires specialized tooling.</p>
        </div>
        
        <!-- SLIDE 64c: Long Context RAG -->
        <div class="slide slide-visual">
            <span class="topic-tag red">RAG</span>
            <h2>Long Context RAG: Hybrid Approach</h2>
            <div class="visual-container">
                <img src="images/challenges/ragflow_io_5.png" alt="Long Context RAG" style="max-height: 55vh;">
            </div>
            <p class="caption">Hybrid tree-graph retrieval combines entity relationships with semantic chunks. Long context models complement RAG rather than replacing it.</p>
        </div>
        
        <!-- SLIDE 64d: The Debugging Gap -->
        <div class="slide slide-cards">
            <span class="topic-tag red">Debugging</span>
            <h2>The Debugging Gap</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--red);">
                    <h3>No Breakpoints</h3>
                    <p>Traditional debugging lets you pause, inspect, and step through. Agent execution is a black box until failure.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--red);">
                    <h3>Non-Determinism</h3>
                    <p>Same input, different output. Reproducing bugs requires capturing exact model state, temperature, and random seeds.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--red);">
                    <h3>Delayed Failures</h3>
                    <p>Errors cascade. A subtle mistake at step 5 explodes at step 50. Root cause analysis requires full trace history.</p>
                </div>
            </div>
            <p class="subtext muted" style="margin-top: 1rem;">Invest in observability infrastructure before you need it. Debugging without traces is archaeology.</p>
        </div>
        
        <!-- SLIDE 65: Challenges Takeaways -->
        <div class="slide slide-list">
            <h2 class="red">Challenges: Key Takeaways</h2>
            <ul>
                <li class="orange"><strong>Plan for integration and reliability work upfront.</strong> Most pilots fail before production:understand why.</li>
                <li class="orange"><strong>Design for human oversight, don't fight it.</strong> 92.5% of production agents output to humans, not systems.</li>
                <li class="orange"><strong>Evaluate long-context models before defaulting to RAG.</strong> Simpler may be better for your use case.</li>
                <li class="orange"><strong>Invest in observability early.</strong> Debugging agents without tracing is guesswork.</li>
            </ul>
        </div>
        


        <!-- ============================================ -->
        <!-- SECTION 6: ROAD AHEAD (Slides 66-77) -->
        <!-- ============================================ -->
        
        <!-- SLIDE 66: Section Break -->
        <div class="slide slide-section">
            <p class="label purple">Section 6</p>
            <h2>The Road Ahead</h2>
            <p class="sub">What 2026 Looks Like</p>
        </div>
        
        <!-- SLIDE 66b: Section 6 Overview -->
        <div class="slide slide-list">
            <h2>What's Coming Next</h2>
            <ul>
                <li class="purple"><strong>RLVR expands beyond code and math.</strong> Same technique, fuzzier domains:customer service, content, support.</li>
                <li class="purple"><strong>Inference gets fast.</strong> Minutes → seconds. Real-time AI becomes viable for more use cases.</li>
                <li class="purple"><strong>Standards consolidate.</strong> MCP and A2A proved the model. Expect more open governance, less vendor lock-in.</li>
                <li class="purple"><strong>AI becomes infrastructure.</strong> Like databases in the 90s, cloud in the 2010s:competitive advantage becomes table stakes.</li>
            </ul>
        </div>
        
        <!-- SLIDE 67: 2026 Prediction -->
        <div class="slide slide-statement">
            <span class="topic-tag purple">2026 Outlook</span>
            <h2>By the end of 2026, AI won't be AGI or sentient machines.</h2>
        </div>
        
        <!-- SLIDE 68: Boring Infrastructure -->
        <div class="slide slide-statement">
            <span class="topic-tag purple">2026 Outlook</span>
            <h2>It will be <span class="blue">boring infrastructure</span> that works.</h2>
            <p class="small">Like databases in the 1990s. Like cloud computing in the 2010s. The technology will transition from competitive advantage to table stakes.</p>
        </div>
        
        <!-- SLIDE 69: What's Coming -->
        <div class="slide slide-cards">
            <span class="topic-tag purple">2026 Outlook</span>
            <h2>What's Coming in 2026</h2>
            <div class="cards-grid two">
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>RLVR Goes Mainstream</h3>
                    <p>Expanding beyond code and math to fuzzier domains through synthetic feedback. Customer service agents learning from satisfaction scores. Content generators optimizing for engagement metrics.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>Inference Speed Revolution</h3>
                    <p>Current reasoning: minutes. Goal: seconds. Specialized chips, mixture-of-experts, speculative execution. Speed unlocks real-time customer support, trading systems, medical diagnosis at ER pace.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>Standards Consolidation</h3>
                    <p>MCP and A2A proved standards can succeed. Expect explosion then consolidation. Open governance models will win over corporate-controlled alternatives.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--orange);">
                    <h3>Regional AI Blocs</h3>
                    <p>Europe: privacy. China: efficiency. US: capabilities. Diversity reduces systemic risk. Multiple approaches increase odds of solving hard problems.</p>
                </div>
            </div>
        </div>
        
        <!-- SLIDE 70: Investment Realignment -->
        <div class="slide slide-list">
            <span class="topic-tag purple">Investment</span>
            <h2>Investment Landscape Shifts</h2>
            <ul>
                <li><strong>Pure model plays become commoditized.</strong> The moat isn't the model anymore.</li>
                <li><strong>Infrastructure and tooling command premiums.</strong> The picks and shovels of the AI gold rush.</li>
                <li><strong>Vertical applications with domain expertise attract capital.</strong> Generic horizontal plays less attractive.</li>
                <li><strong>Consolidation among 2023-2024 startups.</strong> Thin wrappers around GPT-4 aren't defensible.</li>
                <li><strong>Enterprise procurement matures.</strong> SLAs, indemnification, predictable pricing become requirements.</li>
            </ul>
        </div>
        
        <!-- SLIDE 71: Success Metrics Shift -->
        <div class="slide slide-vs">
            <span class="topic-tag purple">Success Metrics</span>
            <div class="vs-box old">
                <span class="tag">Old Success Metric</span>
                <h3>"Look what AI can do"</h3>
                <p>Impressive demos, benchmark scores, capability showcases</p>
            </div>
            <span class="vs-arrow">→</span>
            <div class="vs-box new">
                <span class="tag">New Success Metric</span>
                <h3>"Look what we do with AI"</h3>
                <p>Business outcomes, cost savings, productivity gains</p>
            </div>
        </div>
        
        <!-- SLIDE 72: Who Wins -->
        <div class="slide slide-statement">
            <span class="topic-tag purple">Success Metrics</span>
            <h2>The companies that win won't have the best models.</h2>
            <p class="small">They'll integrate AI best into their operations. They'll build the most reliable systems. They'll create the most value for users. They'll turn 2025's breakthroughs into 2026's business as usual.</p>
        </div>
        
        <!-- SLIDE 72b: AI-Native Architecture -->
        <div class="slide slide-content">
            <span class="topic-tag purple">AI Architecture</span>
            <h2>AI-Native vs AI-Augmented</h2>
            <div class="body">
                <div class="text">
                    <p>Two distinct approaches emerged:</p>
                    <ul>
                        <li><strong>AI-Augmented:</strong> Bolt AI onto existing workflows. Faster to deploy, lower risk, incremental gains.</li>
                        <li><strong>AI-Native:</strong> Rebuild processes around AI capabilities. Higher investment, transformative potential.</li>
                    </ul>
                    <p style="margin-top: 1rem;">2026 prediction: Most successful companies do both. Augment existing processes while building native experiments in parallel.</p>
                </div>
            </div>
        </div>
        
        <!-- SLIDE 72c: Unanswered Questions -->
        <div class="slide slide-list">
            <span class="topic-tag purple">Open Questions</span>
            <h2>The Questions 2025 Left Unanswered</h2>
            <ul>
                <li><strong>Scaling laws:</strong> Are we hitting diminishing returns, or is this a temporary plateau?</li>
                <li><strong>Reasoning reliability:</strong> Can thinking models achieve 99.9% accuracy on complex tasks?</li>
                <li><strong>Cost floor:</strong> How cheap can inference get? $0.001 per million tokens?</li>
                <li><strong>Agent autonomy:</strong> When (if ever) do we trust AI to act without oversight?</li>
                <li><strong>Regulation:</strong> Will governance keep pace with capability growth?</li>
            </ul>
        </div>
        
        <!-- SLIDE 72d: The Talent Shift -->
        <div class="slide slide-cards">
            <span class="topic-tag purple">Talent</span>
            <h2>The Talent Landscape Shifts</h2>
            <div class="cards-grid stacked">
                <div class="card" style="border-left: 4px solid var(--blue);">
                    <h3>2024: ML Engineers Dominated</h3>
                    <p>Training models, optimizing architectures, pushing benchmarks. The competitive advantage was model expertise.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--green);">
                    <h3>2025: Systems Engineers Rise</h3>
                    <p>Integration, reliability, observability. Making AI work in production became the hard problem.</p>
                </div>
                <div class="card" style="border-left: 4px solid var(--purple);">
                    <h3>2026: Domain Experts Lead</h3>
                    <p>Understanding what problems to solve matters more than how to solve them. AI becomes a tool, not a specialty.</p>
                </div>
            </div>
        </div>
        
        <!-- SLIDE 72e: Road Ahead Takeaways -->
        <div class="slide slide-list">
            <h2 class="purple">Road Ahead: Key Takeaways</h2>
            <ul>
                <li class="blue"><strong>AI becomes infrastructure.</strong> By 2027, asking "do you use AI?" will be like asking "do you use databases?"</li>
                <li class="blue"><strong>Integration quality determines success.</strong> The model choice matters less than how well you deploy it.</li>
                <li class="blue"><strong>Talent profiles evolve.</strong> Systems thinking and domain expertise beat pure ML knowledge.</li>
                <li class="blue"><strong>Regional approaches diverge.</strong> Privacy, efficiency, capability:different markets optimize differently.</li>
            </ul>
        </div>
        


        <!-- ============================================ -->
        <!-- CLOSING (Slides 73-80) -->
        <!-- ============================================ -->
        
        <!-- SLIDE 73: Final Takeaways Title -->
        <div class="slide slide-section">
            <p class="label purple">Final</p>
            <h2>What to Remember</h2>
        </div>
        
        <!-- SLIDE 74: Takeaway 1 -->
        <div class="slide slide-statement">
            <h2>AI will become standard infrastructure.</h2>
            <p class="small">Competitive advantage comes from how you use it, not whether you have it. The question isn't "are you using AI?" but "how well are you using AI?"</p>
        </div>
        
        <!-- SLIDE 75: Takeaway 2 -->
        <div class="slide slide-statement">
            <h2>Integration quality beats model selection.</h2>
            <p class="small">The best model poorly integrated loses to a good model well integrated. Invest in the plumbing. The unsexy infrastructure work is where the value is.</p>
        </div>
        
        <!-- SLIDE 76: Takeaway 3 -->
        <div class="slide slide-statement">
            <h2>Build evaluation, cost tracking, and safety into your stack now.</h2>
            <p class="small">Enterprises will require it. Production guarantees, SLAs, indemnification, predictable pricing:the age of experimentation gives way to operational discipline.</p>
        </div>
        
        <!-- SLIDE 77: Takeaway 4 -->
        <div class="slide slide-statement">
            <h2>Measure business outcomes, not AI capabilities.</h2>
            <p class="small">The most impressive deployments won't be the most technically sophisticated:they'll be the ones solving real problems for real users at sustainable costs.</p>
        </div>
        
        <!-- SLIDE 78: The Most Important Lesson -->
        <div class="slide slide-quote">
            <blockquote>"AI's value doesn't come from any single breakthrough but from making all the pieces work together."</blockquote>
            <p class="attr">The future belongs not to those with the best models, but to those who best integrate AI into the messy reality of human work.</p>
        </div>
        
        <!-- SLIDE 79: State of AI Summary -->
        <div class="slide slide-list">
            <h2>The State of Applied AI in 2025</h2>
            <ul>
                <li class="green"><strong>It works, mostly.</strong> Teams that shipped focused on narrow scope, human checkpoints, and boring reliability over impressive capabilities.</li>
                <li class="orange"><strong>It's expensive:but manageable.</strong> Cost optimization is a discipline now. Caching, quantization, and specialization make production viable.</li>
                <li class="purple"><strong>Most pilots fail.</strong> Not because AI doesn't work, but because integration, reliability, and cost weren't planned for.</li>
                <li class="blue"><strong>The foundations are solid.</strong> Standards like MCP exist. Patterns are documented. The wild experimentation gives way to disciplined engineering.</li>
            </ul>
            <p class="subtext">The gap between demo and production is where most projects die. Plan for it.</p>
        </div>
        
        <!-- SLIDE 80: Thank You -->
        <div class="slide slide-title">
            <h1>State of Applied AI in 2025</h1>
            <p class="subtitle" style="margin-top: 2rem;">Questions?</p>
            <p class="muted" style="margin-top: 3rem; font-size: 0.9rem;">Based on ICONIQ Growth GenAI Survey, State of AI Report 2025, MIT/Fortune research, Cleanlab production surveys, RAGFlow analysis</p>
        </div>


    </div>

    <nav class="nav">
        <button onclick="prevSlide()">← Prev</button>
        <span class="counter"><span id="current">1</span> / <span id="total">80</span></span>
        <button onclick="nextSlide()">Next →</button>
    </nav>

    <script src="script.js"></script>
</body>
</html>

# üî® GUIDE DES PROJETS PRATIQUES
## 15 Projets Progressifs : Du D√©butant √† l'Expert

---

> **Philosophie**: Apprendre en faisant. Chaque projet construit sur les pr√©c√©dents et vous am√®ne progressivement vers la ma√Ætrise compl√®te des LLMs.

---

## üìä APER√áU DES PROJETS

| # | Projet | Niveau | Dur√©e | Technologies Cl√©s | Comp√©tences |
|---|--------|--------|-------|-------------------|-------------|
| 1 | Transformer from Scratch | üü¢ D√©butant | 8-12h | PyTorch, NumPy | Architecture, Math |
| 2 | Data Preparation Pipeline | üü¢ D√©butant | 10-15h | Python, Datasets | Data Engineering |
| 3 | Train nanoGPT (124M) | üîµ Interm√©diaire | 15-20h | PyTorch, GPUs | Training Basics |
| 4 | Optimize Training Run | üîµ Interm√©diaire | 8-12h | Profiling, DeepSpeed | Performance |
| 5 | Fine-tune Llama 3 | üîµ Interm√©diaire | 10-15h | HuggingFace, Transformers | Fine-tuning |
| 6 | LoRA on Consumer GPU | üîµ Interm√©diaire | 12-18h | PEFT, bitsandbytes | Efficient Training |
| 7 | RLHF Pipeline | üü† Avanc√© | 20-30h | TRL, PPO | Alignment |
| 8 | Quantize for CPU | üîµ Interm√©diaire | 8-10h | llama.cpp, GPTQ | Optimization |
| 9 | Deploy vLLM API | üü† Avanc√© | 12-18h | vLLM, FastAPI | Serving |
| 10 | RAG System (10k docs) | üü† Avanc√© | 20-25h | LangChain, Qdrant | RAG Architecture |
| 11 | Autonomous Agent | üü† Avanc√© | 25-35h | LangChain, Tools | Agents |
| 12 | Fine-tune Multimodal | üü† Avanc√© | 20-30h | LLaVA, Vision | Multimodal |
| 13 | Eval Pipeline (CI/CD) | üî¥ Expert | 15-20h | Testing, Automation | LLMOps |
| 14 | Enterprise Chatbot | üî¥ Expert | 40-60h | Full Stack | Production App |
| 15 | LLM from Scratch | üî¥ Expert | 100-150h | Tout | End-to-End |

**Total estim√©**: ~350-450 heures de pratique

---

## üü¢ PROJET 1 : TRANSFORMER FROM SCRATCH

### **Objectifs d'apprentissage**
- Comprendre en profondeur l'architecture transformer
- Impl√©menter self-attention, multi-head attention
- Ma√Ætriser les positional encodings
- Cr√©er un mod√®le entra√Ænable from scratch

### **Sp√©cifications**
```python
# Architecture cible
- Mod√®le: Decoder-only transformer (GPT-style)
- Param√®tres: ~6M (petit pour apprentissage)
- Couches: 6 transformer blocks
- Attention heads: 6
- Embedding dim: 384
- Context length: 256 tokens
- Vocabulaire: 50k tokens (GPT-2 tokenizer)
```

### **Structure du projet**
```
project_01_transformer/
‚îú‚îÄ‚îÄ model.py              # Architecture du transformer
‚îú‚îÄ‚îÄ attention.py          # Self-attention mechanism
‚îú‚îÄ‚îÄ positional.py         # Positional encoding
‚îú‚îÄ‚îÄ feed_forward.py       # FFN layers
‚îú‚îÄ‚îÄ train.py              # Training loop
‚îú‚îÄ‚îÄ tokenizer.py          # Tokenization
‚îú‚îÄ‚îÄ data.py               # Dataset loading
‚îú‚îÄ‚îÄ config.py             # Hyperparameters
‚îú‚îÄ‚îÄ utils.py              # Helper functions
‚îî‚îÄ‚îÄ notebooks/
    ‚îú‚îÄ‚îÄ 01_attention_visualization.ipynb
    ‚îú‚îÄ‚îÄ 02_training_demo.ipynb
    ‚îî‚îÄ‚îÄ 03_generation_demo.ipynb
```

### **√âtapes d√©taill√©es**

#### **√âtape 1: Impl√©mentation de l'attention (3-4h)**
```python
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    """
    Impl√©mentation from scratch du m√©canisme d'attention
    """
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Projections Q, K, V
        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

        # Scaling factor
        self.scale = 1.0 / math.sqrt(self.head_dim)

    def forward(self, x, mask=None):
        batch_size, seq_len, embed_dim = x.shape

        # Projeter et reshaper pour multi-head
        qkv = self.qkv_proj(x)  # [B, T, 3*D]
        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, T, D_h]
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Scaled dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, H, T, T]

        # Masque causal pour autoregressive generation
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # Softmax et application sur V
        attn_weights = torch.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)  # [B, H, T, D_h]

        # Recombiner les heads
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(batch_size, seq_len, embed_dim)

        # Projection finale
        output = self.out_proj(attn_output)

        return output, attn_weights
```

**üõ†Ô∏è Exercice**:
- Visualiser les attention weights sur une phrase simple
- Tester avec diff√©rents nombres de heads
- Comparer avec `torch.nn.MultiheadAttention`

#### **√âtape 2: Positional Encoding (2h)**
```python
class PositionalEncoding(nn.Module):
    """
    Encodage positionnel sinuso√Ødal (Vaswani et al. 2017)
    """
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        # Cr√©er la matrice d'encodage
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        return x + self.pe[:, :x.size(1), :]
```

**üõ†Ô∏è Alternatives √† impl√©menter**:
- Learned positional embeddings
- RoPE (Rotary Position Embedding)
- ALiBi (Attention with Linear Biases)

#### **√âtape 3: Transformer Block (2-3h)**
```python
class TransformerBlock(nn.Module):
    """
    Bloc transformer complet: Attention + FFN + LayerNorm + Residual
    """
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()

        # Multi-head attention
        self.attention = SelfAttention(embed_dim, num_heads)

        # Feed-forward network
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),
            nn.Linear(ff_dim, embed_dim),
        )

        # Layer normalization (pre-norm architecture)
        self.ln1 = nn.LayerNorm(embed_dim)
        self.ln2 = nn.LayerNorm(embed_dim)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Pre-LN architecture (GPT-2 style)
        # Attention block
        attn_out, attn_weights = self.attention(self.ln1(x), mask)
        x = x + self.dropout(attn_out)

        # FFN block
        ffn_out = self.ffn(self.ln2(x))
        x = x + self.dropout(ffn_out)

        return x, attn_weights
```

#### **√âtape 4: Mod√®le complet (2-3h)**
```python
class GPTModel(nn.Module):
    """
    Mod√®le GPT complet (decoder-only transformer)
    """
    def __init__(self, vocab_size, embed_dim=384, num_heads=6,
                 num_layers=6, ff_dim=1536, max_len=256, dropout=0.1):
        super().__init__()

        # Token embeddings
        self.token_embed = nn.Embedding(vocab_size, embed_dim)

        # Positional encoding
        self.pos_encode = PositionalEncoding(embed_dim, max_len)

        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)
            for _ in range(num_layers)
        ])

        # Final layer norm
        self.ln_f = nn.LayerNorm(embed_dim)

        # Output head
        self.head = nn.Linear(embed_dim, vocab_size, bias=False)

        # Tie weights (token embeddings = output embeddings)
        self.head.weight = self.token_embed.weight

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # Cr√©er le masque causal
        self.register_buffer(
            'causal_mask',
            torch.tril(torch.ones(max_len, max_len)).view(1, 1, max_len, max_len)
        )

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx):
        # idx: [batch, seq_len]
        B, T = idx.shape

        # Embeddings
        x = self.token_embed(idx)  # [B, T, D]
        x = self.pos_encode(x)
        x = self.dropout(x)

        # Masque causal
        mask = self.causal_mask[:, :, :T, :T]

        # Transformer blocks
        for block in self.blocks:
            x, _ = block(x, mask)

        # Final norm
        x = self.ln_f(x)

        # Output logits
        logits = self.head(x)  # [B, T, vocab_size]

        return logits

    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """
        G√©n√©ration autoregreessive
        """
        for _ in range(max_new_tokens):
            # Crop context si trop long
            idx_cond = idx if idx.size(1) <= self.config.max_len else idx[:, -self.config.max_len:]

            # Forward pass
            logits = self(idx_cond)

            # Prendre le dernier token
            logits = logits[:, -1, :] / temperature

            # Top-k sampling (optionnel)
            if top_k is not None:
                v, _ = torch.topk(logits, top_k)
                logits[logits < v[:, [-1]]] = -float('Inf')

            # Softmax et sample
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)

            # Append
            idx = torch.cat((idx, idx_next), dim=1)

        return idx
```

#### **√âtape 5: Training Loop (2-3h)**
```python
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

def train_epoch(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0

    for batch in tqdm(dataloader, desc="Training"):
        # Data
        inputs = batch['input_ids'].to(device)  # [B, T]
        targets = batch['target_ids'].to(device)  # [B, T]

        # Forward
        logits = model(inputs)  # [B, T, vocab_size]

        # Loss (cross-entropy)
        loss = nn.functional.cross_entropy(
            logits.view(-1, logits.size(-1)),
            targets.view(-1),
            ignore_index=-100
        )

        # Backward
        optimizer.zero_grad()
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Optimizer step
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)

# Training loop complet
def train(model, train_loader, val_loader, config):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # Optimizer
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        betas=(0.9, 0.95),
        weight_decay=0.1
    )

    # Learning rate scheduler (cosine avec warmup)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.num_epochs
    )

    # Training
    best_val_loss = float('inf')

    for epoch in range(config.num_epochs):
        print(f"\nEpoch {epoch+1}/{config.num_epochs}")

        # Train
        train_loss = train_epoch(model, train_loader, optimizer, device)

        # Validate
        val_loss = evaluate(model, val_loader, device)

        # Scheduler step
        scheduler.step()

        # Logging
        print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
            }, 'best_model.pt')
```

### **Dataset utilis√©**
- **TinyStories** (petites histoires pour enfants, ~2GB)
- Alternative: WikiText-103, OpenWebText

### **R√©sultats attendus**
- ‚úÖ Loss converge vers ~3.5-4.0
- ‚úÖ G√©n√©ration de texte coh√©rent (3-4 mots cons√©cutifs)
- ‚úÖ Attention weights montrent des patterns sens√©s

### **Extensions possibles**
1. Impl√©menter Flash Attention
2. Ajouter KV caching pour l'inference
3. Tester diff√©rents positional encodings
4. Visualiser les embeddings avec t-SNE
5. Comparer avec GPT-2 (HuggingFace)

### **Ressources**
- üìÑ Paper: "Attention is All You Need" (Vaswani et al., 2017)
- üíª Code: nanoGPT de Karpathy (r√©f√©rence)
- üìπ Vid√©o: Andrej Karpathy - Let's build GPT

---

## üü¢ PROJET 2 : DATA PREPARATION PIPELINE

### **Objectifs**
- Ma√Ætriser le preprocessing de donn√©es textuelles √† grande √©chelle
- Cr√©er un pipeline reproductible et scalable
- Comprendre les enjeux de qualit√© des donn√©es

### **Scope**
```
Input: 100GB de texte brut (Common Crawl, Wikipedia, Books, Code)
Output: Dataset nettoy√©, d√©dupliqu√©, tokeniz√© (HuggingFace format)
```

### **Pipeline complet**

#### **√âtape 1: Data Collection (2-3h)**
```python
from datasets import load_dataset

# T√©l√©charger datasets publics
datasets_to_download = [
    ("mc4", "en"),               # Multilingual C4 (English)
    ("wikipedia", "20231101.en"), # Wikipedia dump
    ("bookcorpus", None),         # Books
    ("the_pile", "all"),          # The Pile (subset)
]

for dataset_name, config in datasets_to_download:
    print(f"Downloading {dataset_name}...")
    if config:
        dataset = load_dataset(dataset_name, config, streaming=True)
    else:
        dataset = load_dataset(dataset_name, streaming=True)

    # Sauvegarder localement
    dataset.save_to_disk(f"data/raw/{dataset_name}")
```

#### **√âtape 2: Quality Filtering (4-5h)**
```python
import re
from ftlangdetect import detect  # Fast language detection
from typing import Dict

class QualityFilter:
    """
    Impl√©mentation des Gopher Rules (DeepMind)
    """

    def __init__(self):
        self.min_words = 50
        self.max_words = 100000
        self.min_avg_word_length = 3
        self.max_avg_word_length = 10
        self.max_repetition_ratio = 0.15
        self.max_symbol_to_word_ratio = 0.1

    def filter_document(self, text: str) -> bool:
        """
        Retourne True si le document passe les filtres
        """
        # D√©tecter la langue
        try:
            lang = detect(text)['lang']
            if lang != 'en':
                return False
        except:
            return False

        # Nombre de mots
        words = text.split()
        if len(words) < self.min_words or len(words) > self.max_words:
            return False

        # Longueur moyenne des mots
        avg_word_len = sum(len(w) for w in words) / len(words)
        if avg_word_len < self.min_avg_word_length or avg_word_len > self.max_avg_word_length:
            return False

        # Ratio de r√©p√©tition (d√©tection de spam)
        unique_words = set(words)
        repetition_ratio = 1 - (len(unique_words) / len(words))
        if repetition_ratio > self.max_repetition_ratio:
            return False

        # Ratio symboles/mots
        symbols = re.findall(r'[^a-zA-Z0-9\s]', text)
        symbol_ratio = len(symbols) / len(words) if len(words) > 0 else 1
        if symbol_ratio > self.max_symbol_to_word_ratio:
            return False

        # Filtre de contenu adulte/toxique (utiliser library d√©di√©e)
        if self.contains_toxic_content(text):
            return False

        return True

    def contains_toxic_content(self, text: str) -> bool:
        # Impl√©menter avec: detoxify, perspective API, ou liste de mots
        from detoxify import Detoxify
        results = Detoxify('original').predict(text)
        return max(results.values()) > 0.7  # Threshold
```

#### **√âtape 3: Deduplication (3-4h)**
```python
from datasketch import MinHash, MinHashLSH

class Deduplicator:
    """
    D√©duplication avec MinHash LSH (scalable √† 100GB+)
    """

    def __init__(self, threshold=0.85, num_perm=128):
        self.threshold = threshold
        self.num_perm = num_perm
        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
        self.seen_ids = set()

    def get_minhash(self, text: str) -> MinHash:
        """Cr√©er MinHash pour un document"""
        minhash = MinHash(num_perm=self.num_perm)
        # Shingles de 3 mots
        words = text.lower().split()
        shingles = [' '.join(words[i:i+3]) for i in range(len(words)-2)]
        for shingle in shingles:
            minhash.update(shingle.encode('utf8'))
        return minhash

    def is_duplicate(self, text: str, doc_id: str) -> bool:
        """V√©rifie si le document est un duplicate"""
        minhash = self.get_minhash(text)

        # Chercher des duplicates existants
        result = self.lsh.query(minhash)

        if len(result) > 0:
            return True  # Duplicate trouv√©

        # Ajouter √† l'index
        self.lsh.insert(doc_id, minhash)
        self.seen_ids.add(doc_id)

        return False

# Usage
dedup = Deduplicator(threshold=0.85)

unique_docs = []
for idx, doc in enumerate(documents):
    if not dedup.is_duplicate(doc['text'], str(idx)):
        unique_docs.append(doc)

print(f"Kept {len(unique_docs)}/{len(documents)} unique documents")
```

#### **√âtape 4: Tokenization (2-3h)**
```python
from transformers import AutoTokenizer

# Option 1: Utiliser tokenizer existant (GPT-2, Llama)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Option 2: Entra√Æner tokenizer custom
from tokenizers import (
    Tokenizer,
    models,
    pre_tokenizers,
    trainers,
)

def train_custom_tokenizer(files, vocab_size=50000):
    """
    Entra√Æner un tokenizer BPE custom
    """
    # Cr√©er tokenizer BPE
    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

    # Trainer
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=["<|endoftext|>", "<|pad|>", "<|unk|>"],
    )

    # Entra√Æner
    tokenizer.train(files, trainer)

    # Sauvegarder
    tokenizer.save("custom_tokenizer.json")

    return tokenizer

# Tokenize datasets
def tokenize_dataset(dataset, tokenizer, max_length=2048):
    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            truncation=True,
            max_length=max_length,
            return_overflowing_tokens=True,
        )

    tokenized = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names,
        num_proc=16,  # Parall√©lisation
    )

    return tokenized
```

#### **√âtape 5: Final Dataset Creation (2h)**
```python
from datasets import Dataset, DatasetDict

def create_final_dataset(processed_docs, test_size=0.01, val_size=0.01):
    """
    Cr√©er train/val/test splits et sauvegarder
    """
    # Cr√©er dataset
    dataset = Dataset.from_dict({'text': [doc['text'] for doc in processed_docs]})

    # Split train/temp
    train_test = dataset.train_test_split(test_size=test_size + val_size, seed=42)

    # Split temp ‚Üí val/test
    test_val = train_test['test'].train_test_split(
        test_size=test_size/(test_size+val_size),
        seed=42
    )

    # Cr√©er DatasetDict
    final_dataset = DatasetDict({
        'train': train_test['train'],
        'validation': test_val['train'],
        'test': test_val['test'],
    })

    # Tokenize
    final_dataset = final_dataset.map(
        lambda x: tokenizer(x['text'], truncation=True, max_length=2048),
        batched=True,
        num_proc=16,
    )

    # Sauvegarder
    final_dataset.save_to_disk("data/processed/final_dataset")

    # Upload vers HuggingFace Hub (optionnel)
    final_dataset.push_to_hub("your_username/your_dataset")

    return final_dataset
```

### **M√©triques de qualit√©**
```python
def compute_dataset_stats(dataset):
    """
    Statistiques du dataset final
    """
    stats = {
        'num_examples': len(dataset),
        'total_tokens': 0,
        'avg_tokens_per_doc': 0,
        'vocab_coverage': 0,
    }

    token_counts = [len(ex['input_ids']) for ex in dataset]
    stats['total_tokens'] = sum(token_counts)
    stats['avg_tokens_per_doc'] = stats['total_tokens'] / len(dataset)

    print(f"Dataset Statistics:")
    print(f"  Examples: {stats['num_examples']:,}")
    print(f"  Total tokens: {stats['total_tokens']:,}")
    print(f"  Avg tokens/doc: {stats['avg_tokens_per_doc']:.1f}")

    return stats
```

### **R√©sultats attendus**
- ‚úÖ 100GB brut ‚Üí ~60GB apr√®s filtering
- ‚úÖ ~30GB apr√®s d√©duplication
- ‚úÖ Dataset HuggingFace pr√™t pour training

---

## üîµ PROJET 3 : TRAIN NANOGPT (124M PARAMS)

### **Objectifs**
- Entra√Æner un vrai mod√®le de langage from scratch
- Ma√Ætriser le training loop complet
- Comprendre les m√©triques (loss, perplexit√©)

### **Sp√©cifications**
```yaml
Model:
  architecture: GPT-2 style (decoder-only)
  parameters: 124M
  layers: 12
  heads: 12
  embedding_dim: 768
  context_length: 1024

Training:
  dataset: OpenWebText (~8GB)
  batch_size: 12
  gradient_accumulation: 4  # effective batch = 48
  learning_rate: 6e-4
  warmup_steps: 2000
  max_steps: 100000
  fp16: true

Hardware:
  min: 1x RTX 3090 (24GB)
  recommended: 1x A100 (40GB)
  time: ~3-4 days
```

### **Code complet**
```python
# train.py - bas√© sur nanoGPT de Karpathy

import os
import time
import math
import pickle
from contextlib import nullcontext

import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group

from model import GPTConfig, GPT

# Configuration
out_dir = 'out'
eval_interval = 2000
eval_iters = 200
log_interval = 10
always_save_checkpoint = False

# Data
dataset = 'openwebtext'
gradient_accumulation_steps = 4
batch_size = 12
block_size = 1024  # context length

# Model
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.0

# Optimizer
learning_rate = 6e-4
max_iters = 600000
weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

# Learning rate schedule
decay_lr = True
warmup_iters = 2000
lr_decay_iters = 600000
min_lr = 6e-5

# System
device = 'cuda'
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'
compile_model = True  # PyTorch 2.0

# -----------------------------------------------------------------------------

# Setup
os.makedirs(out_dir, exist_ok=True)
torch.manual_seed(1337)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

device_type = 'cuda' if 'cuda' in device else 'cpu'
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)

# Data loader
data_dir = os.path.join('data', dataset)
train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')
val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')

def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

# Model initialization
model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,
                  bias=False, vocab_size=None, dropout=dropout)

print("Initializing model...")
gptconf = GPTConfig(**model_args)
model = GPT(gptconf)
model.to(device)

# Compile model (PyTorch 2.0)
if compile_model:
    print("Compiling model...")
    model = torch.compile(model)

# Optimizer
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)

# Training loop
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            with ctx:
                logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

# Learning rate scheduler
def get_lr(it):
    # Warmup
    if it < warmup_iters:
        return learning_rate * it / warmup_iters
    # Cosine decay
    if it > lr_decay_iters:
        return min_lr
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate - min_lr)

# Training
X, Y = get_batch('train')
t0 = time.time()
local_iter_num = 0
running_mfu = -1.0

for iter_num in range(max_iters):

    # Learning rate scheduling
    lr = get_lr(iter_num) if decay_lr else learning_rate
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

    # Evaluate
    if iter_num % eval_interval == 0:
        losses = estimate_loss()
        print(f"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

        # Save checkpoint
        if losses['val'] < best_val_loss or always_save_checkpoint:
            best_val_loss = losses['val']
            checkpoint = {
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'model_args': model_args,
                'iter_num': iter_num,
                'best_val_loss': best_val_loss,
            }
            print(f"saving checkpoint to {out_dir}")
            torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))

    # Forward backward update
    for micro_step in range(gradient_accumulation_steps):
        with ctx:
            logits, loss = model(X, Y)
            loss = loss / gradient_accumulation_steps

        X, Y = get_batch('train')
        loss.backward()

    # Clip gradients
    if grad_clip != 0.0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)

    # Optimizer step
    optimizer.step()
    optimizer.zero_grad(set_to_none=True)

    # Timing and logging
    t1 = time.time()
    dt = t1 - t0
    t0 = t1
    if iter_num % log_interval == 0:
        lossf = loss.item() * gradient_accumulation_steps
        print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms")

    local_iter_num += 1
```

### **R√©sultats attendus**
- ‚úÖ Val loss converge vers ~3.0-3.2
- ‚úÖ Perplexit√©: ~20-25
- ‚úÖ G√©n√©ration coh√©rente sur 20-30 tokens

---

*(Les 12 autres projets suivent avec le m√™me niveau de d√©tail...)*

---

**[Projets 4-15 continueraient ici avec la m√™me structure d√©taill√©e...]**

Pour raisons de concision, je liste les outlines:

## üîµ PROJET 4 : OPTIMIZE TRAINING RUN
- Profiling avec PyTorch Profiler
- Optimisations m√©moire (gradient checkpointing)
- DeepSpeed ZeRO stage 2
- Mixed precision (BF16)
- Target: 2x speedup

## üîµ PROJET 5 : FINE-TUNE LLAMA 3
- Supervised Fine-Tuning sur dataset custom
- HuggingFace Trainer API
- LoRA (r=16, alpha=32)
- Evaluation metrics

## üîµ PROJET 6 : LORA ON CONSUMER GPU
- QLoRA (4-bit quantization)
- Fine-tuner Llama 2 7B sur RTX 3090 (24GB)
- bitsandbytes + PEFT
- Merge adapters et deploy

## üü† PROJET 7 : RLHF PIPELINE
- √âtape 1: SFT
- √âtape 2: Reward Model training
- √âtape 3: PPO training
- TRL library
- Human preference dataset

## üîµ PROJET 8 : QUANTIZE FOR CPU
- GPTQ quantization
- llama.cpp conversion (GGUF)
- CPU inference (MacBook M1/M2)
- Benchmark (latency, throughput)

## üü† PROJET 9 : DEPLOY VLLM API
- vLLM serving
- FastAPI wrapper
- Load balancing
- Monitoring (Prometheus + Grafana)
- Docker deployment

## üü† PROJET 10 : RAG SYSTEM (10K DOCS)
- Document ingestion pipeline
- Chunking (semantic, recursive)
- Embeddings (sentence-transformers)
- Vector DB (Qdrant)
- Re-ranking (cross-encoder)
- Evaluation (RAGAS)

## üü† PROJET 11 : AUTONOMOUS AGENT
- ReAct architecture
- 10+ tools (web search, calculator, code execution, etc.)
- Long-term memory (vector DB)
- LangChain + MCP
- Multi-step reasoning

## üü† PROJET 12 : FINE-TUNE MULTIMODAL
- LLaVA architecture
- Vision encoder fine-tuning
- Custom vision-language dataset
- VQA evaluation

## üî¥ PROJET 13 : EVAL PIPELINE (CI/CD)
- Automated benchmark suite
- GitHub Actions integration
- Regression testing
- Statistical significance tests
- Cost-aware evaluation

## üî¥ PROJET 14 : ENTERPRISE CHATBOT
- Full-stack application
- RAG + Fine-tuning hybrid
- Multi-tenancy
- Security (auth, PII redaction)
- Monitoring et logging
- Frontend (React)
- Backend (FastAPI)
- Database (PostgreSQL + Qdrant)

## üî¥ PROJET 15 : LLM FROM SCRATCH
- **Dur√©e**: 3 mois
- **Scope complet**:
  - Data collection (200GB)
  - Custom tokenizer training
  - Model architecture (1.5B params)
  - Distributed training (multi-GPU)
  - Checkpointing et reprise
  - Evaluation benchmarks
  - Instruction tuning
  - RLHF
  - Quantization (GPTQ + llama.cpp)
  - Deployment (vLLM)
  - Monitoring production
  - Documentation compl√®te

---

## üìä PROGRESSION RECOMMAND√âE

### **Track D√©butant ‚Üí Interm√©diaire** (3-4 mois)
```
Projets 1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5 ‚Üí 6 ‚Üí 8
```

### **Track Praticien Rapide** (2 mois)
```
Projets 5 ‚Üí 6 ‚Üí 9 ‚Üí 10
```

### **Track Expert Production** (4-6 mois)
```
Projets 1 ‚Üí 3 ‚Üí 5 ‚Üí 7 ‚Üí 9 ‚Üí 10 ‚Üí 11 ‚Üí 13 ‚Üí 14 ‚Üí 15
```

---

## üéØ REPOSITORIES GITHUB

Tous les projets auront des repositories d√©di√©s:

```
github.com/ai-bible-2026/project-01-transformer-from-scratch
github.com/ai-bible-2026/project-02-data-preparation-pipeline
...
github.com/ai-bible-2026/project-15-llm-from-scratch
```

Chaque repo contient:
- ‚úÖ Code source complet et comment√©
- ‚úÖ README d√©taill√©
- ‚úÖ Requirements.txt / environment.yml
- ‚úÖ Notebooks Jupyter de d√©monstration
- ‚úÖ Datasets (ou instructions de t√©l√©chargement)
- ‚úÖ Checkpoints pr√©-entra√Æn√©s (si applicable)
- ‚úÖ Documentation API
- ‚úÖ Tests unitaires

---

## üí° CONSEILS G√âN√âRAUX

### **Avant de commencer**
1. Setup environnement (conda/venv)
2. V√©rifier hardware requirements
3. Lire le chapitre th√©orique correspondant
4. Cloner le repository du projet

### **Pendant le projet**
1. Suivre les √©tapes dans l'ordre
2. Comprendre chaque ligne de code (ne pas copier-coller)
3. Exp√©rimenter avec les hyperparam√®tres
4. Documenter vos observations
5. D√©bugger m√©thodiquement

### **Apr√®s le projet**
1. Comparer r√©sultats avec benchmarks
2. Cr√©er un notebook de d√©monstration
3. Partager sur LinkedIn/Twitter
4. Ajouter au portfolio

---

## üÜò SUPPORT

- **Discord**: #projet-X-help
- **GitHub Issues**: Pour bugs/questions
- **Office Hours**: Hebdomadaires (live coding)

---

**Pr√™t √† construire? Let's code! üöÄ**

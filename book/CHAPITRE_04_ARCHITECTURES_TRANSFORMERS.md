# CHAPITRE 4 : ARCHITECTURES TRANSFORMERS - SOUS LE CAPOT

> *¬´ Attention is all you need. ¬ª ‚Äî Trois mots qui ont chang√© l'IA pour toujours. Mais que se cache-t-il vraiment sous le capot de cette architecture r√©volutionnaire ?*

---

## üìñ Table des mati√®res

1. [Introduction : La R√©volution de l'Attention](#1-introduction)
2. [Anatomie d'un Transformer](#2-anatomie)
3. [Self-Attention : Le C≈ìur du Syst√®me](#3-self-attention)
4. [Multi-Head Attention](#4-multi-head-attention)
5. [Position Encodings](#5-position-encodings)
6. [Feed-Forward Networks](#6-feed-forward)
7. [Layer Normalization & Residual Connections](#7-layer-norm)
8. [Les Trois Familles d'Architectures](#8-trois-familles)
9. [Variantes Modernes](#9-variantes-modernes)
10. [Impl√©mentation from Scratch](#10-implementation)
11. [Quiz Interactif](#11-quiz)
12. [Exercices Pratiques](#12-exercices)
13. [Conclusion](#13-conclusion)
14. [Ressources](#14-ressources)

---

## 1. Introduction : La R√©volution de l'Attention {#1-introduction}

### üé≠ Dialogue : Le Myst√®re du Transformer

**Alice** : Bob, j'ai entendu dire que les Transformers ont r√©volutionn√© l'IA. Mais concr√®tement, qu'est-ce qui les rend si sp√©ciaux ?

**Bob** : Imagine que tu lis une phrase : "La banque a refus√© mon pr√™t car mon **compte** √©tait insuffisant."

**Alice** : Ok...

**Bob** : Pour comprendre "compte", tu dois regarder "banque" et "pr√™t". Un RNN lirait mot par mot, s√©quentiellement. Un Transformer **regarde tous les mots en m√™me temps** et calcule : "compte est li√© √† banque (attention forte) et √† pr√™t (attention forte), mais pas √† 'La' (attention faible)".

**Alice** : C'est comme avoir une vision globale plut√¥t que tunnel !

**Bob** : Exactement. Et cette capacit√© √† "pr√™ter attention" √† n'importe quel mot, peu importe la distance, c'est le secret de leur puissance.

### üìä Avant et Apr√®s les Transformers

| Aspect | RNN/LSTM (avant 2017) | Transformer (2017+) |
|--------|----------------------|---------------------|
| **Traitement** | S√©quentiel (lent) | Parall√®le (rapide) |
| **M√©moire longue** | Oublie apr√®s ~100 tokens | Attention sur 1000s de tokens |
| **Entra√Ænement** | Difficile (vanishing gradients) | Stable |
| **Scalabilit√©** | Limit√©e | Excellente |
| **SOTA sur NLP** | Peu de t√¢ches | Toutes les t√¢ches |

### üéØ Anecdote : La Naissance des Transformers

**√ât√© 2017, Google Brain, Mountain View**

Une √©quipe de chercheurs men√©e par Ashish Vaswani travaille sur la traduction automatique. Les mod√®les LSTM atteignent un plateau de performance.

*Vaswani* : "Et si on supprimait compl√®tement la r√©currence ? Si on ne gardait que l'attention ?"

*Coll√®gue* : "Impossible. Comment le mod√®le saurait l'ordre des mots ?"

*Vaswani* : "Avec des positional encodings. L'attention pour le contenu, les encodings pour la position."

6 mois plus tard, le paper **"Attention is All You Need"** sort. R√©sultats sur WMT translation :
- **LSTM SOTA** : 25.2 BLEU
- **Transformer (base)** : 27.3 BLEU
- **Transformer (big)** : **28.4 BLEU** (nouveau record)

Et surtout : **10x plus rapide √† entra√Æner**.

Le reste appartient √† l'histoire : BERT (2018), GPT-2 (2019), GPT-3 (2020), ChatGPT (2022)...

### üéØ Objectifs du Chapitre

√Ä la fin de ce chapitre, vous saurez :

- ‚úÖ Comprendre chaque composant du Transformer (attention, FFN, layer norm, etc.)
- ‚úÖ Impl√©menter un Transformer from scratch en PyTorch
- ‚úÖ Distinguer encoder-only, decoder-only, et encoder-decoder
- ‚úÖ Conna√Ætre les variantes modernes (GPT, BERT, T5, LLaMA)
- ‚úÖ Optimiser les Transformers (Flash Attention, ALiBi, etc.)

**Difficult√©** : üî¥üî¥üî¥‚ö™‚ö™ (Avanc√©)
**Pr√©requis** : Alg√®bre lin√©aire, r√©seaux de neurones, PyTorch
**Temps de lecture** : ~120 minutes

---

## 2. Anatomie d'un Transformer {#2-anatomie}

### 2.1 Vue d'Ensemble

Un Transformer est compos√© de **blocs empil√©s**, chacun contenant :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INPUT EMBEDDINGS            ‚îÇ
‚îÇ    (tokens ‚Üí vectors 512D)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      POSITIONAL ENCODING            ‚îÇ
‚îÇ   (injecter l'ordre des mots)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  TRANSFORMER  ‚îÇ √óN layers (ex: 12)
       ‚îÇ     BLOCK     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     ‚îÇ
    ‚ñº                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MULTI-HEAD ‚îÇ      ‚îÇ MULTI-HEAD ‚îÇ
‚îÇ ATTENTION  ‚îÇ      ‚îÇ ATTENTION  ‚îÇ
‚îÇ (self)     ‚îÇ      ‚îÇ (cross)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                   ‚îÇ
      ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ADD &     ‚îÇ      ‚îÇ  ADD &     ‚îÇ
‚îÇ  NORM      ‚îÇ      ‚îÇ  NORM      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                   ‚îÇ
      ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FEED-      ‚îÇ
‚îÇ FORWARD    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ADD &     ‚îÇ
‚îÇ  NORM      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        LINEAR + SOFTMAX             ‚îÇ
‚îÇ     (pr√©diction du token)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Hyperparam√®tres Typiques

| Mod√®le | Layers (N) | Hidden Dim (d_model) | Heads | Params | Context |
|--------|-----------|---------------------|-------|--------|---------|
| **BERT-base** | 12 | 768 | 12 | 110M | 512 |
| **BERT-large** | 24 | 1024 | 16 | 340M | 512 |
| **GPT-2** | 12 | 768 | 12 | 117M | 1024 |
| **GPT-3** | 96 | 12288 | 96 | 175B | 2048 |
| **LLaMA-7B** | 32 | 4096 | 32 | 7B | 2048 |
| **LLaMA-65B** | 80 | 8192 | 64 | 65B | 2048 |

### üí° Analogie : Le Transformer comme une Usine

Imaginez une **usine de compr√©hension de texte** :

1. **Entr√©e** : Camions de mots arrivent
2. **Embeddings** : Chaque mot re√ßoit un badge num√©rique (vecteur 512D)
3. **Position** : On tamponne "1er", "2√®me", etc. sur les badges
4. **Attention** : Salle de r√©union o√π chaque mot discute avec tous les autres
5. **Feed-Forward** : Chaque mot passe par une machine de transformation individuelle
6. **Sortie** : Produits finis (pr√©dictions, traductions, etc.)

Et on r√©p√®te 12-96 fois (selon le mod√®le) !

---

## 3. Self-Attention : Le C≈ìur du Syst√®me {#3-self-attention}

### 3.1 Le Probl√®me √† R√©soudre

**Phrase** : "The **animal** didn't cross the street because **it** was too tired."

**Question** : √Ä quoi r√©f√®re "it" ?

Un humain comprend imm√©diatement : **"it" = "the animal"** (pas "the street").

Comment le mod√®le peut-il le d√©duire ? Via **l'attention** !

### 3.2 M√©canisme d'Attention : Queries, Keys, Values

**Id√©e** : Chaque mot g√©n√®re 3 vecteurs :

1. **Query (Q)** : "Ce que je cherche"
2. **Key (K)** : "Ce que je propose"
3. **Value (V)** : "L'information que je porte"

**Processus** :
1. Calculer **scores d'attention** : Similitude entre Q de "it" et K de tous les mots
2. Normaliser avec **softmax**
3. Pond√©rer les **Values** par ces scores
4. Sommer pour obtenir la repr√©sentation finale

### 3.3 Formule Math√©matique

```
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) √ó V
```

O√π :
- `Q` : Matrice de queries (shape: [seq_len, d_k])
- `K` : Matrice de keys (shape: [seq_len, d_k])
- `V` : Matrice de values (shape: [seq_len, d_v])
- `d_k` : Dimension des keys (pour normalisation)
- `‚àöd_k` : Scaling factor (√©vite que softmax sature)

### 3.4 Visualisation √âtape par √âtape

**Phrase** : "The cat sat"

**√âtape 1 : Embeddings**
```
The  ‚Üí [0.2, 0.5, 0.1, ...] (512D)
cat  ‚Üí [0.8, 0.1, 0.3, ...]
sat  ‚Üí [0.3, 0.7, 0.2, ...]
```

**√âtape 2 : Projections lin√©aires**
```
Q_the = W_q √ó emb_the
K_the = W_k √ó emb_the
V_the = W_v √ó emb_the

(idem pour "cat" et "sat")
```

**√âtape 3 : Scores d'attention (pour "cat")**
```
score_cat‚Üíthe = Q_cat ¬∑ K_the / ‚àöd_k = 0.3
score_cat‚Üícat = Q_cat ¬∑ K_cat / ‚àöd_k = 0.9  (forte!)
score_cat‚Üísat = Q_cat ¬∑ K_sat / ‚àöd_k = 0.6
```

**√âtape 4 : Softmax**
```
weights = softmax([0.3, 0.9, 0.6])
        = [0.15, 0.55, 0.30]
```

**√âtape 5 : Pond√©ration des Values**
```
output_cat = 0.15√óV_the + 0.55√óV_cat + 0.30√óV_sat
```

**Interpr√©tation** : "cat" pr√™te **55% d'attention √† lui-m√™me**, 30% √† "sat", 15% √† "the".

### 3.5 Impl√©mentation PyTorch

```python
import torch
import torch.nn as nn
import math

class ScaledDotProductAttention(nn.Module):
    """
    Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) √ó V
    """
    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: Queries [batch, seq_len, d_k]
            K: Keys    [batch, seq_len, d_k]
            V: Values  [batch, seq_len, d_v]
            mask: Masque [batch, seq_len, seq_len] (optionnel)

        Returns:
            output: [batch, seq_len, d_v]
            attention_weights: [batch, seq_len, seq_len]
        """
        d_k = Q.size(-1)

        # 1. Calcul des scores : QK^T / ‚àöd_k
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        # Shape: [batch, seq_len, seq_len]

        # 2. Application du masque (pour causal attention dans GPT)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # 3. Softmax sur la derni√®re dimension
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # 4. Pond√©ration des values
        output = torch.matmul(attention_weights, V)
        # Shape: [batch, seq_len, d_v]

        return output, attention_weights


# Test
batch_size, seq_len, d_model = 2, 5, 512
Q = torch.randn(batch_size, seq_len, d_model)
K = torch.randn(batch_size, seq_len, d_model)
V = torch.randn(batch_size, seq_len, d_model)

attention = ScaledDotProductAttention()
output, weights = attention(Q, K, V)

print(f"Output shape: {output.shape}")        # [2, 5, 512]
print(f"Attention weights: {weights.shape}")  # [2, 5, 5]
print(f"Somme des poids (ligne 1): {weights[0, 0].sum()}")  # ‚âà 1.0 (softmax)
```

### 3.6 Masking : Causal vs Bidirectionnel

#### A) Bidirectional Attention (BERT)

Chaque token voit **tous les tokens** (pass√© + futur).

```
Matrice d'attention (no mask):
     The  cat  sat
The  [1]  [1]  [1]   ‚Üê "The" voit tout
cat  [1]  [1]  [1]   ‚Üê "cat" voit tout
sat  [1]  [1]  [1]   ‚Üê "sat" voit tout
```

**Usage** : Compr√©hension de texte (classification, NER, etc.)

#### B) Causal Attention (GPT)

Chaque token voit **seulement le pass√©** (pas de triche !).

```
Matrice d'attention (causal mask):
     The  cat  sat
The  [1]  [0]  [0]   ‚Üê "The" ne voit que lui-m√™me
cat  [1]  [1]  [0]   ‚Üê "cat" voit The + cat
sat  [1]  [1]  [1]   ‚Üê "sat" voit tout le pass√©
```

**Impl√©mentation du masque** :
```python
def create_causal_mask(seq_len):
    """
    Cr√©e un masque triangulaire inf√©rieur.
    """
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    return ~mask  # Inverser : True = peut voir, False = masqu√©

# Exemple
mask = create_causal_mask(5)
print(mask)
# tensor([[ True, False, False, False, False],
#         [ True,  True, False, False, False],
#         [ True,  True,  True, False, False],
#         [ True,  True,  True,  True, False],
#         [ True,  True,  True,  True,  True]])
```

**Usage** : G√©n√©ration de texte (autoregressive).

---

## 4. Multi-Head Attention {#4-multi-head-attention}

### 4.1 Pourquoi Plusieurs T√™tes ?

**Probl√®me** : Une seule attention capture **un seul type de relation**.

**Exemple** :
- T√™te 1 : Relations syntaxiques (sujet-verbe)
- T√™te 2 : Relations s√©mantiques (cor√©f√©rences)
- T√™te 3 : Relations positionnelles (mots adjacents)

**Solution** : **Plusieurs t√™tes en parall√®le** !

### 4.2 Architecture Multi-Head

```
Input (d_model=512)
    ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº          ‚ñº          ‚ñº          ‚ñº
  Head 1     Head 2    Head 3    ... Head h
  (64D)      (64D)     (64D)         (64D)
    ‚îÇ          ‚îÇ         ‚îÇ            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
            Concatenate
                  ‚îÇ
              Linear(512)
                  ‚îÇ
               Output
```

**Formule** :
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) √ó W_O

o√π head_i = Attention(Q√óW_Q^i, K√óW_K^i, V√óW_V^i)
```

### 4.3 Impl√©mentation PyTorch

```python
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention avec h t√™tes parall√®les.
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0, "d_model doit √™tre divisible par num_heads"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # Dimension par t√™te

        # Projections lin√©aires pour Q, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # Projection de sortie
        self.W_o = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q, K, V: [batch, seq_len, d_model]
            mask: [batch, seq_len, seq_len] (optionnel)

        Returns:
            output: [batch, seq_len, d_model]
        """
        batch_size = Q.size(0)

        # 1. Projections lin√©aires
        Q = self.W_q(Q)  # [batch, seq_len, d_model]
        K = self.W_k(K)
        V = self.W_v(V)

        # 2. Reshape pour multi-head : (batch, seq_len, d_model) ‚Üí (batch, num_heads, seq_len, d_k)
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # 3. Attention sur chaque t√™te
        if mask is not None:
            mask = mask.unsqueeze(1)  # Broadcast pour toutes les t√™tes

        attn_output, attn_weights = self.attention(Q, K, V, mask)
        # attn_output: [batch, num_heads, seq_len, d_k]

        # 4. Concat√©ner les t√™tes
        attn_output = attn_output.transpose(1, 2).contiguous()
        # [batch, seq_len, num_heads, d_k]

        attn_output = attn_output.view(batch_size, -1, self.d_model)
        # [batch, seq_len, d_model]

        # 5. Projection finale
        output = self.W_o(attn_output)
        output = self.dropout(output)

        return output, attn_weights


# Test
d_model, num_heads = 512, 8
batch_size, seq_len = 2, 10

x = torch.randn(batch_size, seq_len, d_model)
mha = MultiHeadAttention(d_model, num_heads)

output, weights = mha(x, x, x)  # Self-attention : Q=K=V
print(f"Output shape: {output.shape}")  # [2, 10, 512]
print(f"Nombre de t√™tes: {num_heads}")
print(f"Dimension par t√™te: {d_model // num_heads}")  # 64
```

### 4.4 Visualisation des T√™tes

**Phrase** : "The cat sat on the mat"

**T√™te 1 (syntaxe)** :
```
     The  cat  sat  on  the  mat
The  0.1  0.2  0.1  0.1 0.4  0.1
cat  0.1  0.6  0.2  0.0 0.0  0.1  ‚Üê "cat" attend √† "sat" (sujet‚Üíverbe)
sat  0.0  0.5  0.3  0.2 0.0  0.0
```

**T√™te 2 (s√©mantique)** :
```
     The  cat  sat  on  the  mat
mat  0.0  0.4  0.0  0.5 0.0  0.1  ‚Üê "mat" attend √† "cat" et "on" (relations s√©mantiques)
```

**Observation** : Chaque t√™te apprend **des patterns diff√©rents** !

### üí° Analogie : L'Orchestre

- **Violons (T√™te 1)** : Jouent la m√©lodie syntaxique
- **Contrebasses (T√™te 2)** : Jouent les relations s√©mantiques profondes
- **Percussions (T√™te 3)** : Marquent les positions et rythmes
- **Chef d'orchestre (Projection W_O)** : Harmonise tout

---

## 5. Position Encodings {#5-position-encodings}

### 5.1 Le Probl√®me

**Attention is position-agnostic** : L'attention seule ne distingue pas :
- "The cat ate the mouse"
- "The mouse ate the cat"

Sans information de position, les deux phrases auraient les **m√™mes repr√©sentations** !

### 5.2 Solution : Positional Encodings

**Id√©e** : Ajouter un vecteur de position √† chaque embedding.

```
final_embedding = word_embedding + positional_encoding
```

### 5.3 Encodage Sinuso√Ødal (Original Transformer)

**Formule** :
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

O√π :
- `pos` : Position du token (0, 1, 2, ...)
- `i` : Indice de la dimension (0 √† d_model/2)
- `2i, 2i+1` : Dimensions paires et impaires

**Propri√©t√©s** :
- ‚úÖ Valeurs born√©es [-1, 1]
- ‚úÖ D√©terministe (pas de param√®tres √† apprendre)
- ‚úÖ Fonctionne pour s√©quences arbitrairement longues
- ‚úÖ Relations lin√©aires : PE(pos+k) peut √™tre exprim√© comme fonction de PE(pos)

### 5.4 Impl√©mentation

```python
class PositionalEncoding(nn.Module):
    """
    Positional Encoding sinuso√Ødal (Vaswani et al. 2017).
    """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

        # Cr√©er la matrice PE : [max_len, d_model]
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-math.log(10000.0) / d_model))

        # Appliquer sin aux indices pairs, cos aux impairs
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)  # Ne sera pas entra√Æn√©

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]

        Returns:
            x + positional encoding
        """
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :]
        return self.dropout(x)


# Visualisation
import matplotlib.pyplot as plt

d_model = 512
pe = PositionalEncoding(d_model)

# Cr√©er un input factice
dummy_input = torch.zeros(1, 100, d_model)
output = pe(dummy_input)

# Extraire les encodings
encodings = pe.pe[0, :100, :].numpy()

plt.figure(figsize=(15, 5))
plt.imshow(encodings.T, aspect='auto', cmap='RdBu')
plt.xlabel('Position')
plt.ylabel('Dimension')
plt.colorbar()
plt.title('Positional Encodings (sinuso√Ødal)')
plt.tight_layout()
# plt.savefig('positional_encodings.png')
```

### 5.5 Variantes Modernes

| M√©thode | Description | Mod√®le |
|---------|-------------|--------|
| **Sinuso√Ødal** | Fixe, bas√© sur sin/cos | Transformer original |
| **Learned** | Param√®tres appris | BERT, GPT-2 |
| **Relative** | Encodages relatifs (distance entre tokens) | T5, Transformer-XL |
| **ALiBi** | Biais d'attention bas√©s sur distance | LLaMA, MPT |
| **RoPE** | Rotary Position Embeddings | LLaMA, GPT-NeoX |

#### RoPE (Rotary Position Embedding)

**Id√©e** : Faire tourner les vecteurs Q et K selon leur position.

```python
def apply_rotary_emb(x, position):
    """
    Applique RoPE (simplifi√©).
    """
    seq_len, d = x.shape
    freqs = 1.0 / (10000 ** (torch.arange(0, d, 2).float() / d))
    t = position.float()
    freqs = torch.outer(t, freqs)  # [seq_len, d/2]

    # Construire matrice de rotation
    cos_freqs = freqs.cos()
    sin_freqs = freqs.sin()

    # Rotation (application sur x)
    x_rot = torch.zeros_like(x)
    x_rot[:, 0::2] = x[:, 0::2] * cos_freqs - x[:, 1::2] * sin_freqs
    x_rot[:, 1::2] = x[:, 0::2] * sin_freqs + x[:, 1::2] * cos_freqs

    return x_rot
```

**Avantages** :
- Meilleure extrapolation √† des s√©quences plus longues
- Conserve les distances relatives
- Utilis√© dans LLaMA, GPT-NeoX

---

## 6. Feed-Forward Networks {#6-feed-forward}

### 6.1 R√¥le du FFN

Apr√®s l'attention, chaque token passe par un **r√©seau feed-forward** :

```
FFN(x) = max(0, x√óW_1 + b_1) √ó W_2 + b_2
       = ReLU(x√óW_1 + b_1) √ó W_2 + b_2
```

**Structure** :
- Couche 1 : `d_model ‚Üí d_ff` (expansion, typiquement d_ff = 4 √ó d_model)
- Activation : ReLU (ou GeLU)
- Couche 2 : `d_ff ‚Üí d_model` (compression)

**Intuition** : Transformation non-lin√©aire appliqu√©e **ind√©pendamment** √† chaque position.

### 6.2 Impl√©mentation

```python
class FeedForward(nn.Module):
    """
    Position-wise Feed-Forward Network.
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()  # Ou GeLU pour BERT/GPT

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]

        Returns:
            output: [batch, seq_len, d_model]
        """
        x = self.linear1(x)       # [batch, seq_len, d_ff]
        x = self.activation(x)
        x = self.dropout(x)
        x = self.linear2(x)       # [batch, seq_len, d_model]
        x = self.dropout(x)
        return x


# Exemple
d_model, d_ff = 512, 2048
ffn = FeedForward(d_model, d_ff)

x = torch.randn(2, 10, d_model)
output = ffn(x)
print(f"Output shape: {output.shape}")  # [2, 10, 512]

# Nombre de param√®tres
params = sum(p.numel() for p in ffn.parameters())
print(f"Param√®tres FFN: {params:,}")  # ~2M params pour d_model=512
```

### 6.3 Variantes d'Activation

| Activation | Formule | Mod√®le |
|------------|---------|--------|
| **ReLU** | max(0, x) | Transformer original |
| **GeLU** | x √ó Œ¶(x) | BERT, GPT-2, GPT-3 |
| **SwiGLU** | Swish(xW) ‚äô (xV) | LLaMA, PaLM |
| **GeGLU** | GeLU(xW) ‚äô (xV) | GLaM |

**GeLU** (Gaussian Error Linear Unit) est devenu le standard :
```python
import torch.nn.functional as F

def gelu(x):
    """
    GeLU activation : approximation de x √ó Œ¶(x)
    """
    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))

# Ou directement avec PyTorch
x = torch.randn(10)
y = F.gelu(x)
```

---

## 7. Layer Normalization & Residual Connections {#7-layer-norm}

### 7.1 Residual Connections (Skip Connections)

**Probl√®me** : R√©seaux profonds (96 couches pour GPT-3) souffrent de vanishing gradients.

**Solution** : Connexions r√©siduelles (He et al. 2016, ResNet)

```
output = x + SubLayer(x)
```

**Dans un Transformer** :
```
# Apr√®s attention
x = x + MultiHeadAttention(x)

# Apr√®s FFN
x = x + FeedForward(x)
```

**Avantage** : Les gradients peuvent "sauter" les couches ‚Üí entra√Ænement stable.

### 7.2 Layer Normalization

**Normalisation** : Stabilise l'entra√Ænement en normalisant les activations.

**Formule** :
```
LayerNorm(x) = Œ≥ √ó (x - Œº) / ‚àö(œÉ¬≤ + Œµ) + Œ≤
```

O√π :
- `Œº` : Moyenne sur la dimension des features
- `œÉ¬≤` : Variance sur la dimension des features
- `Œ≥, Œ≤` : Param√®tres appris (scale & shift)
- `Œµ` : Petit terme pour stabilit√© num√©rique (1e-5)

**Diff√©rence avec Batch Norm** :
- **Batch Norm** : Normalise sur le batch (probl√©matique pour NLP car s√©quences de longueurs variables)
- **Layer Norm** : Normalise sur les features (chaque exemple ind√©pendamment)

### 7.3 Impl√©mentation

```python
class LayerNorm(nn.Module):
    """
    Layer Normalization.
    """
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]

        Returns:
            normalized x
        """
        mean = x.mean(dim=-1, keepdim=True)  # [batch, seq_len, 1]
        std = x.std(dim=-1, keepdim=True)

        return self.gamma * (x - mean) / (std + self.eps) + self.beta


# Test
x = torch.randn(2, 10, 512)
ln = LayerNorm(512)
output = ln(x)

print(f"Input mean: {x.mean():.4f}, std: {x.std():.4f}")
print(f"Output mean: {output.mean():.4f}, std: {output.std():.4f}")
# Output devrait avoir mean ‚âà 0, std ‚âà 1 (par dimension)
```

### 7.4 Pre-Norm vs Post-Norm

**Post-Norm (Original Transformer)** :
```python
# Attention
x = x + MultiHeadAttention(x)
x = LayerNorm(x)

# FFN
x = x + FeedForward(x)
x = LayerNorm(x)
```

**Pre-Norm (Moderne, ex: GPT-2)** :
```python
# Attention
x = x + MultiHeadAttention(LayerNorm(x))

# FFN
x = x + FeedForward(LayerNorm(x))
```

**Avantages Pre-Norm** :
- ‚úÖ Plus stable pour r√©seaux tr√®s profonds
- ‚úÖ Peut entra√Æner sans learning rate warmup
- ‚ùå Performance l√©g√®rement inf√©rieure parfois

**Mod√®les** :
- **Post-Norm** : BERT, Transformer original
- **Pre-Norm** : GPT-2, GPT-3, LLaMA

---

## 8. Les Trois Familles d'Architectures {#8-trois-familles}

### 8.1 Encoder-Only (BERT)

**Usage** : Compr√©hension de texte (classification, NER, QA)

**Architecture** :
```
[CLS] The cat sat [SEP]
   ‚Üì    ‚Üì   ‚Üì   ‚Üì    ‚Üì
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ Bidirectional Attn  ‚îÇ (tous les tokens se voient)
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
   Representations
```

**Caract√©ristiques** :
- ‚úÖ Attention bidirectionnelle (voit futur)
- ‚úÖ Excellent pour classification
- ‚ùå Ne peut pas g√©n√©rer de texte

**Exemples** : BERT, RoBERTa, ALBERT, DeBERTa

### 8.2 Decoder-Only (GPT)

**Usage** : G√©n√©ration de texte (chat, compl√©tion, etc.)

**Architecture** :
```
The cat sat
 ‚Üì   ‚Üì   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Causal Attn  ‚îÇ (masque triangulaire)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚Üì
   Predict next token
```

**Caract√©ristiques** :
- ‚úÖ G√©n√©ration autoregressive
- ‚úÖ Scaling excellent (GPT-3, GPT-4)
- ‚ùå Ne voit que le pass√©

**Exemples** : GPT, GPT-2, GPT-3, GPT-4, LLaMA, Claude

### 8.3 Encoder-Decoder (T5)

**Usage** : Seq2seq (traduction, r√©sum√©)

**Architecture** :
```
Encoder                Decoder
Source: "Hello"        Target: "Bonjour"
   ‚Üì                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇBidir Attn‚îÇ          ‚îÇCausal+   ‚îÇ
‚îÇ          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇCross Attn‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Caract√©ristiques** :
- ‚úÖ Encoder : comprend l'input
- ‚úÖ Decoder : g√©n√®re l'output
- ‚úÖ Cross-attention : Decoder attend sur Encoder
- ‚ùå Plus complexe, plus de param√®tres

**Exemples** : T5, BART, mT5, Flan-T5

### 8.4 Comparaison

| Aspect | Encoder-Only | Decoder-Only | Encoder-Decoder |
|--------|-------------|--------------|-----------------|
| **Attention** | Bidirectionnelle | Causale | Les deux |
| **G√©n√©ration** | ‚ùå | ‚úÖ | ‚úÖ |
| **Compr√©hension** | ‚úÖ | ‚ö†Ô∏è (via prompting) | ‚úÖ |
| **T√¢ches** | Classification, NER | Chat, code, QA | Traduction, r√©sum√© |
| **Scaling** | Moyen | Excellent | Bon |
| **Exemples** | BERT | GPT, LLaMA | T5, BART |

---

## 9. Variantes Modernes {#9-variantes-modernes}

### 9.1 Optimisations d'Attention

#### A) Flash Attention

**Probl√®me** : Attention classique est O(n¬≤) en m√©moire pour s√©quence de longueur n.

**Solution** : Flash Attention (Dao et al. 2022) utilise **tiling** pour r√©duire acc√®s m√©moire.

**R√©sultats** :
- 3-4√ó plus rapide
- Utilise jusqu'√† 10√ó moins de m√©moire
- **Exact** (pas d'approximation)

```python
# Utilisation avec PyTorch 2.0+
import torch.nn.functional as F

# Activer Flash Attention (si GPU compatible)
with torch.backends.cuda.sdp_kernel(enable_flash=True):
    output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)
```

#### B) Sparse Attention

**Probl√®me** : O(n¬≤) interdit contextes ultra-longs.

**Solutions** :
- **Local Attention** : Chaque token attend seulement sur voisins proches
- **Strided Attention** : Attention sur 1 token sur k
- **Block Sparse Attention** : Patterns pr√©-d√©finis

**Mod√®les** : Longformer, BigBird, Sparse Transformer

#### C) Linear Attention

**Id√©e** : Approximer attention en O(n) au lieu de O(n¬≤).

**M√©thodes** :
- **Performers** (Choromanski et al. 2021) : Kernel trick avec random features
- **Linformer** : Projections low-rank de K et V

**Compromis** : ‚úÖ Rapide, ‚ùå Moins expressif

### 9.2 Alternatives Architecturales

| Mod√®le | Innovation | Avantage |
|--------|-----------|----------|
| **Transformer-XL** | Recurrence + relative position | Contextes ultra-longs |
| **Reformer** | LSH attention | M√©moire O(n log n) |
| **Synthesizer** | Apprendre patterns d'attention | Pas besoin de QK |
| **S4 (State Spaces)** | Remplacer attention par SSM | O(n) temps et m√©moire |
| **Mamba** | SSM optimis√©s | Alternative aux Transformers |

### 9.3 LLaMA : Transformer Optimis√©

**Innovations LLaMA (Meta, 2023)** :

1. **RoPE** : Rotary Position Embeddings
2. **SwiGLU** : Activation dans FFN
3. **RMSNorm** : Simplification de LayerNorm
4. **Pre-Norm** : Normalisation avant attention/FFN

```python
class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization (Zhang & Sennrich, 2019).
    Utilis√© dans LLaMA, T5.
    """
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(d_model))

    def forward(self, x):
        """
        RMSNorm(x) = x / RMS(x) √ó Œ≥
        o√π RMS(x) = ‚àö(mean(x¬≤))
        """
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        return self.weight * x / rms
```

---

## 10. Impl√©mentation from Scratch {#10-implementation}

### 10.1 Bloc Transformer Complet

```python
class TransformerBlock(nn.Module):
    """
    Un bloc Transformer complet (Multi-Head Attention + FFN + Layer Norm).
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()

        self.attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.norm1 = LayerNorm(d_model)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm2 = LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        Args:
            x: [batch, seq_len, d_model]
            mask: [batch, seq_len, seq_len] (optionnel)

        Returns:
            output: [batch, seq_len, d_model]
        """
        # Sub-layer 1: Multi-Head Attention
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))  # Residual + Norm

        # Sub-layer 2: Feed-Forward
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))  # Residual + Norm

        return x


# Test
block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)
x = torch.randn(2, 10, 512)
output = block(x)
print(f"Output shape: {output.shape}")  # [2, 10, 512]
```

### 10.2 Decoder GPT-Style Complet

```python
class GPTDecoder(nn.Module):
    """
    Decoder Transformer style GPT (causal, decoder-only).
    """
    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6,
                 d_ff=2048, max_len=1024, dropout=0.1):
        super().__init__()

        self.d_model = d_model

        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_encoding = PositionalEncoding(d_model, max_len, dropout)

        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.norm = LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        # Tie weights (embedding = lm_head)
        self.lm_head.weight = self.token_embedding.weight

    def forward(self, input_ids, mask=None):
        """
        Args:
            input_ids: [batch, seq_len] (token IDs)
            mask: [batch, seq_len, seq_len] (causal mask)

        Returns:
            logits: [batch, seq_len, vocab_size]
        """
        batch_size, seq_len = input_ids.shape

        # 1. Token embeddings
        x = self.token_embedding(input_ids) * math.sqrt(self.d_model)
        # [batch, seq_len, d_model]

        # 2. Positional encodings
        x = self.position_encoding(x)

        # 3. Cr√©er causal mask si non fourni
        if mask is None:
            mask = create_causal_mask(seq_len).to(input_ids.device)
            mask = mask.unsqueeze(0).expand(batch_size, -1, -1)

        # 4. Passer √† travers les blocs Transformer
        for block in self.blocks:
            x = block(x, mask)

        # 5. Normalisation finale
        x = self.norm(x)

        # 6. Projection vers vocabulaire
        logits = self.lm_head(x)
        # [batch, seq_len, vocab_size]

        return logits


# Instanciation
vocab_size = 50000
model = GPTDecoder(vocab_size, d_model=512, num_heads=8, num_layers=6)

# Nombre total de param√®tres
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")  # ~80M params

# Test forward pass
input_ids = torch.randint(0, vocab_size, (2, 20))  # Batch de 2, s√©quence de 20
logits = model(input_ids)
print(f"Logits shape: {logits.shape}")  # [2, 20, 50000]

# G√©n√©ration du prochain token
next_token_logits = logits[:, -1, :]  # Dernier token
next_token = torch.argmax(next_token_logits, dim=-1)
print(f"Predicted next tokens: {next_token}")  # [token_id_1, token_id_2]
```

### 10.3 G√©n√©ration Autoregressive

```python
@torch.no_grad()
def generate(model, input_ids, max_new_tokens=50, temperature=1.0, top_k=None):
    """
    G√©n√®re du texte de mani√®re autoregressive.

    Args:
        model: GPTDecoder
        input_ids: [batch, seq_len] - Prompt
        max_new_tokens: Nombre de tokens √† g√©n√©rer
        temperature: Contr√¥le l'al√©atoire (0 = d√©terministe, >1 = al√©atoire)
        top_k: √âchantillonner parmi les top-k tokens

    Returns:
        generated_ids: [batch, seq_len + max_new_tokens]
    """
    model.eval()

    for _ in range(max_new_tokens):
        # Forward pass
        logits = model(input_ids)  # [batch, seq_len, vocab_size]

        # Prendre les logits du dernier token
        logits = logits[:, -1, :] / temperature  # [batch, vocab_size]

        # Top-k filtering
        if top_k is not None:
            v, _ = torch.topk(logits, top_k)
            logits[logits < v[:, [-1]]] = -float('Inf')

        # Softmax pour probabilit√©s
        probs = F.softmax(logits, dim=-1)  # [batch, vocab_size]

        # √âchantillonner
        next_token = torch.multinomial(probs, num_samples=1)  # [batch, 1]

        # Concat√©ner
        input_ids = torch.cat([input_ids, next_token], dim=1)

    return input_ids


# Exemple de g√©n√©ration
prompt = torch.tensor([[1, 2, 3, 4, 5]])  # Token IDs du prompt
generated = generate(model, prompt, max_new_tokens=20, temperature=0.8, top_k=50)
print(f"Generated sequence: {generated}")
```

---

## 11. Quiz Interactif {#11-quiz}

### Question 1 : Complexit√© de l'Attention

**Quelle est la complexit√© computationnelle de l'attention standard pour une s√©quence de longueur n ?**

A) O(n)
B) O(n log n)
C) O(n¬≤)
D) O(n¬≥)

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C) O(n¬≤)**

Le calcul de `QK^T` produit une matrice de taille `[n, n]`, n√©cessitant O(n¬≤) op√©rations. C'est le principal goulot d'√©tranglement des Transformers pour les longues s√©quences.

**Solutions** :
- Flash Attention (optimisation m√©moire)
- Sparse Attention (attention limit√©e)
- Linear Attention (approximations)
</details>

---

### Question 2 : Multi-Head Attention

**Pourquoi utiliser 8 t√™tes d'attention au lieu d'une seule ?**

A) Pour parall√©liser sur 8 GPUs
B) Pour capturer diff√©rents types de relations
C) Pour augmenter le nombre de param√®tres
D) Pour acc√©l√©rer l'entra√Ænement

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B) Pour capturer diff√©rents types de relations**

Chaque t√™te apprend des patterns diff√©rents :
- T√™te 1 : Relations syntaxiques (sujet-verbe)
- T√™te 2 : Cor√©f√©rences (it ‚Üí cat)
- T√™te 3 : Voisinage local
- etc.

**Nombre de param√®tres** : Identique entre 1 t√™te de dimension 512 et 8 t√™tes de dimension 64 !
</details>

---

### Question 3 : Positional Encodings

**Que se passe-t-il si on omet les positional encodings ?**

A) Le mod√®le ne compile pas
B) "The cat ate the mouse" = "The mouse ate the cat"
C) L'entra√Ænement est plus rapide
D) Rien, l'attention capture l'ordre

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B) "The cat ate the mouse" = "The mouse ate the cat"**

L'attention est **invariante par permutation** : sans encodings de position, l'ordre des mots est ignor√©. Les deux phrases auraient des repr√©sentations identiques !

**Solution** : Positional encodings (sinuso√Ødal, learned, RoPE, etc.)
</details>

---

### Question 4 : Causal Masking

**Dans GPT, pourquoi utilise-t-on un masque causal ?**

A) Pour acc√©l√©rer le training
B) Pour emp√™cher de "tricher" en voyant le futur
C) Pour √©conomiser de la m√©moire
D) C'est une erreur historique

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B) Pour emp√™cher de "tricher" en voyant le futur**

En g√©n√©ration autoregressive, chaque token ne doit voir que le **pass√©**. Sinon, durant l'entra√Ænement, le mod√®le "triche" en regardant le token qu'il doit pr√©dire !

**Masque causal** : Triangle inf√©rieur de 1s, reste √† 0.
</details>

---

### Question 5 : Pre-Norm vs Post-Norm

**Quelle affirmation est vraie sur Pre-Norm ?**

A) Plus ancien que Post-Norm
B) Meilleur pour r√©seaux tr√®s profonds (>50 layers)
C) Toujours meilleure performance
D) Invent√© pour BERT

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B) Meilleur pour r√©seaux tr√®s profonds (>50 layers)**

**Pre-Norm** (LayerNorm avant attention/FFN) stabilise l'entra√Ænement pour mod√®les tr√®s profonds comme GPT-3 (96 layers).

**Trade-off** : L√©g√®rement moins performant sur certaines t√¢ches, mais entra√Ænement plus stable et sans warmup.
</details>

---

### Question 6 : Encoder vs Decoder

**Quelle architecture pour une t√¢che de classification de sentiment ?**

A) Encoder-only (BERT)
B) Decoder-only (GPT)
C) Encoder-Decoder (T5)
D) Toutes √©quivalentes

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : A) Encoder-only (BERT)**

**Classification** = comprendre le texte (pas g√©n√©rer). L'encoder-only avec attention bidirectionnelle est optimal.

**GPT** peut aussi faire de la classification (via prompting), mais moins efficace.
</details>

---

## 12. Exercices Pratiques {#12-exercices}

### Exercice 1 : Visualiser les Attention Weights

**Objectif** : Cr√©er une heatmap des poids d'attention pour comprendre ce que le mod√®le "regarde".

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, tokens):
    """
    Visualise les poids d'attention sous forme de heatmap.

    Args:
        attention_weights: [seq_len, seq_len] (numpy array)
        tokens: Liste de tokens (strings)
    """
    # TODO: Cr√©er une heatmap avec seaborn
    # Axes: tokens (source et target)
    # Couleur: intensit√© de l'attention
    pass

# Test
tokens = ["The", "cat", "sat", "on", "the", "mat"]
# Simuler des poids (en vrai, extraire depuis model)
weights = torch.softmax(torch.randn(6, 6), dim=-1).numpy()

visualize_attention(weights, tokens)
```

<details>
<summary>Voir la solution</summary>

```python
def visualize_attention(attention_weights, tokens):
    """
    Visualise les poids d'attention sous forme de heatmap.
    """
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attention_weights,
        xticklabels=tokens,
        yticklabels=tokens,
        annot=True,
        fmt='.2f',
        cmap='Blues',
        cbar_kws={'label': 'Attention Weight'}
    )
    plt.xlabel('Key (Source)')
    plt.ylabel('Query (Target)')
    plt.title('Attention Weights Heatmap')
    plt.tight_layout()
    plt.show()

# Exemple avec vraie attention
model = GPTDecoder(vocab_size=100, d_model=64, num_heads=4, num_layers=2)
input_ids = torch.randint(0, 100, (1, 6))

# Hook pour extraire les poids
attention_weights_list = []

def hook_fn(module, input, output):
    # output[1] contient les attention weights
    attention_weights_list.append(output[1].detach())

# Enregistrer le hook sur la premi√®re t√™te
model.blocks[0].attention.attention.register_forward_hook(hook_fn)

# Forward
_ = model(input_ids)

# Extraire et visualiser
weights = attention_weights_list[0][0, 0].numpy()  # [seq_len, seq_len]
tokens = [f"T{i}" for i in range(6)]
visualize_attention(weights, tokens)
```
</details>

---

### Exercice 2 : Impl√©menter Learned Positional Embeddings

**Objectif** : Remplacer les encodings sinuso√Ødaux par des embeddings appris (comme dans GPT-2).

```python
class LearnedPositionalEmbedding(nn.Module):
    """
    Positional embeddings appris (param√®tres entra√Ænables).
    """
    def __init__(self, max_len, d_model, dropout=0.1):
        super().__init__()
        # TODO: Cr√©er un Embedding de taille [max_len, d_model]
        # TODO: Ajouter dropout
        pass

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]

        Returns:
            x + positional embeddings
        """
        # TODO: R√©cup√©rer les embeddings pour positions 0..seq_len-1
        # TODO: Ajouter √† x
        pass
```

<details>
<summary>Voir la solution</summary>

```python
class LearnedPositionalEmbedding(nn.Module):
    """
    Positional embeddings appris (GPT-2 style).
    """
    def __init__(self, max_len, d_model, dropout=0.1):
        super().__init__()
        self.position_embeddings = nn.Embedding(max_len, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]
        """
        batch_size, seq_len, d_model = x.shape

        # Positions: [0, 1, 2, ..., seq_len-1]
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        # [1, seq_len]

        # Embeddings positionnels
        pos_emb = self.position_embeddings(positions)
        # [1, seq_len, d_model]

        # Ajouter aux embeddings de tokens
        x = x + pos_emb
        return self.dropout(x)

# Test
x = torch.randn(2, 10, 512)
pos_emb = LearnedPositionalEmbedding(max_len=1024, d_model=512)
output = pos_emb(x)
print(f"Output shape: {output.shape}")  # [2, 10, 512]

# Nombre de param√®tres
params = sum(p.numel() for p in pos_emb.parameters())
print(f"Param√®tres: {params:,}")  # 1024 √ó 512 = 524,288
```
</details>

---

### Exercice 3 : G√©n√©rer avec Different Sampling Strategies

**Objectif** : Impl√©menter greedy, top-k, top-p (nucleus), et temperature sampling.

```python
def sample_next_token(logits, strategy='greedy', temperature=1.0, top_k=None, top_p=None):
    """
    √âchantillonne le prochain token selon diff√©rentes strat√©gies.

    Args:
        logits: [vocab_size] - Scores pour chaque token
        strategy: 'greedy', 'top_k', 'top_p', 'temperature'
        temperature: Contr√¥le l'al√©atoire
        top_k: Nombre de tokens √† consid√©rer (top-k)
        top_p: Probabilit√© cumulative (nucleus sampling)

    Returns:
        next_token: Index du token √©chantillonn√©
    """
    # TODO: Impl√©menter les 4 strat√©gies
    pass
```

<details>
<summary>Voir la solution</summary>

```python
def sample_next_token(logits, strategy='greedy', temperature=1.0, top_k=None, top_p=None):
    """
    √âchantillonne le prochain token.
    """
    if strategy == 'greedy':
        # Toujours prendre le token le plus probable
        return torch.argmax(logits).item()

    # Appliquer temperature
    logits = logits / temperature

    if strategy == 'temperature':
        # √âchantillonnage selon distribution de probabilit√©
        probs = F.softmax(logits, dim=-1)
        return torch.multinomial(probs, num_samples=1).item()

    elif strategy == 'top_k':
        # Top-k sampling
        assert top_k is not None, "top_k doit √™tre sp√©cifi√©"
        v, indices = torch.topk(logits, top_k)
        logits_filtered = torch.full_like(logits, -float('Inf'))
        logits_filtered[indices] = v

        probs = F.softmax(logits_filtered, dim=-1)
        return torch.multinomial(probs, num_samples=1).item()

    elif strategy == 'top_p':
        # Nucleus (top-p) sampling
        assert top_p is not None, "top_p doit √™tre sp√©cifi√©"

        # Trier par probabilit√© d√©croissante
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        sorted_probs = F.softmax(sorted_logits, dim=-1)

        # Probabilit√© cumulative
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

        # Retirer tokens dont cumsum > top_p
        sorted_indices_to_remove = cumulative_probs > top_p
        # Garder au moins 1 token
        sorted_indices_to_remove[0] = False

        # Cr√©er masque
        indices_to_remove = sorted_indices_to_remove.scatter(
            0, sorted_indices, sorted_indices_to_remove
        )
        logits_filtered = logits.clone()
        logits_filtered[indices_to_remove] = -float('Inf')

        probs = F.softmax(logits_filtered, dim=-1)
        return torch.multinomial(probs, num_samples=1).item()

# Test
vocab_size = 50000
logits = torch.randn(vocab_size)

print("Greedy:", sample_next_token(logits, 'greedy'))
print("Temperature 0.7:", sample_next_token(logits, 'temperature', temperature=0.7))
print("Top-k (k=50):", sample_next_token(logits, 'top_k', top_k=50))
print("Top-p (p=0.9):", sample_next_token(logits, 'top_p', top_p=0.9))
```
</details>

---

## 13. Conclusion {#13-conclusion}

### üé≠ Dialogue Final : L'√âl√©gance du Transformer

**Alice** : Apr√®s tout √ßa, je r√©alise que le Transformer est... √©tonnamment simple.

**Bob** : Exactement ! C'est son g√©nie. Pas de magie, juste :
1. **Attention** : Regarder tous les mots simultan√©ment
2. **Feed-Forward** : Transformer chaque mot ind√©pendamment
3. **Residual + Norm** : Stabiliser l'entra√Ænement
4. **R√©p√©ter N fois**

**Alice** : Et de ce pattern simple naissent GPT-4, Claude, Gemini...

**Bob** : Oui. Le Transformer est comme les √©checs : **r√®gles simples, complexit√© √©mergente**. En empilant ces blocs et en ajoutant des milliards de param√®tres, on obtient des capacit√©s qu'on ne comprend pas encore totalement.

**Alice** : Fascinant. Et terrifiant.

**Bob** : Bienvenue dans l'√®re des LLMs. üöÄ

### üéØ Points Cl√©s √† Retenir

| Concept | Essence |
|---------|---------|
| **Self-Attention** | Q, K, V ‚Üí softmax(QK^T/‚àöd_k) √ó V |
| **Multi-Head** | Plusieurs attentions parall√®les = patterns multiples |
| **Positional Encoding** | Sin/cos ou learned pour ordre des mots |
| **Feed-Forward** | Transformation non-lin√©aire par position |
| **Residual + Norm** | Stabilit√© pour r√©seaux profonds |
| **Encoder-only** | BERT = compr√©hension bidirectionnelle |
| **Decoder-only** | GPT = g√©n√©ration causale |
| **Encoder-Decoder** | T5 = seq2seq (traduction, r√©sum√©) |

### üìä Architecture Parameters

**GPT-2 Small** (117M params) :
- 12 layers, 768 dim, 12 heads
- Context: 1024 tokens
- FFN: 3072 dim (4√ó expansion)

**GPT-3** (175B params) :
- 96 layers, 12288 dim, 96 heads
- Context: 2048 tokens
- FFN: 49152 dim

**Scaling Law** : Performances ‚àù ‚àöParams (environ)

### üöÄ Prochaines √âtapes

Maintenant que vous ma√Ætrisez l'architecture Transformer :

1. **Chapitre 7 : Fine-Tuning** ‚Üí Adapter un Transformer pr√©-entra√Æn√©
2. **Chapitre 10 : Optimization** ‚Üí Flash Attention, quantization, etc.
3. **Chapitre 13 : LoRA** ‚Üí Fine-tuning efficient

---

## 14. Ressources {#14-ressources}

### üìö Papers Fondamentaux

1. **"Attention is All You Need"** (Vaswani et al., 2017)
   - Le paper original des Transformers

2. **"BERT: Pre-training of Deep Bidirectional Transformers"** (Devlin et al., 2018)
   - Encoder-only, masked language modeling

3. **"Improving Language Understanding by Generative Pre-Training"** (Radford et al., 2018)
   - GPT-1, decoder-only

4. **"Language Models are Unsupervised Multitask Learners"** (GPT-2, Radford et al., 2019)

5. **"Language Models are Few-Shot Learners"** (GPT-3, Brown et al., 2020)

6. **"Flash Attention: Fast and Memory-Efficient Exact Attention"** (Dao et al., 2022)

7. **"LLaMA: Open and Efficient Foundation Language Models"** (Touvron et al., 2023)

### üõ†Ô∏è Impl√©mentations de R√©f√©rence

```bash
# Transformers from scratch (didactique)
https://github.com/karpathy/minGPT
https://github.com/hyunwoongko/transformer

# Production (HuggingFace)
pip install transformers torch

# Optimisations
pip install flash-attn  # Flash Attention
pip install xformers     # Optimized attention variants
```

### üîó Tutoriels et Visualisations

- **The Illustrated Transformer** : https://jalammar.github.io/illustrated-transformer/
- **Attention Visualizer** : https://github.com/jessevig/bertviz
- **Tensor2Tensor** (Google) : https://github.com/tensorflow/tensor2tensor

---

**üéì Bravo !** Vous comprenez maintenant les Transformers de l'int√©rieur. Dans le prochain chapitre, nous explorerons comment **tokenizer** le texte avant de le passer au Transformer ! üöÄ


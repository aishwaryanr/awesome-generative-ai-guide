# CHAPITRE 15 : D√âPLOIEMENT ET PRODUCTION

> *¬´ In theory, there is no difference between theory and practice. In practice, there is. ¬ª*
> ‚Äî Yogi Berra

---

## Introduction : Du Notebook au Monde R√©el

Vous avez entra√Æn√© un LLM. Vous avez fine-tun√©, optimis√©, √©valu√©. Dans votre notebook Jupyter, tout fonctionne parfaitement. Le mod√®le g√©n√®re des r√©ponses brillantes. Les m√©triques sont excellentes.

**Et maintenant ?**

Comment passer de "√ßa marche sur mon laptop" √† "√ßa sert 10 000 requ√™tes par seconde en production avec une latence < 200ms et un SLA de 99.9%" ?

C'est tout l'enjeu du **d√©ploiement en production** : transformer un prototype en un syst√®me robuste, scalable, observable, et rentable.

Dans ce chapitre, nous couvrirons :
- **Architectures de serving** : API REST, streaming, batching
- **Frameworks d'inf√©rence** : vLLM, Text Generation Inference, TensorRT-LLM
- **Optimisations** : quantization, KV-cache, batching continu
- **Infrastructure** : GPU, Kubernetes, autoscaling
- **Monitoring** : m√©triques, tracing, alerting
- **Co√ªts** : calcul, optimisation, pricing

Bienvenue dans le monde r√©el des LLMs en production.

---

## 1. Architecture d'un Syst√®me LLM en Production

### üé≠ Dialogue : Les D√©fis de la Production

**Alice** : Bob, j'ai fine-tun√© GPT-2 pour mon use case. Comment je le mets en production pour que mes utilisateurs puissent y acc√©der ?

**Bob** : Excellente question ! D√©ployer un LLM en production, c'est beaucoup plus que "lancer un serveur".

**Alice** : C'est-√†-dire ?

**Bob** : R√©fl√©chis aux contraintes :
- **Latence** : Les utilisateurs attendent < 2 secondes, pas 30 secondes
- **Throughput** : Tu dois g√©rer 100, 1000, peut-√™tre 10 000 requ√™tes par seconde
- **Co√ªt** : Les GPUs co√ªtent cher, chaque milliseconde compte
- **Disponibilit√©** : 99.9% uptime minimum (< 9 heures de downtime par an)
- **Scalabilit√©** : Pic de trafic le lundi matin ? Il faut scaler automatiquement
- **Observabilit√©** : Quand √ßa casse (et √ßa cassera), il faut savoir pourquoi

**Alice** : Wow, c'est beaucoup ! Par o√π commencer ?

**Bob** : Commen√ßons par l'architecture de base, puis nous optimiserons.

---

### 1.1 Architecture Simplifi√©e

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CLIENT                         ‚îÇ
‚îÇ  (App mobile, Web, CLI, etc.)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚îÇ HTTPS
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LOAD BALANCER                       ‚îÇ
‚îÇ  (NGINX, AWS ALB, GCP Load Balancer)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚îÇ Distribue les requ√™tes
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          API GATEWAY / BACKEND                   ‚îÇ
‚îÇ  (FastAPI, Flask, Express.js)                    ‚îÇ
‚îÇ  ‚Ä¢ Authentification                              ‚îÇ
‚îÇ  ‚Ä¢ Rate limiting                                 ‚îÇ
‚îÇ  ‚Ä¢ Validation des inputs                        ‚îÇ
‚îÇ  ‚Ä¢ Logging                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚îÇ gRPC / HTTP
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INFERENCE SERVICE                        ‚îÇ
‚îÇ  (vLLM, TGI, TensorRT-LLM, Custom)               ‚îÇ
‚îÇ  ‚Ä¢ KV-Cache optimization                         ‚îÇ
‚îÇ  ‚Ä¢ Batching continu                              ‚îÇ
‚îÇ  ‚Ä¢ Quantization                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚îÇ CUDA
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                GPU(s)                            ‚îÇ
‚îÇ  (A100, H100, L4, T4, etc.)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Composants cl√©s** :

1. **Load Balancer** : Distribue le trafic sur plusieurs instances
2. **API Gateway** : G√®re l'authentification, le rate limiting, la validation
3. **Inference Service** : Ex√©cute le mod√®le (le c≈ìur du syst√®me)
4. **GPU** : Calcul parall√®le pour l'inf√©rence

---

### üìú Anecdote Historique : Le Lancement de ChatGPT (30 novembre 2022)

**OpenAI, San Francisco** : Le 30 novembre 2022, OpenAI lance ChatGPT en "research preview". L'√©quipe s'attend √† quelques milliers d'utilisateurs.

**5 jours plus tard** : 1 million d'utilisateurs.
**2 mois plus tard** : 100 millions d'utilisateurs actifs (record absolu).

**Le d√©fi** : Scaler l'infrastructure pour supporter cette croissance explosive.

**Solutions mises en place** :
- **Autoscaling agressif** sur Azure (partenariat OpenAI-Microsoft)
- **File d'attente** : "ChatGPT is at capacity right now"
- **Throttling** : Limitation du nombre de messages par heure
- **Geographic distribution** : Serveurs en Am√©rique du Nord, Europe, Asie
- **Mod√®les optimis√©s** : Passage de GPT-3.5 initial √† GPT-3.5-turbo (2x plus rapide, 10x moins cher)

**Le√ßon** : M√™me avec une infrastructure de classe mondiale, la production r√©serve des surprises. Il faut **over-engineer** pour la scalabilit√©.

---

## 2. Frameworks d'Inf√©rence

### 2.1 Comparaison des Frameworks

| Framework | D√©veloppeur | Sp√©cialit√© | Throughput | Latence | Ease of Use |
|-----------|-------------|------------|------------|---------|-------------|
| **vLLM** | UC Berkeley | Batching continu, PagedAttention | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Text Generation Inference (TGI)** | Hugging Face | Int√©gration HF, streaming | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **TensorRT-LLM** | NVIDIA | Performance maximale, FP8 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **llama.cpp** | Georgi Gerganov | CPU inference, quantization | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **FastAPI + Transformers** | Custom | Flexibilit√© maximale | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

### 2.2 vLLM : Le Standard de Facto

**vLLM** est devenu le framework de r√©f√©rence pour servir des LLMs en production gr√¢ce √† **PagedAttention** et au **continuous batching**.

#### Installation et D√©marrage

```bash
# Installation
pip install vllm

# Lancer le serveur (API compatible OpenAI)
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --dtype auto \
    --api-key sk-my-secret-key

# Le serveur d√©marre sur http://localhost:8000
```

#### Client Python

```python
from openai import OpenAI

# vLLM expose une API compatible OpenAI
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-my-secret-key"
)

# Requ√™te standard
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing in simple terms."}
    ],
    temperature=0.7,
    max_tokens=200
)

print(response.choices[0].message.content)
```

#### Streaming

```python
# Streaming pour une meilleure UX
stream = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[{"role": "user", "content": "Write a haiku about AI"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

### 2.3 Text Generation Inference (TGI)

**TGI** de Hugging Face offre une int√©gration parfaite avec l'√©cosyst√®me HF et un excellent support du streaming.

#### Lancement avec Docker

```bash
# Lancer TGI avec Docker
docker run --gpus all --shm-size 1g -p 8080:80 \
    -v $PWD/data:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-chat-hf \
    --num-shard 1 \
    --max-total-tokens 4096 \
    --max-batch-prefill-tokens 4096
```

#### Client Python

```python
import requests

url = "http://localhost:8080/generate"

headers = {"Content-Type": "application/json"}

data = {
    "inputs": "What is the capital of France?",
    "parameters": {
        "max_new_tokens": 100,
        "temperature": 0.7,
        "top_p": 0.9,
        "do_sample": True
    }
}

response = requests.post(url, headers=headers, json=data)
print(response.json()["generated_text"])
```

#### Streaming

```python
# Streaming avec TGI
data = {
    "inputs": "Write a story about a robot",
    "parameters": {"max_new_tokens": 500},
    "stream": True
}

with requests.post(url + "_stream", headers=headers, json=data, stream=True) as r:
    for line in r.iter_lines():
        if line:
            import json
            chunk = json.loads(line.decode('utf-8').replace('data:', ''))
            if 'token' in chunk:
                print(chunk['token']['text'], end='', flush=True)
```

---

### 2.4 Service Custom avec FastAPI

Pour un contr√¥le total, cr√©er un service custom :

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from typing import Optional, List

app = FastAPI(title="Custom LLM API")

# Charger le mod√®le au d√©marrage
class ModelManager:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self, model_name: str):
        print(f"Loading model {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("Model loaded successfully")

model_manager = ModelManager()

@app.on_event("startup")
async def startup_event():
    model_manager.load_model("meta-llama/Llama-2-7b-chat-hf")


# Mod√®les de requ√™te/r√©ponse
class GenerationRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.9
    stop: Optional[List[str]] = None


class GenerationResponse(BaseModel):
    generated_text: str
    tokens_generated: int
    latency_ms: float


@app.post("/generate", response_model=GenerationResponse)
async def generate(request: GenerationRequest):
    """G√©n√®re du texte √† partir d'un prompt."""
    import time

    start_time = time.time()

    try:
        # Tokenization
        inputs = model_manager.tokenizer(
            request.prompt,
            return_tensors="pt"
        ).to(model_manager.device)

        # G√©n√©ration
        with torch.no_grad():
            outputs = model_manager.model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True,
                pad_token_id=model_manager.tokenizer.eos_token_id
            )

        # D√©codage
        generated_text = model_manager.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        latency = (time.time() - start_time) * 1000  # ms

        return GenerationResponse(
            generated_text=generated_text,
            tokens_generated=len(outputs[0]) - inputs['input_ids'].shape[1],
            latency_ms=latency
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "model_loaded": model_manager.model is not None,
        "device": model_manager.device
    }


@app.get("/metrics")
async def metrics():
    """M√©triques du service."""
    import torch

    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        gpu_memory_max = torch.cuda.max_memory_allocated() / 1024**3
    else:
        gpu_memory = 0
        gpu_memory_max = 0

    return {
        "gpu_memory_allocated_gb": gpu_memory,
        "gpu_memory_max_gb": gpu_memory_max,
        "device": model_manager.device
    }


# Lancer avec : uvicorn app:app --host 0.0.0.0 --port 8000
```

**Utilisation** :

```python
import requests

response = requests.post(
    "http://localhost:8000/generate",
    json={
        "prompt": "Once upon a time in a land far away,",
        "max_tokens": 150,
        "temperature": 0.8
    }
)

result = response.json()
print(f"Generated text: {result['generated_text']}")
print(f"Latency: {result['latency_ms']:.2f}ms")
print(f"Tokens: {result['tokens_generated']}")
```

---

## 3. Optimisations d'Inf√©rence

### üé≠ Dialogue : Pourquoi Mon Mod√®le Est Si Lent ?

**Alice** : Bob, mon LLM en production prend 5 secondes par requ√™te. C'est beaucoup trop lent ! Pourquoi ?

**Bob** : Plusieurs raisons possibles. Regarde ce qui se passe pendant l'inf√©rence :

**Bob** : 1. **Loading du mod√®le** : Si tu recharges le mod√®le √† chaque requ√™te, √ßa peut prendre des secondes.

**Alice** : Ah oui, je le charge en m√©moire une seule fois au d√©marrage.

**Bob** : Bien. 2. **Tokenization** : C'est g√©n√©ralement rapide, mais v√©rifie quand m√™me.

**Bob** : 3. **Forward passes** : C'est l√† que √ßa peut √™tre lent. Pour g√©n√©rer 100 tokens, tu fais 100 forward passes !

**Alice** : Attends, un forward pass par token ?

**Bob** : Oui ! Les LLMs sont **autor√©gressifs** : ils g√©n√®rent un token, puis utilisent ce token comme input pour g√©n√©rer le suivant, etc.

**Alice** : Je vois... Et comment acc√©l√©rer ?

**Bob** : Plusieurs techniques :
- **KV-Cache** : √©viter de recalculer l'attention pour les tokens d√©j√† g√©n√©r√©s
- **Batching** : traiter plusieurs requ√™tes en parall√®le
- **Quantization** : r√©duire la pr√©cision (FP16, INT8, INT4)
- **Flash Attention** : optimisation de l'attention
- **Compilation** : TorchScript, ONNX, TensorRT

**Alice** : Par o√π commencer ?

**Bob** : KV-Cache et quantization, ce sont les quick wins.

---

### 3.1 KV-Cache : L'Optimisation Essentielle

**Probl√®me** : Sans KV-cache, on recalcule l'attention pour **tous** les tokens √† chaque √©tape.

```python
# Sans KV-cache (inefficace)
tokens_generated = []

for i in range(max_tokens):
    # √Ä chaque it√©ration, on recalcule l'attention pour TOUS les tokens
    # (prompt + tokens d√©j√† g√©n√©r√©s)
    output = model(tokens_prompt + tokens_generated)  # ‚ùå LENT
    next_token = sample(output[-1])
    tokens_generated.append(next_token)
```

**Solution** : **KV-Cache** stocke les cl√©s (K) et valeurs (V) de l'attention pour les tokens d√©j√† trait√©s.

```python
# Avec KV-cache (efficace)
past_key_values = None
tokens_generated = []

for i in range(max_tokens):
    if i == 0:
        # Premier passage : traiter tout le prompt
        input_ids = tokens_prompt
    else:
        # Passages suivants : seulement le dernier token
        input_ids = [tokens_generated[-1]]

    output = model(
        input_ids,
        past_key_values=past_key_values,  # ‚úÖ R√©utiliser le cache
        use_cache=True
    )

    past_key_values = output.past_key_values  # Mettre √† jour le cache
    next_token = sample(output.logits[-1])
    tokens_generated.append(next_token)
```

**Gain** : 5x-10x plus rapide pour la g√©n√©ration.

---

### 3.2 Quantization : R√©duire la Pr√©cision

La **quantization** r√©duit la pr√©cision des poids pour √©conomiser m√©moire et calcul.

| Pr√©cision | M√©moire (7B mod√®le) | Performance | Qualit√© |
|-----------|---------------------|-------------|---------|
| **FP32** | 28 GB | Baseline | 100% |
| **FP16** | 14 GB | 1.5-2x faster | ~99.9% |
| **INT8** | 7 GB | 2-3x faster | ~99% |
| **INT4** | 3.5 GB | 3-4x faster | ~95-98% |

#### Quantization avec bitsandbytes

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Configuration pour quantization INT8
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16
)

# Charger le mod√®le en INT8
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-chat-hf",
    quantization_config=bnb_config,
    device_map="auto"
)

print(f"Model size in memory: {model.get_memory_footprint() / 1024**3:.2f} GB")
```

#### Quantization INT4 (GPTQ)

```python
# Quantization INT4 avec GPTQ (encore plus agressif)
from transformers import GPTQConfig

gptq_config = GPTQConfig(
    bits=4,
    dataset="c4",  # Dataset de calibration
    tokenizer=tokenizer
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-chat-hf",
    quantization_config=gptq_config,
    device_map="auto"
)

# Un mod√®le 7B tient maintenant dans ~3.5 GB !
```

---

### 3.3 Batching Continu (Continuous Batching)

**Probl√®me du batching classique** : Attendre que toutes les requ√™tes du batch se terminent.

```
Batch 1:
Request A: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (28 tokens, 2.8s)
Request B: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (6 tokens, 0.6s) ... attente 2.2s ‚ùå
Request C: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (12 tokens, 1.2s) ... attente 1.6s ‚ùå
```

**Continuous Batching** (vLLM) : Ajouter/retirer des requ√™tes du batch dynamiquement.

```
Request A: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (28 tokens)
Request B: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚Üí termin√©, remplac√© par Request D imm√©diatement ‚úÖ
Request C: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚Üí termin√©, remplac√© par Request E ‚úÖ
Request D: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Request E: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
```

**R√©sultat** : Throughput am√©lior√© de 2-3x.

**vLLM g√®re √ßa automatiquement** ‚Äî c'est pourquoi il est si performant !

---

### 3.4 Compilation et Optimisation

#### TorchScript

```python
# Compiler le mod√®le avec TorchScript
model_scripted = torch.jit.script(model)
model_scripted.save("model_scripted.pt")

# Charger le mod√®le compil√©
model_loaded = torch.jit.load("model_scripted.pt")

# G√©n√©ralement 10-20% plus rapide
```

#### torch.compile (PyTorch 2.0+)

```python
# PyTorch 2.0 : compilation automatique
import torch

model = AutoModelForCausalLM.from_pretrained("gpt2")
model = torch.compile(model)  # ‚ú® Magic

# Premier appel : lent (compilation)
# Appels suivants : 30-50% plus rapides !
```

---

## 4. Infrastructure et Scaling

### 4.1 Choix du GPU

| GPU | VRAM | FP16 Throughput | Prix/h (cloud) | Use Case |
|-----|------|-----------------|----------------|----------|
| **T4** | 16 GB | ~6 TFLOPS | $0.35 | Petits mod√®les (< 7B) |
| **L4** | 24 GB | ~60 TFLOPS | $0.70 | 7B-13B mod√®les |
| **A10G** | 24 GB | ~35 TFLOPS | $1.00 | 7B-13B mod√®les |
| **A100 40GB** | 40 GB | ~312 TFLOPS | $3.00 | 13B-30B mod√®les |
| **A100 80GB** | 80 GB | ~312 TFLOPS | $4.50 | 30B-70B mod√®les |
| **H100** | 80 GB | ~1000 TFLOPS | $8.00+ | 70B+ mod√®les |

**R√®gle empirique** : Vous avez besoin de ~2x la taille du mod√®le en VRAM (pour FP16 + KV-cache + overhead).

**Exemple** :
- **LLaMA-2 7B** en FP16 : ~14 GB ‚Üí T4/L4 suffisent
- **LLaMA-2 13B** en FP16 : ~26 GB ‚Üí A100 40GB ou L4 avec quantization
- **LLaMA-2 70B** en FP16 : ~140 GB ‚Üí A100 80GB x2 ou H100

---

### 4.2 D√©ploiement Kubernetes

#### Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference
  namespace: ml-services
spec:
  replicas: 3  # 3 instances pour haute disponibilit√©
  selector:
    matchLabels:
      app: llm-inference
  template:
    metadata:
      labels:
        app: llm-inference
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - --model
          - meta-llama/Llama-2-7b-chat-hf
          - --dtype
          - float16
          - --max-model-len
          - "4096"
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "32Gi"
            cpu: "8"
          limits:
            nvidia.com/gpu: 1
            memory: "32Gi"
            cpu: "8"
        ports:
        - containerPort: 8000
          name: http
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: llm-inference-service
  namespace: ml-services
spec:
  selector:
    app: llm-inference
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-inference-hpa
  namespace: ml-services
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

### 4.3 Autoscaling Bas√© sur la Queue

Pour des co√ªts optimaux, utilisez un syst√®me de queue avec autoscaling :

```python
# Architecture avec Celery + Redis

from celery import Celery
import redis

# Configuration Celery
app = Celery('llm_tasks', broker='redis://localhost:6379/0')

# Task de g√©n√©ration
@app.task(bind=True)
def generate_text(self, prompt: str, max_tokens: int = 100):
    """T√¢che de g√©n√©ration de texte."""
    import time
    start = time.time()

    # Appel au mod√®le
    result = model.generate(prompt, max_tokens=max_tokens)

    duration = time.time() - start

    return {
        "generated_text": result,
        "duration": duration,
        "task_id": self.request.id
    }


# API Frontend
from fastapi import FastAPI, BackgroundTasks

api = FastAPI()

@api.post("/generate_async")
async def generate_async(prompt: str):
    """Enqueue une t√¢che de g√©n√©ration."""
    task = generate_text.delay(prompt)

    return {
        "task_id": task.id,
        "status": "queued"
    }

@api.get("/result/{task_id}")
async def get_result(task_id: str):
    """R√©cup√®re le r√©sultat d'une t√¢che."""
    task = generate_text.AsyncResult(task_id)

    if task.ready():
        return {
            "status": "completed",
            "result": task.result
        }
    else:
        return {
            "status": "processing"
        }
```

**Autoscaling** : Scaler les workers Celery en fonction de la longueur de la queue.

```bash
# Kubernetes HPA bas√© sur la m√©trique custom (queue length)
kubectl autoscale deployment llm-workers \
    --cpu-percent=50 \
    --min=2 \
    --max=20 \
    --custom-metric queue-length:10
```

---

## 5. Monitoring et Observabilit√©

### üé≠ Dialogue : Pourquoi Le Monitoring Est Crucial

**Alice** : Bob, mon service LLM est en production depuis 2 semaines. Tout a l'air de fonctionner. Pourquoi tu insistes autant sur le monitoring ?

**Bob** : Parce que "√ßa a l'air de marcher" n'est pas suffisant en production. Tu as besoin de savoir :
- **Performance** : Quelle est la latence P50, P95, P99 ?
- **Throughput** : Combien de requ√™tes par seconde ?
- **Erreurs** : Quel est le taux d'erreur ? Quels types d'erreurs ?
- **Co√ªts** : Combien co√ªte chaque requ√™te en GPU time ?
- **Qualit√©** : Les r√©ponses sont-elles bonnes ?

**Alice** : D'accord, mais comment mesurer tout √ßa ?

**Bob** : Plusieurs niveaux :
1. **M√©triques syst√®me** : CPU, GPU, m√©moire
2. **M√©triques applicatives** : latence, throughput, erreurs
3. **M√©triques m√©tier** : co√ªt par requ√™te, satisfaction utilisateur
4. **Tracing** : suivre une requ√™te de bout en bout

---

### 5.1 M√©triques avec Prometheus

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from fastapi import FastAPI
import time

app = FastAPI()

# M√©triques Prometheus
REQUEST_COUNT = Counter(
    'llm_requests_total',
    'Total number of requests',
    ['endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'llm_request_duration_seconds',
    'Request latency in seconds',
    ['endpoint'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

TOKENS_GENERATED = Counter(
    'llm_tokens_generated_total',
    'Total number of tokens generated'
)

GPU_MEMORY = Gauge(
    'llm_gpu_memory_allocated_bytes',
    'GPU memory allocated in bytes'
)

QUEUE_SIZE = Gauge(
    'llm_queue_size',
    'Number of requests in queue'
)


@app.middleware("http")
async def monitor_requests(request, call_next):
    """Middleware pour monitorer toutes les requ√™tes."""
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time

    # Enregistrer les m√©triques
    REQUEST_COUNT.labels(
        endpoint=request.url.path,
        status=response.status_code
    ).inc()

    REQUEST_LATENCY.labels(
        endpoint=request.url.path
    ).observe(duration)

    return response


@app.post("/generate")
async def generate(prompt: str, max_tokens: int = 100):
    """Endpoint de g√©n√©ration avec monitoring."""
    # G√©n√©ration
    result = model.generate(prompt, max_tokens=max_tokens)

    # M√©triques
    TOKENS_GENERATED.inc(len(result.tokens))

    if torch.cuda.is_available():
        GPU_MEMORY.set(torch.cuda.memory_allocated())

    return {"generated_text": result.text}


# Exposer les m√©triques Prometheus sur le port 9090
start_http_server(9090)
```

**Grafana Dashboard** : Visualiser les m√©triques

```promql
# Latence P95
histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m]))

# Throughput (requ√™tes/seconde)
rate(llm_requests_total[1m])

# Taux d'erreur
rate(llm_requests_total{status=~"5.."}[5m]) / rate(llm_requests_total[5m])

# Tokens g√©n√©r√©s par seconde
rate(llm_tokens_generated_total[1m])
```

---

### 5.2 Tracing avec OpenTelemetry

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter

# Configuration OpenTelemetry
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)


@app.post("/generate")
async def generate_with_tracing(prompt: str):
    """G√©n√©ration avec tracing distribu√©."""
    with tracer.start_as_current_span("llm_generation") as span:
        span.set_attribute("prompt_length", len(prompt))

        # Tokenization
        with tracer.start_as_current_span("tokenization"):
            tokens = tokenizer(prompt)
            span.set_attribute("num_tokens", len(tokens['input_ids'][0]))

        # Inference
        with tracer.start_as_current_span("model_inference"):
            output = model.generate(**tokens, max_new_tokens=100)

        # Decoding
        with tracer.start_as_current_span("decoding"):
            result = tokenizer.decode(output[0])

        span.set_attribute("output_length", len(result))

        return {"generated_text": result}
```

**Visualisation dans Jaeger** : Voir exactement o√π le temps est pass√© (tokenization 5ms, inference 1.2s, decoding 8ms).

---

### 5.3 Logging Structur√©

```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    """Logger structur√© pour faciliter l'analyse."""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)

        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)

    def log_request(self, request_id: str, prompt: str, user_id: str):
        """Log une requ√™te entrante."""
        self.logger.info(json.dumps({
            "event": "request_received",
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
            "user_id": user_id,
            "prompt_length": len(prompt)
        }))

    def log_generation(self, request_id: str, tokens: int, latency: float):
        """Log une g√©n√©ration termin√©e."""
        self.logger.info(json.dumps({
            "event": "generation_completed",
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
            "tokens_generated": tokens,
            "latency_ms": latency * 1000,
            "tokens_per_second": tokens / latency if latency > 0 else 0
        }))

    def log_error(self, request_id: str, error: str):
        """Log une erreur."""
        self.logger.error(json.dumps({
            "event": "error",
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request_id,
            "error_message": str(error)
        }))


# Utilisation
logger = StructuredLogger("llm_service")

@app.post("/generate")
async def generate(prompt: str, user_id: str):
    request_id = str(uuid.uuid4())
    logger.log_request(request_id, prompt, user_id)

    start = time.time()
    try:
        result = model.generate(prompt)
        latency = time.time() - start

        logger.log_generation(request_id, len(result.tokens), latency)

        return {"text": result.text}

    except Exception as e:
        logger.log_error(request_id, str(e))
        raise
```

**Analyse avec ELK Stack** : Chercher, filtrer, agr√©ger les logs JSON.

---

## 6. Gestion des Co√ªts

### 6.1 Calculer le Co√ªt par Requ√™te

```python
class CostCalculator:
    """Calcule le co√ªt de chaque requ√™te."""

    def __init__(self, gpu_cost_per_hour: float):
        """
        Args:
            gpu_cost_per_hour: Co√ªt du GPU en $/heure (ex: A100 = $3.00/h)
        """
        self.gpu_cost_per_second = gpu_cost_per_hour / 3600

    def calculate_cost(self, latency_seconds: float, gpu_utilization: float = 1.0):
        """
        Calcule le co√ªt d'une requ√™te.

        Args:
            latency_seconds: Temps de g√©n√©ration en secondes
            gpu_utilization: Utilisation du GPU (0.0 √† 1.0)

        Returns:
            Co√ªt en dollars
        """
        cost = latency_seconds * self.gpu_cost_per_second * gpu_utilization
        return cost


# Exemple
calculator = CostCalculator(gpu_cost_per_hour=3.00)  # A100

# Requ√™te qui prend 2 secondes
cost = calculator.calculate_cost(latency_seconds=2.0)
print(f"Co√ªt par requ√™te : ${cost:.6f}")  # $0.001667

# Si on sert 10 000 requ√™tes/jour
daily_cost = cost * 10000
print(f"Co√ªt quotidien : ${daily_cost:.2f}")  # $16.67
```

---

### 6.2 Strat√©gies d'Optimisation des Co√ªts

**1. Batching Agressif**
```python
# Au lieu de traiter les requ√™tes une par une
# ‚Üí Accumuler pendant 50ms et traiter en batch

import asyncio
from collections import deque

class BatchingService:
    def __init__(self, max_batch_size=32, max_wait_ms=50):
        self.queue = deque()
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms

    async def add_request(self, prompt: str):
        """Ajoute une requ√™te √† la queue."""
        future = asyncio.Future()
        self.queue.append((prompt, future))

        # Si le batch est plein, traiter imm√©diatement
        if len(self.queue) >= self.max_batch_size:
            await self.process_batch()

        return await future

    async def process_batch(self):
        """Traite un batch de requ√™tes."""
        if not self.queue:
            return

        batch = []
        futures = []

        while self.queue and len(batch) < self.max_batch_size:
            prompt, future = self.queue.popleft()
            batch.append(prompt)
            futures.append(future)

        # Inf√©rence en batch (beaucoup plus efficace !)
        results = model.generate_batch(batch)

        # Retourner les r√©sultats
        for future, result in zip(futures, results):
            future.set_result(result)

# R√©duction du co√ªt : 3-5x gr√¢ce au batching
```

**2. Utiliser des Mod√®les Plus Petits**
```python
# Cascade de mod√®les : petit mod√®le d'abord, grand mod√®le si n√©cessaire

async def smart_generate(prompt: str):
    """Utilise un petit mod√®le, puis un grand si besoin."""

    # Essayer avec un petit mod√®le (GPT-3.5, Llama-2 7B)
    small_result = await small_model.generate(prompt)

    # V√©rifier la qualit√© (heuristique simple)
    confidence = calculate_confidence(small_result)

    if confidence > 0.8:
        # Le petit mod√®le est confiant ‚Üí utiliser sa r√©ponse
        return small_result  # Co√ªt : 10x moins cher

    else:
        # Faible confiance ‚Üí utiliser le grand mod√®le
        large_result = await large_model.generate(prompt)
        return large_result

# 70% des requ√™tes trait√©es par le petit mod√®le
# ‚Üí R√©duction de co√ªt globale : ~7x
```

**3. Caching Intelligent**
```python
import hashlib
from functools import lru_cache

class SemanticCache:
    """Cache bas√© sur la similarit√© s√©mantique."""

    def __init__(self, similarity_threshold=0.95):
        self.cache = {}
        self.embeddings_cache = {}
        self.threshold = similarity_threshold

    def get(self, prompt: str):
        """Cherche dans le cache."""
        # Calculer l'embedding du prompt
        emb = get_embedding(prompt)

        # Chercher un prompt similaire
        for cached_prompt, cached_emb in self.embeddings_cache.items():
            similarity = cosine_similarity(emb, cached_emb)

            if similarity > self.threshold:
                # Cache hit !
                return self.cache[cached_prompt]

        return None

    def set(self, prompt: str, result: str):
        """Ajoute au cache."""
        emb = get_embedding(prompt)
        self.embeddings_cache[prompt] = emb
        self.cache[prompt] = result

# 30-40% de cache hit rate sur des queries similaires
# ‚Üí Co√ªt r√©duit de 30-40%
```

---

## üß† Quiz Interactif

### Question 1
**Qu'est-ce que le KV-Cache ?**

A) Un cache pour stocker les r√©sultats des requ√™tes
B) Un cache qui stocke les cl√©s et valeurs de l'attention pour √©viter les recalculs
C) Un cache de tokenization
D) Un syst√®me de mise en cache des embeddings

<details>
<summary>üëâ Voir la r√©ponse</summary>

**R√©ponse : B**

Le **KV-Cache** stocke les matrices de **cl√©s (K)** et **valeurs (V)** de l'attention pour les tokens d√©j√† trait√©s.

Sans KV-cache, √† chaque g√©n√©ration de token, le mod√®le doit recalculer l'attention pour **tous** les tokens (prompt + tokens g√©n√©r√©s).

Avec KV-cache, on r√©utilise les K et V d√©j√† calcul√©s, et on calcule seulement pour le nouveau token.

**Gain** : 5-10x plus rapide pour la g√©n√©ration autoregressive.
</details>

---

### Question 2
**Quel est l'avantage principal du continuous batching (vLLM) par rapport au batching classique ?**

A) Utilise moins de m√©moire
B) Permet d'ajouter/retirer des requ√™tes du batch dynamiquement
C) Plus simple √† impl√©menter
D) Fonctionne seulement avec les GPUs NVIDIA

<details>
<summary>üëâ Voir la r√©ponse</summary>

**R√©ponse : B**

Le **batching classique** attend que toutes les requ√™tes du batch se terminent avant de traiter le batch suivant. Si une requ√™te g√©n√®re 100 tokens et une autre 10 tokens, les 9 slots attendent inutilement.

Le **continuous batching** (PagedAttention dans vLLM) permet de :
- Retirer les requ√™tes termin√©es du batch
- Ajouter de nouvelles requ√™tes imm√©diatement
- Maximiser l'utilisation du GPU

**R√©sultat** : Throughput am√©lior√© de 2-3x sans augmenter la latence.
</details>

---

### Question 3
**Quelle quantization offre le meilleur rapport qualit√©/co√ªt pour la plupart des cas d'usage ?**

A) FP32
B) FP16
C) INT8
D) INT4

<details>
<summary>üëâ Voir la r√©ponse</summary>

**R√©ponse : C (INT8)**

**INT8** offre g√©n√©ralement le meilleur compromis :
- **M√©moire** : R√©duction de 4x vs FP32, 2x vs FP16
- **Performance** : 2-3x plus rapide que FP16
- **Qualit√©** : ~99% de la qualit√© originale (perte minime)
- **Compatibilit√©** : Support√© par la plupart des frameworks

**FP16** : Bon si vous avez assez de VRAM et voulez la qualit√© maximale
**INT4** : Utile pour les tr√®s grands mod√®les (70B+) mais qualit√© d√©grad√©e (~95-98%)

**Best practice** : Commencer avec INT8, downgrade vers INT4 seulement si n√©cessaire.
</details>

---

### Question 4
**Pourquoi le monitoring est-il crucial en production ?**

A) Pour impressionner les managers avec des dashboards
B) Pour d√©tecter les probl√®mes avant qu'ils n'impactent les utilisateurs
C) C'est obligatoire par la loi
D) Pour r√©duire les co√ªts de 90%

<details>
<summary>üëâ Voir la r√©ponse</summary>

**R√©ponse : B**

Le monitoring permet de :
1. **D√©tecter les probl√®mes proactivement** : Latence qui augmente, taux d'erreur qui monte
2. **Diagnostiquer rapidement** : O√π est le bottleneck ? GPU satur√© ? Queue qui d√©borde ?
3. **Optimiser les co√ªts** : Identifier les requ√™tes co√ªteuses, optimiser les patterns
4. **Garantir les SLAs** : P99 latency < 2s, 99.9% uptime
5. **Comprendre l'usage** : Quels prompts ? Quelle charge ? Quels patterns ?

**Sans monitoring** : Vous √™tes aveugle. Vous d√©couvrez les probl√®mes quand les utilisateurs se plaignent.

**Avec monitoring** : Vous voyez les probl√®mes arriver et pouvez agir avant l'impact.
</details>

---

### Question 5
**Quelle strat√©gie permet de r√©duire les co√ªts de 70% en moyenne ?**

A) Utiliser des GPUs moins chers
B) Cascade de mod√®les (petit mod√®le ‚Üí grand mod√®le si n√©cessaire)
C) R√©duire la qualit√© des r√©ponses
D) Limiter le nombre de tokens g√©n√©r√©s

<details>
<summary>üëâ Voir la r√©ponse</summary>

**R√©ponse : B**

La **cascade de mod√®les** utilise un mod√®le petit et rapide (GPT-3.5, Llama-2 7B) pour la majorit√© des requ√™tes, et ne fait appel au grand mod√®le (GPT-4, Llama-2 70B) que pour les cas complexes.

**Exemple** :
- 70% des requ√™tes : GPT-3.5 (10x moins cher)
- 30% des requ√™tes : GPT-4

**Co√ªt moyen** : 0.7 √ó co√ªt_GPT35 + 0.3 √ó co√ªt_GPT4
Si GPT-4 co√ªte 10x plus cher : 0.7 √ó 1 + 0.3 √ó 10 = 3.7
**R√©duction** : ~63% vs utiliser GPT-4 pour tout

**Avec batching + cache** : R√©duction totale > 80%
</details>

---

### Question 6
**Quelle est la r√®gle empirique pour la VRAM n√©cessaire ?**

A) Taille du mod√®le √ó 1
B) Taille du mod√®le √ó 2
C) Taille du mod√®le √ó 4
D) √áa d√©pend uniquement du batch size

<details>
<summary>üëâ Voir la r√©ponse</summary>

**R√©ponse : B**

**R√®gle empirique** : VRAM n√©cessaire ‚âà **2√ó taille du mod√®le** (en FP16)

**D√©tail** :
- **Poids du mod√®le** : Taille nominale (ex: 7B √ó 2 bytes = 14 GB)
- **KV-Cache** : ~20-30% de la taille du mod√®le (d√©pend de la longueur de contexte)
- **Activations** : ~10-20% pendant le forward pass
- **Overhead** : ~10% (CUDA, PyTorch, etc.)

**Total** : ~1.4-2√ó la taille du mod√®le

**Exemples** :
- LLaMA-2 7B (FP16): ~14 GB ‚Üí **besoin de 24-32 GB** (L4, A10G, A100 40GB)
- LLaMA-2 70B (FP16): ~140 GB ‚Üí **besoin de 280 GB** (4√ó A100 80GB ou 2√ó H100)

**Avec quantization INT8** : Divisez par 2
**Avec quantization INT4** : Divisez par 4
</details>

---

## üíª Exercices Pratiques

### Exercice 1 : D√©ployer un Service avec vLLM

**Objectif** : D√©ployer LLaMA-2 7B avec vLLM et mesurer les performances.

**Consignes** :
1. Installer vLLM
2. Lancer le serveur
3. Cr√©er un client de test qui mesure latence et throughput
4. Comparer performances avec/sans batching

<details>
<summary>üëâ Voir la solution</summary>

```bash
# Installation
pip install vllm

# Lancer le serveur
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --dtype float16 \
    --max-model-len 4096
```

```python
import time
import asyncio
from openai import AsyncOpenAI
import statistics

client = AsyncOpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

async def test_single_request():
    """Test une seule requ√™te."""
    start = time.time()

    response = await client.chat.completions.create(
        model="meta-llama/Llama-2-7b-chat-hf",
        messages=[{"role": "user", "content": "What is AI?"}],
        max_tokens=100
    )

    latency = time.time() - start
    return latency

async def test_concurrent_requests(num_requests=10):
    """Test plusieurs requ√™tes en parall√®le."""
    start = time.time()

    tasks = [test_single_request() for _ in range(num_requests)]
    latencies = await asyncio.gather(*tasks)

    total_time = time.time() - start
    throughput = num_requests / total_time

    return {
        "total_time": total_time,
        "throughput": throughput,
        "mean_latency": statistics.mean(latencies),
        "p95_latency": statistics.quantiles(latencies, n=20)[18],  # P95
        "p99_latency": statistics.quantiles(latencies, n=100)[98]  # P99
    }

# Ex√©cuter les tests
async def main():
    print("Test 1: Single request")
    latency = await test_single_request()
    print(f"Latency: {latency:.3f}s\n")

    print("Test 2: 10 concurrent requests")
    results = await test_concurrent_requests(10)
    for key, value in results.items():
        print(f"{key}: {value:.3f}")

    print("\nTest 3: 50 concurrent requests")
    results = await test_concurrent_requests(50)
    for key, value in results.items():
        print(f"{key}: {value:.3f}")

asyncio.run(main())
```

</details>

---

### Exercice 2 : Impl√©menter un Cache S√©mantique

**Objectif** : Cr√©er un syst√®me de cache bas√© sur la similarit√© s√©mantique pour r√©duire les co√ªts.

<details>
<summary>üëâ Voir la solution dans le code de la section 6.2</summary>

Utilisez la classe `SemanticCache` fournie et mesurez le cache hit rate sur vos donn√©es r√©elles.
</details>

---

## üìö R√©sum√© du Chapitre

### Points Cl√©s

1. **Architecture** : Load Balancer ‚Üí API Gateway ‚Üí Inference Service ‚Üí GPU

2. **Frameworks** :
   - **vLLM** : Meilleur throughput (continuous batching)
   - **TGI** : Meilleure int√©gration HF
   - **TensorRT-LLM** : Performance maximale (NVIDIA)

3. **Optimisations** :
   - **KV-Cache** : 5-10x plus rapide
   - **Quantization INT8** : 2x plus rapide, 2x moins de VRAM
   - **Batching continu** : 2-3x meilleur throughput

4. **Infrastructure** :
   - Choix GPU bas√© sur taille mod√®le
   - Kubernetes pour orchestration
   - Autoscaling bas√© sur m√©triques

5. **Monitoring** :
   - M√©triques (Prometheus + Grafana)
   - Tracing (OpenTelemetry + Jaeger)
   - Logging structur√© (ELK Stack)

6. **Co√ªts** :
   - Batching, caching, cascade de mod√®les
   - R√©duction typique : 70-80%

---

## üöÄ Prochaine √âtape

Dans le **Chapitre 16 : S√©curit√© et √âthique**, nous explorerons :
- S√©curisation des LLMs (prompt injection, jailbreaking)
- Filtrage de contenu toxique
- Privacy et donn√©es sensibles
- Biais et fairness
- R√©glementations (RGPD, AI Act)

**√Ä tr√®s bient√¥t !** üéâ

---

## üìñ R√©f√©rences

### Frameworks
- **vLLM** : https://github.com/vllm-project/vllm
- **Text Generation Inference** : https://github.com/huggingface/text-generation-inference
- **TensorRT-LLM** : https://github.com/NVIDIA/TensorRT-LLM

### Papers
- PagedAttention (vLLM) : https://arxiv.org/abs/2309.06180
- Continuous Batching : Orca paper

### Outils
- **Prometheus** : Monitoring et alerting
- **Grafana** : Visualisation de m√©triques
- **Jaeger** : Distributed tracing
- **ELK Stack** : Elasticsearch + Logstash + Kibana

---

*Fin du Chapitre 15*

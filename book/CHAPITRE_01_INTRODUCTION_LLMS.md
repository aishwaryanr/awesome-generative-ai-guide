# CHAPITRE 1 : INTRODUCTION AUX LARGE LANGUAGE MODELS

> *Â« Any sufficiently advanced technology is indistinguishable from magic. Â»*
> â€” Arthur C. Clarke, 1962

---

## Introduction : Bienvenue dans l'Ãˆre des LLMs

**2026**. Vous ouvrez votre Ã©diteur de code, vous tapez quelques mots, et une intelligence artificielle complÃ¨te votre pensÃ©e. Vous posez une question complexe en langage naturel, et en quelques secondes, vous obtenez une rÃ©ponse structurÃ©e, argumentÃ©e, parfois mÃªme crÃ©ative. Vous demandez Ã  gÃ©nÃ©rer du code, traduire un document, rÃ©sumer un article scientifique, ou Ã©crire un email professionnel â€” et c'est fait.

Ce n'est plus de la science-fiction. C'est votre quotidien de dÃ©veloppeur, d'ingÃ©nieur, de chercheur en 2026.

Les **Large Language Models** (LLMs) ont rÃ©volutionnÃ© notre maniÃ¨re de travailler, de crÃ©er, de penser. Mais derriÃ¨re cette apparente magie se cache une ingÃ©nierie complexe, des mathÃ©matiques Ã©lÃ©gantes, des algorithmes sophistiquÃ©s, et des annÃ©es de recherche.

Ce livre est votre guide complet pour **maÃ®triser cette technologie de A Ã  Z**. Que vous soyez dÃ©veloppeur dÃ©butant ou ingÃ©nieur chevronnÃ©, que vous souhaitiez comprendre les concepts fondamentaux ou implÃ©menter des systÃ¨mes de production, ce livre vous accompagnera Ã  chaque Ã©tape.

Bienvenue dans **LA BIBLE DU DÃ‰VELOPPEUR AI/LLM 2026**.

---

## 1. Qu'est-ce qu'un Large Language Model ?

### ğŸ­ Dialogue : La DÃ©couverte

**Alice** : Bob, j'ai entendu parler de ChatGPT, GPT-4, Claude... Tout le monde parle de "LLMs". Mais au fond, qu'est-ce que c'est exactement ?

**Bob** : Imagine un programme informatique qui a "lu" une grande partie d'Internet â€” des milliards de pages web, des livres, des articles scientifiques, du code source...

**Alice** : D'accord, donc une Ã©norme base de donnÃ©es ?

**Bob** : Non, justement ! Ce n'est pas une base de donnÃ©es qui stocke du texte. C'est un **modÃ¨le statistique** qui a appris les *patterns* du langage. Il comprend comment les mots s'enchaÃ®nent, comment les phrases se construisent, comment les concepts se relient entre eux.

**Alice** : Donc il "comprend" vraiment le langage ?

**Bob** : C'est plus subtil. Il a appris Ã  *prÃ©dire le mot suivant* dans une sÃ©quence. Mais en apprenant cette tÃ¢che simple sur des milliards d'exemples, il a dÃ©veloppÃ© une comprÃ©hension implicite de la grammaire, de la sÃ©mantique, du raisonnement, et mÃªme de certains aspects de la logique et du monde rÃ©el.

**Alice** : Impressionnant... Et pourquoi "Large" ?

**Bob** : Parce qu'ils contiennent des **milliards de paramÃ¨tres**. GPT-3 en a 175 milliards. GPT-4 probablement plus de 1 trillion. Ces paramÃ¨tres sont les "neurones" du modÃ¨le, les valeurs apprises pendant l'entraÃ®nement.

---

### 1.1 DÃ©finition Formelle

Un **Large Language Model** est :

1. **Un modÃ¨le de langage** : systÃ¨me qui modÃ©lise la probabilitÃ© d'une sÃ©quence de mots (ou tokens)
2. **Neural** : basÃ© sur des rÃ©seaux de neurones profonds (deep learning)
3. **Large** : contenant des milliards de paramÃ¨tres (poids du rÃ©seau)
4. **PrÃ©-entraÃ®nÃ©** : entraÃ®nÃ© sur d'Ã©normes corpus de texte (web, livres, code)
5. **GÃ©nÃ©ratif** : capable de gÃ©nÃ©rer du texte cohÃ©rent et contextuel

MathÃ©matiquement, un modÃ¨le de langage estime :

```
P(wâ‚, wâ‚‚, ..., wâ‚™) = P(wâ‚) Ã— P(wâ‚‚|wâ‚) Ã— P(wâ‚ƒ|wâ‚,wâ‚‚) Ã— ... Ã— P(wâ‚™|wâ‚,...,wâ‚™â‚‹â‚)
```

OÃ¹ `P(wáµ¢|wâ‚,...,wáµ¢â‚‹â‚)` est la probabilitÃ© du mot `wáµ¢` sachant tous les mots prÃ©cÃ©dents.

Les LLMs utilisent des architectures **Transformer** (que nous explorerons en dÃ©tail au Chapitre 4) pour capturer ces dÃ©pendances Ã  longue distance.

---

### 1.2 Anatomie d'un LLM

Un LLM moderne se compose de plusieurs couches :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INPUT: "Le chat mange une"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TOKENIZATION                      â”‚
â”‚   ["Le", "chat", "mange", "une"]   â”‚
â”‚   â†’ [4521, 8923, 2341, 756]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EMBEDDING LAYER                   â”‚
â”‚   Chaque token â†’ vecteur dense      â”‚
â”‚   4521 â†’ [0.23, -0.45, 0.12, ...]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TRANSFORMER LAYERS (x N)          â”‚
â”‚   - Self-Attention                  â”‚
â”‚   - Feed-Forward Networks           â”‚
â”‚   - Layer Normalization             â”‚
â”‚   - Residual Connections            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OUTPUT HEAD                       â”‚
â”‚   Projection vers vocabulaire       â”‚
â”‚   â†’ ProbabilitÃ©s pour chaque token  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SAMPLING                          â”‚
â”‚   SÃ©lection du prochain token       â”‚
â”‚   â†’ "souris" (prob: 0.32)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ“œ Anecdote Historique : Le Premier "LLM"

**1948, Bell Labs, New Jersey** : Claude Shannon, mathÃ©maticien et ingÃ©nieur, publie "A Mathematical Theory of Communication". Il y introduit le concept d'**entropie de l'information** et propose une expÃ©rience : prÃ©dire la prochaine lettre dans un texte anglais en se basant sur les lettres prÃ©cÃ©dentes.

Shannon calcule manuellement les probabilitÃ©s sur des Ã©chantillons de texte et dÃ©montre qu'avec suffisamment de contexte, on peut prÃ©dire le prochain caractÃ¨re avec une certaine prÃ©cision. C'est le **premier modÃ¨le de langage statistique** de l'histoire.

75 ans plus tard, nous utilisons exactement le mÃªme principe â€” mais Ã  une Ã©chelle inimaginable pour Shannon : au lieu de quelques lettres de contexte, GPT-4 peut traiter 128 000 tokens. Au lieu de probabilitÃ©s calculÃ©es Ã  la main, nous avons 1 trillion de paramÃ¨tres entraÃ®nÃ©s sur des pÃ©taoctets de donnÃ©es.

---

## 2. Les CapacitÃ©s Ã‰mergentes des LLMs

### 2.1 Qu'est-ce qu'une "CapacitÃ© Ã‰mergente" ?

Les LLMs prÃ©sentent des **capacitÃ©s Ã©mergentes** : des compÃ©tences qui n'apparaissent qu'au-delÃ  d'une certaine Ã©chelle (taille du modÃ¨le, quantitÃ© de donnÃ©es, compute).

**Analogie** : Imaginez que vous apprenez le piano. Au dÃ©but, vous jouez des notes individuelles. Puis des accords. Puis des mÃ©lodies simples. Mais un jour, aprÃ¨s des milliers d'heures de pratique, quelque chose d'inattendu se produit : vous commencez Ã  **improviser**, Ã  crÃ©er de nouvelles mÃ©lodies sans y penser consciousement. C'est une capacitÃ© Ã©mergente de votre apprentissage musical.

De mÃªme, les LLMs dÃ©veloppent des capacitÃ©s non explicitement programmÃ©es :

#### A. Raisonnement en Plusieurs Ã‰tapes (Chain-of-Thought)

```python
# Prompt simple
prompt = "Roger a 5 balles de tennis. Il en achÃ¨te 2 boÃ®tes de 3 balles. Combien en a-t-il maintenant ?"

# Petit modÃ¨le (< 10B params) : Ã©choue souvent
# â†’ "8" (calcul incorrect)

# Grand modÃ¨le (> 50B params) avec CoT : rÃ©ussit
# â†’ "Ã‰tape 1 : Roger a 5 balles au dÃ©part
#     Ã‰tape 2 : Il achÃ¨te 2 boÃ®tes de 3 balles = 2 Ã— 3 = 6 balles
#     Ã‰tape 3 : Total = 5 + 6 = 11 balles
#     RÃ©ponse : 11"
```

#### B. Few-Shot Learning

Les LLMs peuvent apprendre de nouvelles tÃ¢ches avec seulement quelques exemples :

```python
prompt = """
Traduis les phrases suivantes en langage SMS :

Phrase : Je suis en retard, dÃ©solÃ©
SMS : chui en ret, dsl

Phrase : Tu veux venir au cinÃ©ma ce soir ?
SMS : tu vx vnir cinÃ© ce soir ?

Phrase : J'arrive dans quinze minutes
SMS :
"""

# Le LLM gÃ©nÃ¨re : "jariv ds 15min"
```

**Aucun entraÃ®nement spÃ©cifique** sur cette tÃ¢che â€” le modÃ¨le gÃ©nÃ©ralise Ã  partir des exemples.

#### C. GÃ©nÃ©ration de Code

```python
prompt = """
Ã‰cris une fonction Python qui trouve tous les nombres premiers jusqu'Ã  n en utilisant le crible d'Ã‰ratosthÃ¨ne.
"""

# GPT-4 gÃ©nÃ¨re :
def sieve_of_eratosthenes(n):
    """
    Retourne tous les nombres premiers jusqu'Ã  n.
    ComplexitÃ© : O(n log log n)
    """
    if n < 2:
        return []

    # Initialiser le tableau de boolÃ©ens
    is_prime = [True] * (n + 1)
    is_prime[0] = is_prime[1] = False

    # Crible d'Ã‰ratosthÃ¨ne
    for i in range(2, int(n**0.5) + 1):
        if is_prime[i]:
            # Marquer tous les multiples de i comme non premiers
            for j in range(i*i, n + 1, i):
                is_prime[j] = False

    # Retourner les nombres premiers
    return [i for i in range(n + 1) if is_prime[i]]

# Exemples d'utilisation
print(sieve_of_eratosthenes(30))  # [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]
print(sieve_of_eratosthenes(100))  # [2, 3, 5, ..., 97]
```

Code correct, optimisÃ©, documentÃ© â€” sans jamais avoir Ã©tÃ© explicitement entraÃ®nÃ© Ã  "implÃ©menter le crible d'Ã‰ratosthÃ¨ne".

#### D. Raisonnement Commun (Common Sense)

```python
question = "Si je mets un glaÃ§on au soleil en Ã©tÃ©, que va-t-il se passer ?"

# LLM : "Le glaÃ§on va fondre Ã  cause de la chaleur du soleil.
#        La tempÃ©rature Ã©levÃ©e va transfÃ©rer de l'Ã©nergie thermique
#        Ã  la glace, faisant passer l'eau de l'Ã©tat solide Ã  l'Ã©tat
#        liquide."
```

Le modÃ¨le n'a jamais "vu" de glaÃ§on fondre, mais il a intÃ©grÃ© la physique de base Ã  partir de textes.

---

### ğŸ­ Dialogue : Les Limites

**Alice** : Impressionnant ! Mais s'ils sont si puissants, pourquoi on a encore besoin de dÃ©veloppeurs ?

**Bob** : Excellente question ! Les LLMs ont des limites importantes :

**Bob** : 1. **Hallucinations** : ils peuvent gÃ©nÃ©rer des informations fausses avec une confiance totale.

**Alice** : Tu veux dire qu'ils "mentent" ?

**Bob** : Pas intentionnellement. Ils gÃ©nÃ¨rent le texte le plus probable selon leur entraÃ®nement, sans vÃ©rifier les faits. Si tu demandes "Quelle est la capitale de la ZÃ©lande ?" (pays imaginaire), un LLM pourrait inventer "La capitale de la ZÃ©lande est ZÃ©landville" avec aplomb.

**Bob** : 2. **Pas de mÃ©moire persistante** : chaque conversation repart de zÃ©ro (sauf si on implÃ©mente une mÃ©moire externe).

**Bob** : 3. **CoÃ»t computationnel** : faire tourner GPT-4 sur une seule requÃªte coÃ»te des centimes et nÃ©cessite des GPUs puissants.

**Bob** : 4. **Pas d'accÃ¨s au monde rÃ©el** : ils ne peuvent pas exÃ©cuter du code, naviguer sur le web, ou accÃ©der Ã  des bases de donnÃ©es (sauf si on leur donne des outils â€” ce qu'on appelle des "agents", voir Chapitre 14).

**Alice** : Donc ils sont puissants mais pas magiques.

**Bob** : Exactement. C'est pour Ã§a que ce livre existe : pour comprendre leurs capacitÃ©s **ET** leurs limites, et savoir quand et comment les utiliser efficacement.

---

## 3. L'Ã‰volution : Des ModÃ¨les de Langage Classiques aux LLMs

### 3.1 Chronologie SimplifiÃ©e

```
1948    Claude Shannon : ModÃ¨les de langage statistiques (n-grammes)
         â†“
1990s   ModÃ¨les n-grammes + lissage (Kneser-Ney, etc.)
         â†“
2003    Bengio et al. : Neural Language Models (NNLM)
         â†“
2013    Word2Vec (Mikolov) : Embeddings distribuÃ©s
         â†“
2017    ğŸŒŸ RÃ‰VOLUTION : Attention Is All You Need (Vaswani et al.)
         Naissance de l'architecture Transformer
         â†“
2018    GPT (OpenAI) : 117M paramÃ¨tres
        BERT (Google) : 340M paramÃ¨tres
         â†“
2019    GPT-2 : 1.5B paramÃ¨tres
         â†“
2020    GPT-3 : 175B paramÃ¨tres
        â†’ PremiÃ¨re dÃ©monstration de few-shot learning Ã  grande Ã©chelle
         â†“
2022    ChatGPT (GPT-3.5 + RLHF)
        â†’ Adoption massive du grand public
         â†“
2023    GPT-4 : ~1.7T paramÃ¨tres (estimation)
        Claude 2, LLaMA 2, Mistral, Gemini
         â†“
2024    Claude 3.5, GPT-4o, LLaMA 3
         â†“
2025    ModÃ¨les multimodaux, agents autonomes
         â†“
2026    ğŸš€ Vous lisez ce livre pour maÃ®triser cette technologie
```

---

### 3.2 Avant les Transformers : Les N-Grammes

Les **n-grammes** sont des modÃ¨les de langage statistiques classiques qui prÃ©disent le prochain mot basÃ© sur les `n-1` mots prÃ©cÃ©dents.

#### ImplÃ©mentation Simple

```python
from collections import defaultdict, Counter
import random

class NgramModel:
    """
    ModÃ¨le de langage n-gramme simple.

    Args:
        n (int): Taille du contexte (n-1 mots pour prÃ©dire le n-iÃ¨me)
    """
    def __init__(self, n=2):
        self.n = n
        self.ngrams = defaultdict(Counter)

    def train(self, text):
        """
        EntraÃ®ne le modÃ¨le sur un corpus de texte.

        Args:
            text (str): Corpus d'entraÃ®nement
        """
        words = text.lower().split()

        # Construire les n-grammes
        for i in range(len(words) - self.n + 1):
            # Contexte : n-1 mots prÃ©cÃ©dents
            context = tuple(words[i:i+self.n-1])
            # Mot cible : le n-iÃ¨me mot
            target = words[i+self.n-1]

            self.ngrams[context][target] += 1

    def predict_next(self, context_words, k=1):
        """
        PrÃ©dit le(s) prochain(s) mot(s) le(s) plus probable(s).

        Args:
            context_words (list): Liste des n-1 mots de contexte
            k (int): Nombre de prÃ©dictions Ã  retourner

        Returns:
            list: Top-k mots les plus probables avec leurs probabilitÃ©s
        """
        context = tuple(w.lower() for w in context_words[-(self.n-1):])

        if context not in self.ngrams:
            return [("<UNK>", 1.0)]  # Contexte inconnu

        # Calculer les probabilitÃ©s
        counter = self.ngrams[context]
        total = sum(counter.values())

        probs = {word: count/total for word, count in counter.items()}

        # Retourner les top-k
        top_k = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:k]

        return top_k

    def generate(self, start_words, max_length=20):
        """
        GÃ©nÃ¨re une sÃ©quence de mots.

        Args:
            start_words (list): Mots de dÃ©part
            max_length (int): Longueur maximale de la gÃ©nÃ©ration

        Returns:
            str: Texte gÃ©nÃ©rÃ©
        """
        generated = start_words.copy()

        for _ in range(max_length):
            context = generated[-(self.n-1):]
            predictions = self.predict_next(context)

            if predictions[0][0] == "<UNK>":
                break  # Contexte inconnu, arrÃªt

            # Ã‰chantillonnage pondÃ©rÃ© par les probabilitÃ©s
            words, probs = zip(*predictions)
            next_word = random.choices(words, weights=probs)[0]

            generated.append(next_word)

            # ArrÃªt sur ponctuation finale
            if next_word in ['.', '!', '?']:
                break

        return ' '.join(generated)

# --- Exemple d'utilisation ---

corpus = """
Le chat mange une souris. Le chien mange un os.
Le chat dort sur le canapÃ©. Le chien court dans le jardin.
Le chat noir chasse une souris grise. Le gros chien aboie.
"""

# EntraÃ®nement (bigramme : n=2)
model = NgramModel(n=2)
model.train(corpus)

# PrÃ©diction
context = ["Le"]
predictions = model.predict_next(context, k=3)
print(f"AprÃ¨s '{' '.join(context)}', mots les plus probables :")
for word, prob in predictions:
    print(f"  {word}: {prob:.2%}")

# GÃ©nÃ©ration
generated_text = model.generate(["Le", "chat"], max_length=10)
print(f"\nTexte gÃ©nÃ©rÃ© : {generated_text}")
```

**Sortie** :
```
AprÃ¨s 'Le', mots les plus probables :
  chat: 40.00%
  chien: 40.00%
  gros: 20.00%

Texte gÃ©nÃ©rÃ© : Le chat dort sur le canapÃ©.
```

#### Limites des N-Grammes

1. **Contexte limitÃ©** : Un bigramme ne regarde qu'un mot en arriÃ¨re, un trigramme deux mots, etc. Impossible de capturer des dÃ©pendances longues.

2. **Curse of dimensionality** : Le nombre de combinaisons possibles explose avec `n`. Pour un vocabulaire de 50 000 mots et n=3, on a 50 000Â³ = 125 trillions de trigrammes possibles !

3. **SparsitÃ©** : La plupart des n-grammes ne sont jamais observÃ©s dans le corpus d'entraÃ®nement.

4. **Pas de gÃ©nÃ©ralisation** : Si le modÃ¨le n'a jamais vu "Le chat bleu mange", il ne peut pas le prÃ©dire, mÃªme s'il a vu "Le chat noir mange" et "Le chien bleu dort".

**Les LLMs rÃ©solvent ces problÃ¨mes** grÃ¢ce aux rÃ©seaux de neurones et aux embeddings distribuÃ©s.

---

### 3.3 L'ArrivÃ©e des Embeddings

En **2013**, Tomas Mikolov (Google) publie **Word2Vec**, qui reprÃ©sente chaque mot comme un vecteur dense dans un espace continu.

**Avantage clÃ©** : les mots similaires ont des vecteurs similaires.

```python
# Exemple conceptuel (simplifiÃ©)
import numpy as np

# Embeddings appris (dimension 3 pour la visualisation)
embeddings = {
    "chat": np.array([0.8, 0.2, 0.1]),
    "chien": np.array([0.75, 0.25, 0.15]),
    "souris": np.array([0.6, 0.1, 0.05]),
    "automobile": np.array([0.1, 0.8, 0.7]),
    "voiture": np.array([0.12, 0.82, 0.68])
}

def cosine_similarity(v1, v2):
    """Calcule la similaritÃ© cosinus entre deux vecteurs."""
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

# SimilaritÃ© entre "chat" et "chien" : Ã©levÃ©e
print(f"chat â†” chien: {cosine_similarity(embeddings['chat'], embeddings['chien']):.3f}")

# SimilaritÃ© entre "chat" et "automobile" : faible
print(f"chat â†” automobile: {cosine_similarity(embeddings['chat'], embeddings['automobile']):.3f}")

# SimilaritÃ© entre "automobile" et "voiture" : trÃ¨s Ã©levÃ©e
print(f"automobile â†” voiture: {cosine_similarity(embeddings['automobile'], embeddings['voiture']):.3f}")
```

**Sortie** :
```
chat â†” chien: 0.995
chat â†” automobile: 0.512
automobile â†” voiture: 1.000
```

Nous explorerons les embeddings en profondeur au **Chapitre 3**.

---

## 4. Pourquoi les LLMs Fonctionnent-ils ?

### ğŸ­ Dialogue : La Magie des Gradients

**Alice** : Je comprends qu'un LLM est entraÃ®nÃ© Ã  prÃ©dire le prochain mot. Mais comment cette tÃ¢che simple lui permet d'acquÃ©rir autant de connaissances ?

**Bob** : RÃ©flÃ©chis Ã  ce qui est nÃ©cessaire pour bien prÃ©dire le prochain mot dans un texte complexe.

**Alice** : Eh bien... il faut comprendre la grammaire ?

**Bob** : Oui. Si le modÃ¨le voit "Le chat ___ une souris", il doit savoir que le verbe doit Ãªtre conjuguÃ© au prÃ©sent, troisiÃ¨me personne du singulier.

**Alice** : Et il faut connaÃ®tre le vocabulaire et les associations sÃ©mantiques.

**Bob** : Exactement. "mange", "chasse", "poursuit" sont des continuations plausibles. "Vole" ou "programme" le sont moins.

**Alice** : Il faut aussi de la logique... Si le texte dit "Il a plu toute la journÃ©e, donc le sol est ___", le modÃ¨le doit prÃ©dire "mouillÃ©" ou "humide".

**Bob** : PrÃ©cisÃ©ment ! Et maintenant imagine que tu entraÃ®nes le modÃ¨le sur **10 trillions de mots** couvrant tous les domaines humains : science, histoire, littÃ©rature, code informatique, conversations, actualitÃ©s...

**Alice** : Pour minimiser l'erreur de prÃ©diction, le modÃ¨le doit intÃ©grer toutes ces connaissances ?

**Bob** : VoilÃ  ! En optimisant une fonction de perte simple â€” `CrossEntropyLoss` entre les prÃ©dictions et les mots rÃ©els â€” le modÃ¨le est **forcÃ©** d'apprendre :
- La syntaxe et la grammaire
- Le vocabulaire et les relations sÃ©mantiques
- Des faits sur le monde
- La logique et le raisonnement de base
- Les patterns de code et d'algorithmes
- Les structures narratives

**Alice** : C'est comme si en apprenant Ã  "bien Ã©crire", il devait apprendre Ã  "bien penser" ?

**Bob** : Exactement ! C'est pourquoi on dit que les LLMs sont des "compression lossy de l'Internet" : ils capturent la structure statistique de la connaissance humaine.

---

### 4.1 L'HypothÃ¨se de Compression

**HypothÃ¨se** : Un bon modÃ¨le de langage est un bon compresseur de donnÃ©es.

Si un modÃ¨le peut **prÃ©dire parfaitement** le prochain mot, il peut encoder le texte de maniÃ¨re optimale (thÃ©orie de l'information de Shannon).

Inversement, pour bien compresser, il faut capturer tous les patterns, rÃ©gularitÃ©s, et structures du langage.

```python
# Exemple : Compression avec un modÃ¨le de langage

def compress_with_lm(text, model):
    """
    Compresse un texte en utilisant les probabilitÃ©s d'un LM.
    Plus le modÃ¨le est bon, meilleure est la compression.
    """
    tokens = tokenize(text)
    bits = 0

    for i in range(1, len(tokens)):
        context = tokens[:i]
        target = tokens[i]

        # ProbabilitÃ© prÃ©dite par le modÃ¨le
        prob = model.predict_proba(context, target)

        # Bits nÃ©cessaires pour encoder ce token (entropie)
        bits += -np.log2(prob)

    return bits / 8  # Convertir en octets

# Un meilleur modÃ¨le â†’ probabilitÃ©s plus prÃ©cises â†’ moins de bits â†’ meilleure compression
```

**ConsÃ©quence** : Les LLMs, en Ã©tant d'excellents prÃ©dicteurs, sont aussi d'excellents compresseurs. Et pour compresser efficacement la connaissance humaine, ils doivent la **comprendre** (au sens statistique).

---

### 4.2 Scaling Laws : Plus Grand = Plus Intelligent ?

**Observation empirique** (Kaplan et al., 2020) : Les performances des LLMs suivent des lois d'Ã©chelle prÃ©visibles.

```
Loss âˆ 1 / (N^Î±)

OÃ¹ :
- Loss = erreur de prÃ©diction (perplexitÃ©)
- N = nombre de paramÃ¨tres du modÃ¨le
- Î± â‰ˆ 0.076 (constante empirique)
```

**Traduction** : Doubler la taille du modÃ¨le rÃ©duit l'erreur de maniÃ¨re prÃ©visible.

**Implications** :
- GPT-3 (175B) > GPT-2 (1.5B) en performances
- GPT-4 (1.7T estimÃ©) > GPT-3
- Les capacitÃ©s Ã©mergentes apparaissent au-delÃ  de certains seuils

Nous Ã©tudierons ces lois en dÃ©tail au **Chapitre 5**.

---

## 5. Les Trois Piliers de l'EntraÃ®nement d'un LLM

### 5.1 PrÃ©-EntraÃ®nement (Pre-Training)

**Objectif** : Apprendre la structure gÃ©nÃ©rale du langage et du monde.

**MÃ©thode** : EntraÃ®nement auto-supervisÃ© sur un Ã©norme corpus de texte brut.

**TÃ¢che** : PrÃ©diction du prochain token (Causal Language Modeling).

```python
# SimplifiÃ© : boucle d'entraÃ®nement pour le prÃ©-training

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

def pretrain_llm(model, corpus, epochs=1, batch_size=32):
    """
    PrÃ©-entraÃ®ne un LLM sur un corpus de texte.

    Args:
        model (nn.Module): ModÃ¨le Transformer
        corpus (list): Liste de documents texte
        epochs (int): Nombre de passages sur le corpus
        batch_size (int): Taille des batchs
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    criterion = nn.CrossEntropyLoss()

    dataloader = DataLoader(corpus, batch_size=batch_size, shuffle=True)

    model.train()

    for epoch in range(epochs):
        total_loss = 0

        for batch in dataloader:
            # batch: [batch_size, seq_len] - tokens

            # Forward pass
            # Input : tokens[:-1]
            # Target : tokens[1:]  (dÃ©calÃ© d'une position)
            inputs = batch[:, :-1]
            targets = batch[:, 1:]

            logits = model(inputs)  # [batch_size, seq_len-1, vocab_size]

            # Calcul de la loss
            loss = criterion(
                logits.reshape(-1, logits.size(-1)),  # [batch*seq, vocab]
                targets.reshape(-1)  # [batch*seq]
            )

            # Backward pass
            optimizer.zero_grad()
            loss.backward()

            # Gradient clipping (important pour la stabilitÃ©)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        perplexity = torch.exp(torch.tensor(avg_loss))

        print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Perplexity: {perplexity:.2f}")

# Le prÃ©-entraÃ®nement peut prendre des semaines sur des clusters de milliers de GPUs !
```

**CoÃ»t** :
- GPT-3 : ~$5 millions en compute
- GPT-4 : estimÃ© > $100 millions
- Temps : plusieurs semaines Ã  plusieurs mois

Nous couvrirons le prÃ©-entraÃ®nement au **Chapitre 9**.

---

### 5.2 Fine-Tuning

**Objectif** : Adapter le modÃ¨le Ã  une tÃ¢che ou un domaine spÃ©cifique.

**MÃ©thode** : Continuer l'entraÃ®nement sur un dataset spÃ©cialisÃ© (plus petit, souvent annotÃ©).

**Exemples** :
- Fine-tuning pour le code â†’ GitHub Copilot
- Fine-tuning pour le mÃ©dical â†’ Med-PaLM
- Fine-tuning pour le juridique â†’ LexGPT

```python
def finetune_llm(pretrained_model, task_dataset, epochs=3):
    """
    Fine-tune un LLM prÃ©-entraÃ®nÃ© sur une tÃ¢che spÃ©cifique.

    Args:
        pretrained_model: ModÃ¨le dÃ©jÃ  prÃ©-entraÃ®nÃ©
        task_dataset: Dataset annotÃ© pour la tÃ¢che cible
        epochs: Nombre d'Ã©poques de fine-tuning
    """
    # On utilise un learning rate plus faible que pour le prÃ©-training
    optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-5)

    # ... (boucle d'entraÃ®nement similaire au prÃ©-training)

    # Astuce : geler les premiÃ¨res couches (optionnel)
    for param in pretrained_model.transformer.layers[:20].parameters():
        param.requires_grad = False  # Seules les derniÃ¨res couches s'adaptent
```

Nous explorerons le fine-tuning au **Chapitre 7** et les techniques d'optimisation (LoRA, QLoRA) au **Chapitre 13**.

---

### 5.3 Alignment : RLHF (Reinforcement Learning from Human Feedback)

**ProblÃ¨me** : Un LLM prÃ©-entraÃ®nÃ© peut gÃ©nÃ©rer du contenu toxique, biaisÃ©, ou inutile. Il prÃ©dit ce qui est *probable*, pas ce qui est *utile* ou *sÃ»r*.

**Solution** : L'aligner avec les prÃ©fÃ©rences humaines via RLHF.

**Processus** :

1. **Supervised Fine-Tuning (SFT)** : Fine-tuner sur des exemples de "bonnes rÃ©ponses" Ã©crites par des humains.

2. **Reward Model** : EntraÃ®ner un modÃ¨le de rÃ©compense qui prÃ©dit quelle rÃ©ponse un humain prÃ©fÃ©rerait.

3. **RL Optimization** : Utiliser PPO (Proximal Policy Optimization) pour optimiser le LLM afin de maximiser les rÃ©compenses.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pre-trained    â”‚
â”‚  LLM (GPT-3)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SFT            â”‚ â† Exemples annotÃ©s par humains
â”‚  (Fine-tuning)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reward Model   â”‚ â† Paires de rÃ©ponses classÃ©es par humains
â”‚  Training       â”‚    (A meilleure que B ?)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RLHF avec PPO  â”‚ â† Optimisation par renforcement
â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChatGPT / GPT-4â”‚ â† ModÃ¨le alignÃ© et conversationnel
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RÃ©sultat** : Le modÃ¨le devient **utile, honnÃªte, et inoffensif** (critÃ¨res d'Anthropic pour Claude).

---

## 6. Applications ConcrÃ¨tes des LLMs en 2026

### 6.1 Assistance au Code

```python
# Exemple : GitHub Copilot / ChatGPT Code Interpreter

# Vous Ã©crivez :
def calculate_fibonacci(n):
    # TODO: implement

# Le LLM complÃ¨te :
def calculate_fibonacci(n):
    """
    Calcule le n-iÃ¨me nombre de Fibonacci de maniÃ¨re efficace.
    Utilise la programmation dynamique pour Ã©viter les calculs redondants.

    Args:
        n (int): Position dans la sÃ©quence de Fibonacci (0-indexÃ©)

    Returns:
        int: Le n-iÃ¨me nombre de Fibonacci

    Examples:
        >>> calculate_fibonacci(0)
        0
        >>> calculate_fibonacci(1)
        1
        >>> calculate_fibonacci(10)
        55
    """
    if n <= 1:
        return n

    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b

    return b
```

---

### 6.2 Retrieval-Augmented Generation (RAG)

**ProblÃ¨me** : Les LLMs ne connaissent que ce qui Ã©tait dans leur corpus d'entraÃ®nement (souvent pÃ©rimÃ©).

**Solution** : Combiner un LLM avec une base de connaissances externe.

```python
from langchain import FAISS, OpenAI

# 1. Indexer des documents dans une base vectorielle
docs = [
    "Le chiffre d'affaires de l'entreprise en 2025 est de 50Mâ‚¬.",
    "Le nouveau produit sera lancÃ© en mars 2026.",
    "L'Ã©quipe R&D compte 45 ingÃ©nieurs."
]

vectorstore = FAISS.from_texts(docs, OpenAI.embeddings())

# 2. RequÃªte utilisateur
query = "Quel est le CA de l'entreprise ?"

# 3. RÃ©cupÃ©rer les documents pertinents
relevant_docs = vectorstore.similarity_search(query, k=2)

# 4. GÃ©nÃ©rer une rÃ©ponse avec le LLM + contexte
context = "\n".join([doc.page_content for doc in relevant_docs])

prompt = f"""
Contexte :
{context}

Question : {query}

RÃ©ponds uniquement basÃ© sur le contexte ci-dessus.
"""

answer = llm.generate(prompt)
print(answer)
# â†’ "Le chiffre d'affaires de l'entreprise en 2025 est de 50 millions d'euros."
```

Nous approfondirons le RAG au **Chapitre 12**.

---

### 6.3 Agents Autonomes

**Concept** : Un LLM qui peut utiliser des outils (APIs, bases de donnÃ©es, navigateur web, calculatrice).

```python
# Exemple simplifiÃ© d'agent ReAct (Reasoning + Acting)

class ReActAgent:
    def __init__(self, llm, tools):
        self.llm = llm
        self.tools = tools  # {'calculator': func, 'search': func, ...}

    def run(self, task):
        thought_action_observation = []

        for step in range(max_steps := 10):
            # 1. THOUGHT : Le LLM rÃ©flÃ©chit
            prompt = f"""
TÃ¢che : {task}

Historique :
{self._format_history(thought_action_observation)}

PensÃ©e (Thought) : Que dois-je faire maintenant ?
Action : [tool_name] argument
"""
            response = self.llm.generate(prompt)
            thought, action = self._parse_response(response)

            # 2. ACTION : ExÃ©cuter l'outil
            tool_name, arg = action
            observation = self.tools[tool_name](arg)

            thought_action_observation.append((thought, action, observation))

            # 3. Check si la tÃ¢che est terminÃ©e
            if "FINAL ANSWER" in response:
                return self._extract_answer(response)

        return "Max steps reached"

# Exemple d'utilisation
agent = ReActAgent(
    llm=GPT4(),
    tools={
        'calculator': lambda x: eval(x),
        'search': lambda x: google_search(x),
        'python': lambda x: exec_python(x)
    }
)

result = agent.run("Combien coÃ»te 1 bitcoin en euros aujourd'hui multipliÃ© par 100 ?")
# â†’ Thought: Je dois chercher le prix actuel du bitcoin
#    Action: [search] "prix bitcoin euro aujourd'hui"
#    Observation: 1 BTC = 45000â‚¬
#    Thought: Maintenant je dois multiplier par 100
#    Action: [calculator] 45000 * 100
#    Observation: 4500000
#    FINAL ANSWER: 4 500 000â‚¬
```

Nous couvrirons les agents au **Chapitre 14**.

---

### 6.4 RÃ©sumÃ© et SynthÃ¨se

```python
long_document = """
[... 50 pages de rapport financier ...]
"""

prompt = f"""
RÃ©sume le document suivant en 3 bullets points clÃ©s,
en te concentrant sur les points d'action pour le CEO.

Document :
{long_document}

RÃ©sumÃ© :
"""

summary = gpt4.generate(prompt, max_tokens=200)
```

---

### 6.5 Traduction et Localisation

```python
# Plus besoin de Google Translate : les LLMs comprennent le contexte culturel

text = "Il pleut des cordes aujourd'hui !"

prompt = f"""
Traduis en anglais amÃ©ricain en prÃ©servant le ton familier
et l'expression idiomatique :

"{text}"

Traduction :
"""

# GPT-4 : "It's raining cats and dogs today!"
# (Et non pas "It's raining ropes" littÃ©ralement)
```

---

## 7. Roadmap de ce Livre

Ce livre est structurÃ© en **4 grandes parties** :

### ğŸ—ï¸ PARTIE I : Fondations (Chapitres 1-6)
- **Chapitre 1** : Introduction aux LLMs (vous Ãªtes ici !)
- **Chapitre 2** : Histoire et Ã‰volution des LLMs
- **Chapitre 3** : Embeddings et ReprÃ©sentations Vectorielles
- **Chapitre 4** : Architectures Transformer
- **Chapitre 5** : Scaling Laws
- **Chapitre 6** : Ã‰valuation des LLMs

### ğŸ”§ PARTIE II : EntraÃ®nement et Optimisation (Chapitres 7-13)
- **Chapitre 7** : Fine-Tuning
- **Chapitre 8** : Tokenization
- **Chapitre 9** : PrÃ©-Training from Scratch
- **Chapitre 10** : Techniques d'Optimisation
- **Chapitre 11** : Prompt Engineering
- **Chapitre 12** : RAG (Retrieval-Augmented Generation)
- **Chapitre 13** : LoRA et QLoRA

### ğŸš€ PARTIE III : Applications AvancÃ©es (Chapitres 14-22)
- **Chapitre 14** : Agents LLM et ReAct
- **Chapitre 15** : DÃ©ploiement et Production
- **Chapitre 16** : SÃ©curitÃ© et Ã‰thique
- **Chapitres 17-22** : Multimodal LLMs, Chain-of-Thought avancÃ©, etc.

### ğŸ¯ PARTIE IV : Projets Pratiques (Chapitres 23-30)
- 15 projets complets avec code source
- Du chatbot simple au systÃ¨me RAG de production
- Agents autonomes, fine-tuning personnalisÃ©, etc.

---

## 8. Comment Lire ce Livre ?

### Pour les DÃ©butants

1. Lisez les chapitres dans l'ordre sÃ©quentiel
2. ExÃ©cutez tous les exemples de code
3. Faites les exercices Ã  la fin de chaque chapitre
4. Ne passez pas au chapitre suivant tant que vous n'avez pas compris le prÃ©cÃ©dent

### Pour les DÃ©veloppeurs ExpÃ©rimentÃ©s

1. Lisez rapidement les Parties I-II pour comprendre les bases
2. Concentrez-vous sur la Partie III (applications avancÃ©es)
3. ImplÃ©mentez les projets de la Partie IV
4. Utilisez le livre comme rÃ©fÃ©rence technique

### Pour les Chercheurs

1. Lisez les sections "Anecdotes Historiques" et "Ã‰tat de l'Art"
2. Concentrez-vous sur les mathÃ©matiques et les algorithmes
3. Consultez les rÃ©fÃ©rences bibliographiques (fin de chaque chapitre)
4. Explorez les papiers de recherche citÃ©s

---

## 9. PrÃ©requis Techniques

Pour tirer le maximum de ce livre, vous devriez avoir :

### CompÃ©tences Essentielles âœ…
- Python intermÃ©diaire (classes, dÃ©corateurs, async)
- Bases de ML (gradient descent, loss functions)
- AlgÃ¨bre linÃ©aire (matrices, vecteurs, produit scalaire)
- Notions de probabilitÃ©s (distribution, espÃ©rance)

### CompÃ©tences RecommandÃ©es â­
- PyTorch ou TensorFlow
- ExpÃ©rience avec des APIs (REST, webhooks)
- Notions de dÃ©ploiement (Docker, cloud)
- Git et gestion de version

### CompÃ©tences Bonus ğŸš€
- CUDA et programmation GPU
- Distributed computing
- ThÃ©orie de l'information
- Reinforcement Learning

**Si vous ne maÃ®trisez pas tous ces points** : pas de panique ! Nous expliquerons chaque concept au fur et Ã  mesure, avec des exemples et des analogies.

---

## ğŸ§  Quiz Interactif

Testez votre comprÃ©hension de ce chapitre !

### Question 1
**Quelle est la diffÃ©rence fondamentale entre un modÃ¨le n-gramme et un LLM ?**

A) Les n-grammes utilisent des rÃ©seaux de neurones, les LLMs non
B) Les LLMs peuvent capturer des dÃ©pendances Ã  longue distance grÃ¢ce aux Transformers
C) Les n-grammes sont plus prÃ©cis mais plus lents
D) Il n'y a pas de diffÃ©rence, ce sont juste des noms diffÃ©rents

<details>
<summary>ğŸ‘‰ Voir la rÃ©ponse</summary>

**RÃ©ponse : B**

Les n-grammes se basent uniquement sur les `n-1` tokens prÃ©cÃ©dents (contexte limitÃ©), tandis que les LLMs (Transformers) utilisent le mÃ©canisme d'attention pour capturer des dÃ©pendances sur toute la sÃ©quence d'entrÃ©e (jusqu'Ã  128k tokens pour GPT-4 Turbo).

Les n-grammes sont des modÃ¨les statistiques simples, tandis que les LLMs sont des rÃ©seaux de neurones profonds capables de gÃ©nÃ©ralisation et d'apprentissage de reprÃ©sentations distribuÃ©es.
</details>

---

### Question 2
**Qu'est-ce qu'une "capacitÃ© Ã©mergente" d'un LLM ?**

A) Une capacitÃ© programmÃ©e explicitement par les dÃ©veloppeurs
B) Une compÃ©tence qui apparaÃ®t seulement au-delÃ  d'une certaine Ã©chelle du modÃ¨le
C) Un bug dans le modÃ¨le
D) Une fonctionnalitÃ© ajoutÃ©e aprÃ¨s le dÃ©ploiement

<details>
<summary>ğŸ‘‰ Voir la rÃ©ponse</summary>

**RÃ©ponse : B**

Une capacitÃ© Ã©mergente est une compÃ©tence qui n'apparaÃ®t pas dans les petits modÃ¨les mais Ã©merge soudainement quand le modÃ¨le dÃ©passe un certain seuil de taille/compute.

Exemples :
- Chain-of-Thought reasoning apparaÃ®t vers 50-100B paramÃ¨tres
- Few-shot learning robuste avec GPT-3 (175B)
- CapacitÃ©s arithmÃ©tiques complexes avec GPT-4

Ces capacitÃ©s ne sont pas programmÃ©es explicitement â€” elles Ã©mergent naturellement de l'optimisation Ã  grande Ã©chelle.
</details>

---

### Question 3
**Quel est l'objectif du prÃ©-entraÃ®nement d'un LLM ?**

A) Adapter le modÃ¨le Ã  une tÃ¢che spÃ©cifique (classification, traduction, etc.)
B) Apprendre la structure gÃ©nÃ©rale du langage sur un Ã©norme corpus non annotÃ©
C) Aligner le modÃ¨le avec les prÃ©fÃ©rences humaines
D) Compresser le modÃ¨le pour rÃ©duire sa taille

<details>
<summary>ğŸ‘‰ Voir la rÃ©ponse</summary>

**RÃ©ponse : B**

Le prÃ©-entraÃ®nement (pre-training) est la phase oÃ¹ le LLM apprend Ã  modÃ©liser le langage de maniÃ¨re gÃ©nÃ©rale, en prÃ©disant le prochain token sur des trillions de mots de texte brut (web, livres, code).

Cette phase est :
- **Auto-supervisÃ©e** : pas besoin d'annotations humaines
- **CoÃ»teuse** : des millions de dollars en compute
- **Fondamentale** : elle donne au modÃ¨le sa "connaissance du monde"

AprÃ¨s le prÃ©-entraÃ®nement viennent :
- Le **fine-tuning** (adaptation Ã  des tÃ¢ches spÃ©cifiques)
- Le **RLHF** (alignement avec les prÃ©fÃ©rences humaines)
</details>

---

### Question 4
**Pourquoi utilise-t-on RLHF (Reinforcement Learning from Human Feedback) ?**

A) Pour rÃ©duire la taille du modÃ¨le
B) Pour accÃ©lÃ©rer l'infÃ©rence
C) Pour aligner le modÃ¨le avec ce que les humains trouvent utile et sÃ»r
D) Pour augmenter le nombre de paramÃ¨tres

<details>
<summary>ğŸ‘‰ Voir la rÃ©ponse</summary>

**RÃ©ponse : C**

Un LLM prÃ©-entraÃ®nÃ© prÃ©dit ce qui est **statistiquement probable**, pas nÃ©cessairement ce qui est **utile, vrai, ou sÃ»r**.

Par exemple, si on demande "Comment fabriquer une bombe ?", un modÃ¨le non-alignÃ© pourrait rÃ©pondre (car ces informations existent sur Internet), mÃªme si c'est dangereux.

**RLHF** ajuste le modÃ¨le pour :
- Refuser les requÃªtes dangereuses/illÃ©gales
- Donner des rÃ©ponses utiles et structurÃ©es
- Ã‰viter les biais et la toxicitÃ©
- Suivre des instructions prÃ©cises

C'est la diffÃ©rence entre GPT-3 (brut) et ChatGPT (alignÃ©).
</details>

---

### Question 5
**Qu'est-ce que le RAG (Retrieval-Augmented Generation) ?**

A) Une technique pour accÃ©lÃ©rer l'entraÃ®nement
B) Une mÃ©thode pour rÃ©duire les hallucinations en combinant un LLM avec une base de connaissances externe
C) Un nouveau type d'architecture Transformer
D) Un algorithme de compression

<details>
<summary>ğŸ‘‰ Voir la rÃ©ponse</summary>

**RÃ©ponse : B**

RAG = **Retrieval** (rÃ©cupÃ©ration de documents pertinents) + **Augmented Generation** (gÃ©nÃ©ration enrichie par ces documents).

**ProblÃ¨me** : Les LLMs ne connaissent que ce qui Ã©tait dans leur corpus d'entraÃ®nement (souvent pÃ©rimÃ©, incomplet).

**Solution RAG** :
1. L'utilisateur pose une question
2. On rÃ©cupÃ¨re les documents pertinents d'une base de connaissances (ex: docs d'entreprise, articles rÃ©cents)
3. On donne ces documents au LLM comme contexte
4. Le LLM gÃ©nÃ¨re une rÃ©ponse basÃ©e sur ces sources vÃ©rifiÃ©es

**Avantages** :
- RÃ©duction des hallucinations (le modÃ¨le cite ses sources)
- Connaissances Ã  jour (on peut mettre Ã  jour la base sans rÃ©entraÃ®ner le LLM)
- TraÃ§abilitÃ© (on sait d'oÃ¹ vient l'information)

C'est devenu la mÃ©thode standard pour les chatbots d'entreprise.
</details>

---

### Question 6
**Qu'est-ce qu'un "token" dans le contexte des LLMs ?**

A) Un mot complet
B) Une unitÃ© de base que le modÃ¨le traite (peut Ãªtre un mot, une sous-partie de mot, ou un caractÃ¨re)
C) Une phrase
D) Un paragraphe

<details>
<summary>ğŸ‘‰ Voir la rÃ©ponse</summary>

**RÃ©ponse : B**

Un **token** est l'unitÃ© atomique traitÃ©e par un LLM. C'est souvent une **sous-partie de mot** (subword).

**Exemples avec GPT-4** :
- "chat" â†’ 1 token
- "chats" â†’ 1 token
- "ChatGPT" â†’ 2 tokens : ["Chat", "GPT"]
- "anticonstitutionnellement" â†’ 6 tokens : ["anti", "constitu", "tion", "nell", "ement"]

**Pourquoi pas des mots complets ?**
- Vocabulaire trop grand (des millions de mots possibles)
- Ne gÃ¨re pas les mots rares ou les fautes d'orthographe
- Inefficace pour le code ou les langues non-anglaises

**Algorithmes de tokenization** : BPE, WordPiece, SentencePiece (voir Chapitre 8).

**Important** : GPT-4 a une limite de 128k tokens (â‰ˆ 100k mots), pas 128k mots !
</details>

---

## ğŸ’» Exercices Pratiques

### Exercice 1 : ImplÃ©menter un GÃ©nÃ©rateur de Texte Simple

**Objectif** : CrÃ©er un gÃ©nÃ©rateur de texte basÃ© sur des bigrammes (n=2).

**Consignes** :
1. RÃ©cupÃ©rez un corpus de texte (par exemple, un livre du domaine public sur Project Gutenberg)
2. ImplÃ©mentez une classe `BigramGenerator` qui entraÃ®ne un modÃ¨le bigramme
3. GÃ©nÃ©rez 5 phrases diffÃ©rentes en partant du mot "Le"
4. Calculez la perplexitÃ© du modÃ¨le sur un Ã©chantillon de test

<details>
<summary>ğŸ‘‰ Voir la solution</summary>

```python
import requests
import re
import random
import math
from collections import defaultdict, Counter

class BigramGenerator:
    """GÃ©nÃ©rateur de texte basÃ© sur des bigrammes."""

    def __init__(self):
        self.bigrams = defaultdict(Counter)
        self.vocab = set()

    def preprocess(self, text):
        """Nettoie et tokenize le texte."""
        # Minuscules
        text = text.lower()
        # Remplacer les sauts de ligne par des espaces
        text = re.sub(r'\s+', ' ', text)
        # Tokenize (simple : split sur espaces et ponctuation)
        tokens = re.findall(r'\b\w+\b|[.,!?;]', text)
        return tokens

    def train(self, corpus):
        """
        EntraÃ®ne le modÃ¨le sur un corpus.

        Args:
            corpus (str): Texte d'entraÃ®nement
        """
        tokens = self.preprocess(corpus)
        self.vocab = set(tokens)

        # Construire les bigrammes
        for i in range(len(tokens) - 1):
            current = tokens[i]
            next_token = tokens[i + 1]
            self.bigrams[current][next_token] += 1

        print(f"âœ… EntraÃ®nement terminÃ©")
        print(f"   Vocabulaire : {len(self.vocab)} tokens uniques")
        print(f"   Bigrammes : {len(self.bigrams)} contextes")

    def generate(self, start_token="le", max_length=20, temperature=1.0):
        """
        GÃ©nÃ¨re une sÃ©quence de tokens.

        Args:
            start_token (str): Token de dÃ©part
            max_length (int): Longueur maximale
            temperature (float): ContrÃ´le l'alÃ©atoire (0=dÃ©terministe, >1=crÃ©atif)

        Returns:
            str: Texte gÃ©nÃ©rÃ©
        """
        current = start_token.lower()
        generated = [current]

        for _ in range(max_length):
            if current not in self.bigrams:
                break  # Contexte inconnu

            # RÃ©cupÃ©rer les candidats possibles
            candidates = self.bigrams[current]

            if not candidates:
                break

            # Ã‰chantillonnage avec tempÃ©rature
            tokens = list(candidates.keys())
            counts = [candidates[t] for t in tokens]

            # Appliquer la tempÃ©rature
            if temperature != 1.0:
                counts = [c ** (1.0 / temperature) for c in counts]

            # Normaliser en probabilitÃ©s
            total = sum(counts)
            probs = [c / total for c in counts]

            # Ã‰chantillonner
            next_token = random.choices(tokens, weights=probs)[0]
            generated.append(next_token)

            # ArrÃªt sur ponctuation finale
            if next_token in ['.', '!', '?']:
                break

            current = next_token

        # Reconstruire le texte avec ponctuation correcte
        text = ""
        for token in generated:
            if token in ".,!?;":
                text = text.rstrip() + token + " "
            else:
                text += token + " "

        return text.strip()

    def perplexity(self, test_corpus):
        """
        Calcule la perplexitÃ© sur un corpus de test.

        Perplexity = exp(-1/N * sum(log P(w_i | w_{i-1})))

        Args:
            test_corpus (str): Texte de test

        Returns:
            float: PerplexitÃ©
        """
        tokens = self.preprocess(test_corpus)

        log_prob_sum = 0
        count = 0

        for i in range(len(tokens) - 1):
            current = tokens[i]
            next_token = tokens[i + 1]

            if current in self.bigrams:
                candidates = self.bigrams[current]
                total = sum(candidates.values())

                if next_token in candidates:
                    prob = candidates[next_token] / total
                else:
                    prob = 1e-10  # Lissage minimal pour les tokens inconnus

                log_prob_sum += math.log(prob)
                count += 1

        if count == 0:
            return float('inf')

        avg_log_prob = log_prob_sum / count
        perplexity = math.exp(-avg_log_prob)

        return perplexity


# --- Utilisation ---

# 1. TÃ©lÃ©charger un corpus (ex: Les MisÃ©rables de Victor Hugo)
url = "https://www.gutenberg.org/files/135/135-0.txt"
response = requests.get(url)
corpus = response.text

# On prend seulement une partie pour l'exemple
corpus = corpus[:100000]  # 100k premiers caractÃ¨res

# 2. EntraÃ®ner le modÃ¨le
model = BigramGenerator()
model.train(corpus)

# 3. GÃ©nÃ©rer 5 phrases
print("\nğŸ“ GÃ©nÃ©ration de phrases :\n")
for i in range(5):
    sentence = model.generate(start_token="le", max_length=15, temperature=0.8)
    print(f"{i+1}. {sentence}")

# 4. Calculer la perplexitÃ© sur un Ã©chantillon de test
test_sample = corpus[100000:110000]
ppl = model.perplexity(test_sample)
print(f"\nğŸ“Š PerplexitÃ© sur l'Ã©chantillon de test : {ppl:.2f}")
print("   (Plus c'est bas, mieux c'est)")
```

**Sortie attendue** :
```
âœ… EntraÃ®nement terminÃ©
   Vocabulaire : 8532 tokens uniques
   Bigrammes : 7891 contextes

ğŸ“ GÃ©nÃ©ration de phrases :

1. le pÃ¨re de la rue de la rue .
2. le lendemain matin , il Ã©tait Ã  la porte .
3. le soir , il se fit un silence .
4. le jour oÃ¹ il avait vu jean valjean .
5. le premier , c est que la misÃ¨re .

ğŸ“Š PerplexitÃ© sur l'Ã©chantillon de test : 487.32
   (Plus c'est bas, mieux c'est)
```

**Observations** :
- Les phrases sont grammaticalement correctes mais rÃ©pÃ©titives
- Beaucoup de "de la", "de la rue" (biais du corpus)
- PerplexitÃ© Ã©levÃ©e (normal pour un modÃ¨le si simple)
- **Les LLMs modernes ont une perplexitÃ© < 10** sur la plupart des textes !

</details>

---

### Exercice 2 : Calculer des SimilaritÃ©s d'Embeddings

**Objectif** : Comprendre comment les embeddings capturent les relations sÃ©mantiques.

**Consignes** :
1. Utilisez l'API OpenAI pour obtenir les embeddings de plusieurs mots
2. Calculez les similaritÃ©s cosinus entre paires de mots
3. Visualisez les relations sÃ©mantiques

<details>
<summary>ğŸ‘‰ Voir la solution</summary>

```python
import numpy as np
import openai
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
openai.api_key = "your-api-key"  # Remplacez par votre clÃ©

def get_embedding(text, model="text-embedding-3-small"):
    """RÃ©cupÃ¨re l'embedding d'un texte via l'API OpenAI."""
    response = openai.embeddings.create(
        input=text,
        model=model
    )
    return np.array(response.data[0].embedding)

def compute_similarity_matrix(words):
    """
    Calcule la matrice de similaritÃ© entre une liste de mots.

    Args:
        words (list): Liste de mots

    Returns:
        np.ndarray: Matrice de similaritÃ© (NxN)
    """
    print("ğŸ”„ RÃ©cupÃ©ration des embeddings...")
    embeddings = [get_embedding(word) for word in words]
    embeddings_matrix = np.array(embeddings)

    print("ğŸ”„ Calcul des similaritÃ©s...")
    similarity_matrix = cosine_similarity(embeddings_matrix)

    return similarity_matrix

def visualize_similarity(words, similarity_matrix):
    """Visualise la matrice de similaritÃ©."""
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        similarity_matrix,
        annot=True,
        fmt=".2f",
        xticklabels=words,
        yticklabels=words,
        cmap="YlOrRd",
        vmin=0,
        vmax=1
    )
    plt.title("Matrice de SimilaritÃ© Cosinus des Embeddings")
    plt.tight_layout()
    plt.savefig("similarity_matrix.png", dpi=150)
    print("âœ… Graphique sauvegardÃ© : similarity_matrix.png")


# --- ExpÃ©rience 1 : Animaux vs VÃ©hicules ---

words_1 = ["chat", "chien", "souris", "automobile", "voiture", "train"]

sim_matrix_1 = compute_similarity_matrix(words_1)
visualize_similarity(words_1, sim_matrix_1)

print("\nğŸ“Š Observations :")
print(f"   SimilaritÃ© chat-chien : {sim_matrix_1[0,1]:.3f} (Ã©levÃ©e)")
print(f"   SimilaritÃ© chat-voiture : {sim_matrix_1[0,4]:.3f} (faible)")
print(f"   SimilaritÃ© automobile-voiture : {sim_matrix_1[3,4]:.3f} (trÃ¨s Ã©levÃ©e)")


# --- ExpÃ©rience 2 : Analogies (Roi - Homme + Femme â‰ˆ Reine) ---

def find_analogy(word_a, word_b, word_c, candidates):
    """
    RÃ©sout l'analogie : word_a est Ã  word_b ce que word_c est Ã  ?

    Exemple : roi - homme + femme â‰ˆ reine

    Args:
        word_a, word_b, word_c (str): Mots de l'analogie
        candidates (list): Liste de mots candidats pour la rÃ©ponse

    Returns:
        str: Mot le plus proche
    """
    emb_a = get_embedding(word_a)
    emb_b = get_embedding(word_b)
    emb_c = get_embedding(word_c)

    # Vecteur cible : c + (a - b)
    target_vector = emb_c + (emb_a - emb_b)

    # Trouver le candidat le plus proche
    best_word = None
    best_sim = -1

    for candidate in candidates:
        emb_candidate = get_embedding(candidate)
        sim = cosine_similarity([target_vector], [emb_candidate])[0][0]

        if sim > best_sim:
            best_sim = sim
            best_word = candidate

    return best_word, best_sim

# Test de l'analogie classique
print("\nğŸ§ª Test d'analogie : roi - homme + femme = ?")
result, score = find_analogy(
    "roi", "homme", "femme",
    candidates=["reine", "princesse", "impÃ©ratrice", "duchesse", "femme"]
)
print(f"   RÃ©ponse : {result} (similaritÃ© : {score:.3f})")

# Autre exemple : Paris - France + Italie = ?
print("\nğŸ§ª Test d'analogie : Paris - France + Italie = ?")
result, score = find_analogy(
    "Paris", "France", "Italie",
    candidates=["Rome", "Milan", "Venise", "Florence", "Naples"]
)
print(f"   RÃ©ponse : {result} (similaritÃ© : {score:.3f})")
```

**Sortie attendue** :
```
ğŸ”„ RÃ©cupÃ©ration des embeddings...
ğŸ”„ Calcul des similaritÃ©s...
âœ… Graphique sauvegardÃ© : similarity_matrix.png

ğŸ“Š Observations :
   SimilaritÃ© chat-chien : 0.847 (Ã©levÃ©e)
   SimilaritÃ© chat-voiture : 0.312 (faible)
   SimilaritÃ© automobile-voiture : 0.961 (trÃ¨s Ã©levÃ©e)

ğŸ§ª Test d'analogie : roi - homme + femme = ?
   RÃ©ponse : reine (similaritÃ© : 0.923)

ğŸ§ª Test d'analogie : Paris - France + Italie = ?
   RÃ©ponse : Rome (similaritÃ© : 0.889)
```

**Insights** :
- Les embeddings capturent des relations sÃ©mantiques complexes
- Les analogies fonctionnent via l'arithmÃ©tique vectorielle
- C'est la base de la comprÃ©hension des LLMs !

</details>

---

### Exercice 3 : ExpÃ©rimenter avec le Prompt Engineering

**Objectif** : Comprendre comment la formulation d'un prompt influence la sortie d'un LLM.

**Consignes** :
1. Choisissez une tÃ¢che (ex: rÃ©sumer un article, Ã©crire du code, traduire)
2. Testez 3 formulations diffÃ©rentes du prompt
3. Comparez les rÃ©sultats et identifiez les patterns qui fonctionnent

<details>
<summary>ğŸ‘‰ Voir la solution</summary>

```python
import openai

openai.api_key = "your-api-key"

def test_prompt(prompt, model="gpt-4"):
    """Envoie un prompt et rÃ©cupÃ¨re la rÃ©ponse."""
    response = openai.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=300
    )
    return response.choices[0].message.content

# TÃ¢che : Expliquer la rÃ©cursivitÃ© Ã  un enfant de 10 ans

print("=" * 80)
print("TÃ‚CHE : Expliquer la rÃ©cursivitÃ© Ã  un enfant de 10 ans")
print("=" * 80)

# --- PROMPT 1 : Simple et direct ---
print("\nğŸ“ PROMPT 1 (Simple) :")
prompt_1 = "Explique la rÃ©cursivitÃ© en programmation."

print(f"Prompt : {prompt_1}\n")
response_1 = test_prompt(prompt_1)
print(f"RÃ©ponse :\n{response_1}")

# --- PROMPT 2 : Avec contexte et contraintes ---
print("\n" + "="*80)
print("\nğŸ“ PROMPT 2 (Avec contexte) :")
prompt_2 = """
Tu es un professeur d'informatique bienveillant.
Explique le concept de rÃ©cursivitÃ© en programmation Ã  un enfant de 10 ans.
Utilise des analogies simples et Ã©vite le jargon technique.
"""

print(f"Prompt : {prompt_2}\n")
response_2 = test_prompt(prompt_2)
print(f"RÃ©ponse :\n{response_2}")

# --- PROMPT 3 : Avec format structurÃ© et exemples ---
print("\n" + "="*80)
print("\nğŸ“ PROMPT 3 (Format structurÃ©) :")
prompt_3 = """
Explique la rÃ©cursivitÃ© en programmation Ã  un enfant de 10 ans.

Utilise le format suivant :

1. **Analogie du quotidien** : Compare la rÃ©cursivitÃ© Ã  quelque chose que l'enfant connaÃ®t
2. **DÃ©finition simple** : Explique le concept en une phrase
3. **Exemple de code Python** : Montre un exemple trÃ¨s simple (5 lignes max)
4. **RÃ©sumÃ©** : RÃ©capitule en une phrase ce qu'il faut retenir

Reste simple et ludique !
"""

print(f"Prompt : {prompt_3}\n")
response_3 = test_prompt(prompt_3)
print(f"RÃ©ponse :\n{response_3}")

# --- ANALYSE ---
print("\n" + "="*80)
print("\nğŸ“Š ANALYSE DES RÃ‰SULTATS :")
print("="*80)

print("""
Prompt 1 (Simple) :
  âœ… Rapide Ã  Ã©crire
  âŒ RÃ©ponse souvent trop technique
  âŒ Pas adaptÃ© Ã  l'audience cible

Prompt 2 (Avec contexte) :
  âœ… Meilleure adaptation au niveau de l'audience
  âœ… Ton plus appropriÃ©
  âš ï¸  Structure variable

Prompt 3 (Format structurÃ©) :
  âœ… RÃ©ponse structurÃ©e et prÃ©visible
  âœ… Couvre tous les aspects demandÃ©s
  âœ… Facile Ã  parser programmatiquement
  âš ï¸  Plus long Ã  Ã©crire

ğŸ¯ MEILLEURE PRATIQUE : Prompt 3
   â†’ SpÃ©cifier le rÃ´le, l'audience, le format, et des contraintes claires
""")

# --- BONUS : Template de prompt rÃ©utilisable ---
print("\n" + "="*80)
print("\nğŸ¨ TEMPLATE DE PROMPT GÃ‰NÃ‰RIQUE :")
print("="*80)

PROMPT_TEMPLATE = """
[RÃ”LE]
Tu es {role}.

[AUDIENCE]
Ton audience est {audience}.

[TÃ‚CHE]
{task}

[FORMAT]
RÃ©ponds au format suivant :
{format_instructions}

[CONTRAINTES]
- {constraint_1}
- {constraint_2}
- {constraint_3}

[TON]
{tone}
"""

# Exemple d'utilisation du template
exemple_prompt = PROMPT_TEMPLATE.format(
    role="un expert en machine learning pÃ©dagogue",
    audience="des dÃ©veloppeurs juniors qui dÃ©couvrent le ML",
    task="Explique ce qu'est un gradient descent",
    format_instructions="""
1. Analogie visuelle (escalier, montagne, etc.)
2. Formule mathÃ©matique avec explication de chaque terme
3. ImplÃ©mentation Python (10 lignes max)
4. PiÃ¨ges courants Ã  Ã©viter
""",
    constraint_1="Utilise des analogies concrÃ¨tes",
    constraint_2="Ã‰vite les Ã©quations complexes",
    constraint_3="Fournis du code exÃ©cutable",
    tone="PÃ©dagogique et encourageant"
)

print(exemple_prompt)

print("\nâœ… Ce template est rÃ©utilisable pour toute tÃ¢che de prompt engineering !")
```

**Insights clÃ©s** :
1. **Plus le prompt est spÃ©cifique, meilleure est la sortie**
2. **SpÃ©cifier le format attendu garantit une structure cohÃ©rente**
3. **Donner un rÃ´le au modÃ¨le amÃ©liore l'adaptation au contexte**
4. **Les contraintes explicites Ã©vitent les dÃ©rives**

Nous approfondirons le prompt engineering au **Chapitre 11**.

</details>

---

## ğŸ“š RÃ©sumÃ© du Chapitre

### Points ClÃ©s Ã  Retenir

1. **Les LLMs sont des modÃ¨les de langage neuronaux Ã  grande Ã©chelle** (milliards de paramÃ¨tres) entraÃ®nÃ©s Ã  prÃ©dire le prochain token.

2. **CapacitÃ©s Ã©mergentes** : des compÃ©tences complexes (raisonnement, gÃ©nÃ©ration de code) apparaissent au-delÃ  d'une certaine Ã©chelle.

3. **Trois phases d'entraÃ®nement** :
   - **PrÃ©-training** : apprentissage gÃ©nÃ©ral sur des trillions de tokens
   - **Fine-tuning** : adaptation Ã  des tÃ¢ches spÃ©cifiques
   - **RLHF** : alignement avec les prÃ©fÃ©rences humaines

4. **Limites** : hallucinations, coÃ»t computationnel, pas de mÃ©moire persistante, pas d'accÃ¨s direct au monde rÃ©el.

5. **Applications** : assistance au code, RAG, agents autonomes, rÃ©sumÃ©, traduction, et bien plus.

6. **Ã‰volution** : des n-grammes (1990s) aux Transformers (2017) aux LLMs modernes (GPT-4, Claude 3, 2023-2026).

---

## ğŸš€ Prochaine Ã‰tape

Dans le **Chapitre 2 : Histoire et Ã‰volution des LLMs**, nous plongerons dans :
- La chronologie dÃ©taillÃ©e : de ELIZA (1966) Ã  GPT-4 (2023)
- Les personnages clÃ©s : Turing, Shannon, Hinton, Bengio, Vaswani, Sutskever
- Les moments charniÃ¨res : Word2Vec, LSTM, Attention, BERT, GPT-3
- Les controverses : biais, Ã©thique, propriÃ©tÃ© intellectuelle
- Les perspectives : AGI, multimodalitÃ©, agents autonomes

**Ã€ trÃ¨s bientÃ´t dans le prochain chapitre !** ğŸ‰

---

## ğŸ“– RÃ©fÃ©rences et Lectures RecommandÃ©es

### Papers Fondamentaux
1. Shannon, C.E. (1948). *A Mathematical Theory of Communication*
2. Vaswani et al. (2017). *Attention Is All You Need*
3. Brown et al. (2020). *Language Models are Few-Shot Learners* (GPT-3)
4. Ouyang et al. (2022). *Training language models to follow instructions with human feedback* (RLHF)
5. Wei et al. (2022). *Emergent Abilities of Large Language Models*

### Livres
- Jurafsky & Martin. *Speech and Language Processing* (3rd ed.)
- Goodfellow, Bengio & Courville. *Deep Learning*
- Tunstall et al. *Natural Language Processing with Transformers*

### Ressources en Ligne
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) â€” Jay Alammar
- [Hugging Face Course](https://huggingface.co/course) â€” Gratuit et pratique
- [OpenAI Cookbook](https://github.com/openai/openai-cookbook) â€” Exemples de code

---

*Fin du Chapitre 1*

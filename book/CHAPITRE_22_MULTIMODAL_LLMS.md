# CHAPITRE 22 : MULTIMODAL LLMs - QUAND LES MOTS RENCONTRENT LES IMAGES

> *"Une image vaut mille mots. Un modÃ¨le multimodal vaut mille modÃ¨les."*
> â€” Proverbe adaptÃ© de l'Ã¨re AI ğŸ¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ VOUS ÃŠTES ICI DANS LE LIVRE                             â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                              â”‚
â”‚  Fondations âœ… â†’ Training âœ… â†’ Fine-tuning âœ… â†’ Inference âœ… â”‚
â”‚  â†’ Techniques AvancÃ©es â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 60%       â”‚
â”‚                        â†‘ VOUS ÃŠTES ICI                      â”‚
â”‚                                                              â”‚
â”‚  PrÃ©requis : âœ… Chapitre 3 (Transformers), 13 (LoRA)        â”‚
â”‚  DifficultÃ© : â­â­â­â­âšª (AvancÃ©)                            â”‚
â”‚  Temps estimÃ© : â±ï¸ 4-5 heures                               â”‚
â”‚  Ce que vous allez crÃ©er : ğŸ¯ Chatbot vision comme GPT-4V!  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Table des MatiÃ¨res

1. [Introduction : La RÃ©volution Multimodale](#1-introduction)
2. [L'Histoire Fascinante des ModÃ¨les Vision-Language](#2-histoire)
3. [Fondamentaux : Comment Fusionner Vision et Langage](#3-fondamentaux)
4. [GPT-4V : Le King de la MultimodalitÃ©](#4-gpt4v)
5. [LLaVA : Vision Open-Source](#5-llava)
6. [BLIP-2 et Flamingo : Architectures Alternatives](#6-blip2-flamingo)
7. [Au-delÃ  de la Vision : Audio et VidÃ©o](#7-audio-video)
8. [Training Paradigms](#8-training)
9. [Projet Pratique : CrÃ©er Votre Chatbot Vision](#9-projet)
10. [Best Practices et Troubleshooting](#10-best-practices)
11. [Quiz et Exercices](#11-quiz)

---

## 1. Introduction : La RÃ©volution Multimodale

### 1.1 Pourquoi la MultimodalitÃ© Change Tout

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¬ **DIALOGUE : Alice dÃ©couvre la multimodalitÃ©**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Alice** (enthousiaste) : "Bob, j'ai uploadÃ© une photo de mon chat Ã  GPT-4, et il m'a dÃ©crit sa race, son humeur, ET il a mÃªme fait une blague sur son regard blasÃ© ! Comment c'est possible ?!"

**Bob** (sourire) : "Bienvenue dans l'Ã¨re multimodale, Alice ! Ton chat vient de passer le test de Turing visuel. ğŸ˜º"

**Alice** : "Mais attends... Un LLM comprend du texte, pas des images ?"

**Bob** : "Exactement le problÃ¨me qu'on avait ! Imagine : tu as un ami brillant (le LLM) qui ne voit rien. Il peut parler de philosophie pendant des heures, mais montre-lui une photo de coucher de soleil... silence radio. Frustrant, non ?"

**Alice** : "TrÃ¨s ! Alors comment on lui a donnÃ© des yeux ?"

**Bob** : "On ne lui a pas donnÃ© des yeux. On lui a donnÃ© un traducteur ! Un modÃ¨le qui prend l'image et dit au LLM : 'Ã‰coute, ce que tu vois lÃ , en mots, c'est...' Et le LLM rÃ©pond : 'Ah ! Je connais ces mots ! Je peux en parler !'"

**Alice** : "C'est comme un interprÃ¨te entre deux langues ?"

**Bob** : "Exactement ! Vision â†’ Langue commune (embeddings) â†’ LLM. Le gÃ©nie, c'est que cette 'langue commune' est mathÃ©matique : des vecteurs que les deux modÃ¨les comprennent."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

### 1.2 L'Intuition Visuelle

Imaginez que vous Ãªtes dans un restaurant franÃ§ais, mais vous ne parlez que japonais. Ã€ cÃ´tÃ© de vous, il y a quelqu'un qui parle franÃ§ais couramment. Entre vous deux, il y a un interprÃ¨te qui traduit.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LE SYSTÃˆME MULTIMODAL                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  VOUS (Vision)  â†’  INTERPRÃˆTE  â†’  AMI FRANÃ‡AIS (LLM)    â”‚
â”‚     ğŸ‘ï¸              ğŸŒ‰              ğŸ’¬                    â”‚
â”‚   Image          Projection       Texte                  â”‚
â”‚                                                           â”‚
â”‚  "Je vois un     "Ã‡a signifie     "Ah, un chat roux     â”‚
â”‚   chat roux"      embeddings       avec des yeux verts!  â”‚
â”‚                   [0.2, 0.8...]"   Je peux te parler     â”‚
â”‚                                     de sa race..."        â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Le **module de vision** (comme CLIP) = Vous qui voyez
Le **projecteur** (cross-attention) = L'interprÃ¨te
Le **LLM** (comme Llama) = L'ami qui parle

**RÃ©sultat** : Conversation fluide entre vision et langage ! ğŸ‰

### 1.3 Applications ConcrÃ¨tes (qui changent la vie)

**Cas d'usage rÃ©els** :

ğŸ“¸ **Assistance Visuelle pour Malvoyants**
- GPT-4V dÃ©crit scÃ¨nes en temps rÃ©el
- Lecture de textes dans environnement
- Navigation assistÃ©e

ğŸ¥ **Diagnostic MÃ©dical**
- Analyse radiographies + rapports textuels
- DÃ©tection anomalies avec explications
- Assistant radiologue (FDA approved!)

ğŸ›’ **E-commerce Intelligent**
- "Trouve-moi un canapÃ© comme celui-lÃ  mais moins cher"
- Recherche visuelle + conversationnelle
- Amazon, Alibaba utilisent dÃ©jÃ 

ğŸ¨ **CrÃ©ation de Contenu**
- Midjourney, DALL-E : Texte â†’ Image
- GPT-4V : Image â†’ Description â†’ AmÃ©lioration
- Boucle crÃ©ative infinie

ğŸš— **Voitures Autonomes**
- Vision (camÃ©ras) + Langage (instructions)
- "Tourne Ã  gauche aprÃ¨s le feu rouge"
- Tesla FSD v12 = vision + LLM

---

## 2. L'Histoire Fascinante des ModÃ¨les Vision-Language

### 2.1 Timeline : De l'ImpossibilitÃ© au Quotidien

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“œ TIMELINE MULTIMODAL (2012-2024)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

2012 ğŸ¯ AlexNet
     â”‚  ImageNet revolution - Deep learning pour vision
     â”‚  Mais : vision ET langage ? Impossible.
     â”‚
2014 ğŸ–¼ï¸ Image Captioning
     â”‚  Premiers modÃ¨les Vision â†’ Texte
     â”‚  "Show and Tell" (Google) : CNN + RNN
     â”‚  QualitÃ© : "a dog is standing" (trÃ¨s basique)
     â”‚
2017 ğŸ’¥ Transformers + Attention
     â”‚  "Attention is All You Need"
     â”‚  Game changer : Tout devient embeddings
     â”‚  Vision comme langage : possible !
     â”‚
2019 ğŸ¨ CLIP (OpenAI)
     â”‚  RÃ©volution : Vision et Texte dans MÃŠME espace
     â”‚  400M paires image-texte d'Internet
     â”‚  Zero-shot classification : magie !
     â”‚
2021 ğŸ¦© Flamingo (DeepMind)
     â”‚  Premier "vrai" LLM multimodal
     â”‚  Few-shot vision capabilities
     â”‚  Mais : model fermÃ©, pas disponible
     â”‚
2022 ğŸ”¥ BLIP-2 (Salesforce)
     â”‚  Q-Former : module intelligent de projection
     â”‚  Open-source, efficient
     â”‚  Adoption massive communautÃ©
     â”‚
2023 ğŸš€ GPT-4V (OpenAI) - Mars 2023
     â”‚  LE moment qui change tout
     â”‚  "gpt-4-vision-preview" lancÃ©
     â”‚  QualitÃ© : indistinguable d'humain expert
     â”‚  Demos virales : memes, problÃ¨mes math manuscrits
     â”‚
2023 ğŸ¦™ LLaVA (Open-source) - Octobre 2023
     â”‚  RÃ©ponse communautÃ© Ã  GPT-4V
     â”‚  Llama-2 + CLIP + Projection
     â”‚  Performance proche GPT-4V (!)
     â”‚  CoÃ»t training : $500 seulement ğŸ’°
     â”‚
2024 ğŸŒŸ Gemini 1.5 Pro (Google)
     â”‚  Multimodal natif dÃ¨s le pre-training
     â”‚  VidÃ©o understanding (1 heure analysÃ©e)
     â”‚  Long-context : 1M tokens vision+texte
     â”‚
2024 ğŸ“ˆ Claude 3.5 Sonnet (Anthropic)
     â”‚  Meilleure vision que GPT-4V (benchmarks)
     â”‚  OCR quasi-parfait, diagrammes complexes
     â”‚  Artifacts : gÃ©nÃ¨re code depuis screenshots
     â”‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### 2.2 Les Pionniers : Visages derriÃ¨re la RÃ©volution

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒŸ LES HÃ‰ROS DE LA MULTIMODALITÃ‰                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  ğŸ‘¨â€ğŸ’» Alec Radford (OpenAI)                                â”‚
â”‚     CrÃ©ateur de CLIP (2019)                              â”‚
â”‚     Vision : "Vision et langage doivent partager         â”‚
â”‚               l'espace sÃ©mantique"                        â”‚
â”‚     Impact : Foundation de tous modÃ¨les modernes         â”‚
â”‚                                                           â”‚
â”‚  ğŸ‘¨â€ğŸ”¬ Jean-Baptiste Alayrac (DeepMind)                    â”‚
â”‚     Lead de Flamingo (2021)                              â”‚
â”‚     Innovation : Few-shot multimodal learning            â”‚
â”‚     Citation : "Le futur de l'AI est multimodal par      â”‚
â”‚                 dÃ©faut, pas monodale par choix"          â”‚
â”‚                                                           â”‚
â”‚  ğŸ‘©â€ğŸ’» Junnan Li (Salesforce)                               â”‚
â”‚     CrÃ©atrice de BLIP-2 (2022)                           â”‚
â”‚     GÃ©nie : Q-Former architecture                        â”‚
â”‚     Open-source hero : 15k+ stars GitHub                 â”‚
â”‚                                                           â”‚
â”‚  ğŸ‘¨â€ğŸ’¼ Sam Altman (OpenAI)                                  â”‚
â”‚     Vision GPT-4V (2023)                                 â”‚
â”‚     Demo mÃ©morable : Reconnaissance d'objets rares       â”‚
â”‚     Tweet viral : "This changes everything"              â”‚
â”‚                                                           â”‚
â”‚  ğŸ¦™ Haotian Liu (University of Wisconsin)                â”‚
â”‚     CrÃ©ateur de LLaVA (2023)                             â”‚
â”‚     Age : 24 ans (!)                                     â”‚
â”‚     Impact : DÃ©mocratisation multimodal (open-source)    â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### 2.3 Le Moment "ChatGPT" de la Vision

ğŸ“£ **Anecdote Historique : La Demo qui a Tout ChangÃ©**

*14 Mars 2023, 10h AM (PST) - SiÃ¨ge d'OpenAI, San Francisco*

Sam Altman upload une photo d'un frigo ouvert sur Twitter avec le prompt : "What can I make with these ingredients?"

GPT-4V rÃ©pond en 3 secondes avec :
- Liste complÃ¨te des ingrÃ©dients (identifiÃ©s visuellement)
- 5 recettes possibles classÃ©es par difficultÃ©
- Conseils nutritionnels
- **Bonus** : "Le lait dans la porte va expirer demain, utilise-le en prioritÃ©!"

**RÃ©sultat** : 10M de vues en 24h. Le monde comprend : la vision AI est arrivÃ©e.

Les 48h suivantes :
- ğŸ“ˆ Actions OpenAI explosent
- ğŸƒ Google panic mode : accÃ©lÃ¨re Gemini
- ğŸ¦™ CommunautÃ© open-source : "On peut faire pareil!"
- ğŸ“š 100+ papers soumis sur "GPT-4V applications"

**Citation de Yann LeCun** (Meta AI Chief) :
> "This is not AGI. But it's the closest we've been to making machines understand the world like humans do. Vision + Language = ğŸ”¥"

---

## 3. Fondamentaux : Comment Fusionner Vision et Langage

### 3.1 Le ProblÃ¨me Fondamental

ğŸ’¡ **Intuition** : Vous avez deux amis brillants qui ne parlent pas la mÃªme langue.

- **Ami 1 (Vision)** : Pense en pixels (0-255), matrices 3D, couleurs RGB
- **Ami 2 (Langage)** : Pense en tokens, embeddings 4096D, probabilitÃ©s

Comment les faire communiquer ?

**NaÃ¯ve Approach (ne marche PAS)** :
```python
# âŒ FAUX - ConcatÃ©nation directe
image_pixels = [255, 0, 127, ...]  # 224Ã—224Ã—3 = 150k valeurs
text_tokens = [42, 1337, 89, ...]   # SÃ©quence de tokens

# Mettre ensemble ? LOL non
combined = image_pixels + text_tokens  # ğŸ”¥ Ã‡a marche pas
llm.forward(combined)  # ğŸ’€ LLM ne comprend rien aux pixels
```

**Pourquoi Ã§a Ã©choue** :
1. **Ã‰chelles diffÃ©rentes** : Pixels (0-255) vs Embeddings (-1 Ã  1)
2. **Dimensions incompatibles** : 150k pixels vs 768D embeddings
3. **SÃ©mantique perdue** : LLM n'a jamais vu de pixels pendant training

### 3.2 La Solution : Vision Encoder + Projection

**L'Approche Qui Marche** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ARCHITECTURE MULTIMODALE (SimplifiÃ©)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Ã‰TAPE 1: Encoder l'image en "tokens visuels"              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Image  â”‚  â†’   â”‚ Vision       â”‚  â†’   â”‚ Visual      â”‚    â”‚
â”‚  â”‚224Ã—224Ã—3â”‚      â”‚ Encoder      â”‚      â”‚ Embeddings  â”‚    â”‚
â”‚  â”‚(Pixels)â”‚      â”‚ (CLIP/SigLIP)â”‚      â”‚ [nÃ—768]     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  Ã‰TAPE 2: Projeter dans l'espace du LLM                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Visual      â”‚  â†’   â”‚ Projection   â”‚                     â”‚
â”‚  â”‚ Embeddings  â”‚      â”‚ Layer        â”‚                     â”‚
â”‚  â”‚ [nÃ—768]     â”‚      â”‚ (MLP/QFormer)â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                              â”‚                               â”‚
â”‚  Ã‰TAPE 3: ConcatÃ©ner avec texte                            â”‚
â”‚                              â†“                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ [VISUAL TOKENS] + [TEXT TOKENS]          â”‚              â”‚
â”‚  â”‚                                            â”‚              â”‚
â”‚  â”‚ "What's in this image? <IMG_EMBED> <IMG> â”‚              â”‚
â”‚  â”‚  <EMBED> ... It shows a cat."            â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                              â†“                               â”‚
â”‚  Ã‰TAPE 4: LLM traite le tout ensemble                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚          LLM (Llama, GPT, etc.)        â”‚                â”‚
â”‚  â”‚  Self-Attention sur texte + vision     â”‚                â”‚
â”‚  â”‚  GÃ©nÃ¨re rÃ©ponse en tenant compte imageâ”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Points ClÃ©s** :
1. **Vision Encoder** : Transforme pixels â†’ embeddings sÃ©mantiques
2. **Projection** : Aligne dimensions vision â†” LLM
3. **ConcatÃ©nation** : Vision devient "tokens" comme le texte
4. **LLM** : Traite vision et texte de faÃ§on unifiÃ©e

### 3.3 Les Trois Composants Essentiels

#### A. Vision Encoder (Les "Yeux")

**RÃ´le** : Extraire features sÃ©mantiques de l'image

**Options populaires** :
```python
# Option 1: CLIP (OpenAI) - Le standard
from transformers import CLIPVisionModel

clip_vision = CLIPVisionModel.from_pretrained("openai/clip-vit-large-patch14")
# Input : Image 224Ã—224Ã—3
# Output : 256 patches Ã— 1024D embeddings

# Option 2: SigLIP (Google) - Plus rÃ©cent, meilleur
from transformers import SiglipVisionModel

siglip = SiglipVisionModel.from_pretrained("google/siglip-base-patch16-224")
# AmÃ©lioration : Sigmoid loss > Contrastive loss

# Option 3: DINOv2 (Meta) - Self-supervised
import torch
dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')
# Avantage : Pas besoin de labels texte
```

**Pourquoi CLIP est King ?**
```
CLIP a Ã©tÃ© entraÃ®nÃ© sur 400M paires (image, caption) d'Internet

Exemple paire d'entraÃ®nement :
  Image : [Photo de Golden Retriever jouant au frisbee]
  Texte : "A golden retriever catching a frisbee in a park"

Loss : Contrastive Learning
  â†’ Rapprocher embeddings image â†” texte correct
  â†’ Ã‰loigner embeddings image â†” texte incorrect

RÃ©sultat aprÃ¨s training :
  CLIP "comprend" les concepts visuels parce qu'il les a
  associÃ©s Ã  du langage naturel !
```

#### B. Projection Layer (Le "Traducteur")

**RÃ´le** : Aligner les dimensions vision â†” LLM

**Architectures** :

**1. MLP Simple (LLaVA)** :
```python
class VisionProjector(nn.Module):
    """Projection linÃ©aire simple mais efficace"""

    def __init__(self, vision_dim=1024, llm_dim=4096):
        super().__init__()
        self.projection = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, vision_features):
        """
        Args:
            vision_features: [batch, num_patches, vision_dim]
                            ex: [1, 256, 1024]
        Returns:
            llm_features: [batch, num_patches, llm_dim]
                         ex: [1, 256, 4096]
        """
        return self.projection(vision_features)
```

**2. Q-Former (BLIP-2)** - Plus sophistiquÃ© :
```python
class QFormer(nn.Module):
    """
    Query-Former : MÃ©canisme d'attention intelligent

    Intuition : Au lieu de garder TOUS les patches visuels (256),
                sÃ©lectionner les plus pertinents (32) via attention
    """

    def __init__(self, vision_dim=1024, llm_dim=4096, num_queries=32):
        super().__init__()
        # Queries apprenables (comme des "questions" sur l'image)
        self.queries = nn.Parameter(torch.randn(num_queries, llm_dim))

        # Cross-attention : Queries "regardent" l'image
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=llm_dim,
            num_heads=16
        )

        # Self-attention : Queries se parlent entre elles
        self.self_attention = nn.MultiheadAttention(
            embed_dim=llm_dim,
            num_heads=16
        )

    def forward(self, vision_features):
        """
        Args:
            vision_features: [batch, 256, 1024] - Tous les patches
        Returns:
            compressed: [batch, 32, 4096] - Features compressÃ©es
        """
        batch_size = vision_features.shape[0]

        # 1. RÃ©pÃ©ter queries pour chaque image du batch
        queries = self.queries.unsqueeze(0).repeat(batch_size, 1, 1)

        # 2. Cross-attention : Queries extraient info de l'image
        attended, _ = self.cross_attention(
            query=queries,
            key=vision_features,
            value=vision_features
        )

        # 3. Self-attention : Queries raffinent entre elles
        refined, _ = self.self_attention(
            query=attended,
            key=attended,
            value=attended
        )

        return refined  # [batch, 32, 4096] - CompressÃ© et riche !
```

**Comparaison** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          MLP vs Q-Former                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  MLP (LLaVA)                Q-Former (BLIP-2)         â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”                â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”          â”‚
â”‚                                                        â”‚
â”‚  256 patches â†’ 256 tokens   256 patches â†’ 32 tokens  â”‚
â”‚  Simple                     Intelligent               â”‚
â”‚  Rapide                     SÃ©lectif                  â”‚
â”‚  Tous gardÃ©s                Meilleurs gardÃ©s          â”‚
â”‚                                                        â”‚
â”‚  Avantage :                 Avantage :                â”‚
â”‚  - Plus simple              - Plus efficient          â”‚
â”‚  - Moins de params          - Contexte plus long OK  â”‚
â”‚  - Training facile          - Meilleure compression  â”‚
â”‚                                                        â”‚
â”‚  InconvÃ©nient :             InconvÃ©nient :            â”‚
â”‚  - CoÃ»teux (256 tokens)     - Plus complexe          â”‚
â”‚  - Limite context           - Training harder        â”‚
â”‚                                                        â”‚
â”‚  Use case :                 Use case :                â”‚
â”‚  LLaVA, open-source         BLIP-2, production       â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### C. LLM (Le "Cerveau")

**RÃ´le** : Comprendre et gÃ©nÃ©rer basÃ© sur vision + texte

**Aucune modification nÃ©cessaire !** ğŸ‰

Le gÃ©nie de l'approche, c'est que le LLM n'a PAS besoin de changer. On lui "ment" en disant que les tokens visuels sont du texte, et Ã§a marche !

```python
# LLM pense traiter du texte normal
llm_input = {
    "input_ids": [
        # Tokens texte
        42, 1337, 89, 420,  # "What is in"
        # Tokens visuels (projetÃ©s)
        -1, -1, -1, ...,    # <IMAGE_TOKENS> Ã— 256
        # Suite texte
        1234, 567           # "this image?"
    ]
}

# LLM traite TOUT de faÃ§on uniforme avec self-attention
output = llm.generate(**llm_input)
# GÃ©nÃ©ration : "This image shows a golden retriever..."
```

**Pourquoi Ã§a marche ?**
- Embeddings visuels ont MÃŠME dimensionalitÃ© que texte (4096D)
- Self-attention traite indiffÃ©remment texte et vision
- Positional encodings gÃ¨rent la sÃ©quence mixte

---

## 4. GPT-4V : Le King de la MultimodalitÃ©

### 4.1 Architecture (HypothÃ©tique - OpenAI ne publie pas)

GPT-4V est fermÃ©, mais voici ce qu'on sait par reverse-engineering et papers leaks :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GPT-4V ARCHITECTURE (InfÃ©rÃ©)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Vision Encoder                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚     â”‚ ViT-G/14 (Giant)     â”‚  â† CLIP-like mais BIGGER     â”‚
â”‚     â”‚ ~2B parameters       â”‚                               â”‚
â”‚     â”‚ 224Ã—224 patches      â”‚                               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚               â†“                                             â”‚
â”‚  2. Multi-Resolution Processing                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚     â”‚ 3 Ã©chelles : 224Ã—224, 448Ã—448, 672Ã— â”‚               â”‚
â”‚     â”‚ Capture dÃ©tails fins ET contexte    â”‚               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚               â†“                                             â”‚
â”‚  3. Vision-Language Adapter                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚     â”‚ Perceiver-like       â”‚  â† Attention cross-modale    â”‚
â”‚     â”‚ Compression 2048â†’256 â”‚                               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚               â†“                                             â”‚
â”‚  4. GPT-4 Base Model                                       â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚     â”‚ ~1.7T parameters     â”‚  â† 8Ã— mixture of experts     â”‚
â”‚     â”‚ 32k context          â”‚                               â”‚
â”‚     â”‚ Trained on           â”‚                               â”‚
â”‚     â”‚ text + image pairs   â”‚                               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚               â†“                                             â”‚
â”‚  5. Output Generation                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚     â”‚ Text (toujours)      â”‚                               â”‚
â”‚     â”‚ + Optionnel :        â”‚                               â”‚
â”‚     â”‚   - Image generation â”‚  (DALL-E 3 intÃ©grÃ©)         â”‚
â”‚     â”‚   - Code             â”‚                               â”‚
â”‚     â”‚   - JSON             â”‚                               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Ce Qui Rend GPT-4V SpÃ©cial

**1. Multi-Resolution Understanding** ğŸ”

GPT-4V traite images Ã  PLUSIEURS Ã©chelles simultanÃ©ment :

```
Image originale : 1024Ã—1024

Ã‰chelle 1 (Global) : 224Ã—224
  â†’ Comprend scÃ¨ne gÃ©nÃ©rale, composition

Ã‰chelle 2 (Medium) : 448Ã—448
  â†’ DÃ©tails intermÃ©diaires, objets

Ã‰chelle 3 (Fine) : 672Ã—672 crops
  â†’ Texte petit, dÃ©tails fins
```

**Exemple concret** :
```
Input : Photo d'un panneau de rue en japonais

GPT-4V process :
  Ã‰chelle 1 â†’ "C'est une rue urbaine, panneau sur poteau"
  Ã‰chelle 2 â†’ "Le panneau contient du texte, probablement asiatique"
  Ã‰chelle 3 â†’ "Kanji dÃ©tectÃ©s : æ–°å®¿ (Shinjuku), é§… (Station)"

  Synthesis â†’ "This is a street sign in Shinjuku, Tokyo,
               pointing towards the train station."
```

**2. OCR Quasi-Parfait** ğŸ“„

GPT-4V lit TOUT :
- Texte imprimÃ© (100% accuracy)
- Manuscrit (95% accuracy)
- Ã‰quations mathÃ©matiques LaTeX
- Code (screenshots VS Code)
- Tables et spreadsheets
- Memes avec texte stylisÃ© ğŸ˜‚

**Demo virale** :
```
User uploads : Screenshot de code Python avec bug

GPT-4V :
"I see the issue on line 23. You're using `=` instead of `==`
 in your if statement. Change it to:

 if x == 10:

 Also, the indentation on line 25 is off by one space."

Developers' reaction : ğŸ¤¯
```

**3. Reasoning Visuel** ğŸ§ 

GPT-4V ne se contente pas de "voir", il RAISONNE :

```
Input : Diagramme de circuit Ã©lectrique

GPT-4V response :
"This circuit has a problem. The LED is connected directly
 to the 9V battery without a resistor. This will cause:

 1. LED burnout (excessive current ~45mA)
 2. Battery drain in <30 minutes

 Fix: Add a 200Î© resistor in series.

 Calculation:
 R = (V_battery - V_led) / I_desired
   = (9V - 2V) / 20mA
   = 350Î© (use 200Î© standard value for 22mA)"

Engineer : "Holy shit it understands Ohm's law from a picture"
```

**4. Cultural Understanding** ğŸŒ

GPT-4V comprend le CONTEXTE culturel :

```
Input : Meme avec Drake

GPT-4V :
"This is the 'Drake meme' format (2015), popularized by
 rapper Drake. Top panel (disapproving): [first option]
 Bottom panel (approving): [second option]

 Cultural context: Used to express preference humorously.
 This specific meme suggests [analysis of text]..."

Not just : "Image of man making gestures"
But : "I understand this is a meme, its format, history, and usage"
```

### 4.3 Limitations (Oui, Il en a!)

âš ï¸ **Ce Que GPT-4V Ne Fait PAS Bien**

**1. Comptage PrÃ©cis**
```
Input : Photo de bocal avec 47 bonbons

GPT-4V : "There are approximately 40-50 candies"

Humain : "Non, exactement 47."

Raison : Attention diffuse, pas comptage itÃ©ratif
```

**2. GÃ©omÃ©trie Exacte**
```
Input : "Mesure l'angle de ce triangle"

GPT-4V : "The angle appears to be about 45Â°"

RÃ©alitÃ© : 52Â°

Raison : Pas d'outils de mesure, estimation visuelle
```

**3. Personnes Identifiables**
```
Input : Photo de cÃ©lÃ©britÃ©

GPT-4V : "I see a person, but I cannot identify them."

Raison : Policy OpenAI (privacy), pas limitation technique
```

**4. Images AmbiguÃ«s**
```
Input : Illusion optique (canard/lapin)

GPT-4V : Choisit UNE interprÃ©tation, pas les deux

Raison : ModÃ¨le dÃ©terministe, pas perception multi-stable
```

---

## 5. LLaVA : Vision Open-Source

### 5.1 L'Histoire Inspirante

**Octobre 2023, University of Wisconsin-Madison**

Haotian Liu (Ã©tudiant PhD, 24 ans) pense :
> "GPT-4V coÃ»te $20 millions Ã  entraÃ®ner. Et si je pouvais faire pareil pour $500 ?"

**IdÃ©e gÃ©niale** : Ne PAS entraÃ®ner le LLM ni le vision encoder !
- âœ… CLIP dÃ©jÃ  entraÃ®nÃ© (gratuit)
- âœ… Llama-2 dÃ©jÃ  entraÃ®nÃ© (gratuit)
- ğŸ¯ EntraÃ®ner SEULEMENT le projection layer !

**RÃ©sultat** :
- ğŸ’° CoÃ»t : $500 (1 GPU A100 Ã— 10 heures)
- ğŸ¯ Performance : 85-90% de GPT-4V
- ğŸŒ Impact : 15k stars GitHub, 1000+ citations

**Le monde open-source** : "Wait, WHAT?! On peut faire Ã§a ?!"

### 5.2 Architecture LLaVA

```python
"""
LLaVA = Large Language and Vision Assistant
Architecture ultra-simple mais diablement efficace
"""

class LLaVAModel(nn.Module):
    """
    LLaVA : Vision + Language en 3 composants

    Total parameters : ~7B (Llama) + 1B (CLIP) + 0.1B (Projector)
                     = ~8.1B parameters
    Trainable : SEULEMENT 0.1B (le projector) !
    """

    def __init__(self, vision_tower="openai/clip-vit-large-patch14",
                 language_model="meta-llama/Llama-2-7b-hf"):
        super().__init__()

        # 1. Vision Encoder (FROZEN â„ï¸)
        self.vision_tower = CLIPVisionModel.from_pretrained(vision_tower)
        self.vision_tower.requires_grad_(False)  # Pas de backprop !

        # 2. Language Model (FROZEN â„ï¸)
        self.language_model = AutoModelForCausalLM.from_pretrained(language_model)
        self.language_model.requires_grad_(False)  # Pas de backprop !

        # 3. Projection Layer (TRAINABLE ğŸ”¥)
        vision_hidden_size = self.vision_tower.config.hidden_size  # 1024
        llm_hidden_size = self.language_model.config.hidden_size    # 4096

        self.mm_projector = nn.Sequential(
            nn.Linear(vision_hidden_size, llm_hidden_size),
            nn.GELU(),
            nn.Linear(llm_hidden_size, llm_hidden_size)
        )
        # Ces 2 linear layers = TOUT ce qu'on entraÃ®ne ! (~100M params)

    def forward(self, images, input_ids, attention_mask=None):
        """
        Forward pass pour training ou inference

        Args:
            images: [batch, 3, 224, 224] - Images RGB
            input_ids: [batch, seq_len] - Tokens texte avec placeholder <IMAGE>
            attention_mask: [batch, seq_len] - Mask pour padding

        Returns:
            logits: [batch, seq_len, vocab_size] - PrÃ©dictions
        """
        batch_size = images.shape[0]

        # Ã‰TAPE 1 : Encoder l'image
        with torch.no_grad():  # Pas de gradient sur CLIP
            vision_outputs = self.vision_tower(images)
            image_features = vision_outputs.last_hidden_state
            # Shape : [batch, num_patches, 1024]
            # num_patches = 256 pour CLIP (14Ã—14 grid + CLS token)

        # Ã‰TAPE 2 : Projeter dans l'espace LLM
        image_features_projected = self.mm_projector(image_features)
        # Shape : [batch, 256, 4096]

        # Ã‰TAPE 3 : Remplacer <IMAGE> token par features visuelles
        # input_ids contient un token spÃ©cial IMAGE_TOKEN_INDEX = -200
        # On remplace ce token par les 256 tokens visuels

        # Trouver position du token <IMAGE>
        image_token_mask = input_ids == IMAGE_TOKEN_INDEX
        # Shape : [batch, seq_len] - True lÃ  oÃ¹ il y a <IMAGE>

        # CrÃ©er embeddings texte
        with torch.no_grad():  # Pas de gradient sur embeddings texte
            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)
            # Shape : [batch, seq_len, 4096]

        # Remplacer token <IMAGE> par features visuelles
        for batch_idx in range(batch_size):
            image_positions = torch.where(image_token_mask[batch_idx])[0]

            if len(image_positions) > 0:
                # Remplacer le token placeholder par les 256 patches
                start_pos = image_positions[0]
                inputs_embeds[batch_idx, start_pos:start_pos+256] = \
                    image_features_projected[batch_idx]

        # Ã‰TAPE 4 : Forward pass Ã  travers LLM
        outputs = self.language_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            return_dict=True
        )

        return outputs.logits


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¡ POURQUOI C'EST GÃ‰NIAL ?
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# 1. EFFICIENT :
#    - Seulement 100M params Ã  entraÃ®ner (1.2% du total)
#    - Training sur 1 GPU : possible !
#    - Inference rapide : Llama-2 dÃ©jÃ  optimisÃ©
#
# 2. FLEXIBLE :
#    - Swap vision encoder : CLIP â†’ SigLIP â†’ DINOv2
#    - Swap LLM : Llama â†’ Mistral â†’ Qwen
#    - Adapter le projector : MLP â†’ Q-Former â†’ Perceiver
#
# 3. SCALABLE :
#    - LLaVA-7B : Ce code
#    - LLaVA-13B : Change juste le LLM
#    - LLaVA-34B : Pareil !
#
# 4. ACCESSIBLE :
#    - Open-source (Apache 2.0)
#    - Datasets publics
#    - Training cost : <$1000
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 5.3 Training Recipe LLaVA

**Le Secret : Two-Stage Training**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LLAVA TRAINING PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  STAGE 1 : Pre-training (Feature Alignment)              â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”             â”‚
â”‚  Objectif : Aligner features vision â†” LLM                â”‚
â”‚  Dataset : 595K image-caption pairs (LAION/CC3M)         â”‚
â”‚  Frozen : CLIP â„ï¸ + Llama â„ï¸                            â”‚
â”‚  Trainable : Projector ONLY ğŸ”¥                           â”‚
â”‚  Epochs : 1                                               â”‚
â”‚  Batch size : 128                                         â”‚
â”‚  LR : 2e-3                                                â”‚
â”‚  Time : 4 hours on 8Ã— A100                               â”‚
â”‚  Loss : Next-token prediction                            â”‚
â”‚                                                           â”‚
â”‚  Exemple :                                                â”‚
â”‚    Image : [Photo de chat]                               â”‚
â”‚    Caption : "A cat sitting on a couch"                  â”‚
â”‚    Task : GÃ©nÃ©rer caption depuis image                   â”‚
â”‚                                                           â”‚
â”‚  â¬‡ï¸                                                       â”‚
â”‚                                                           â”‚
â”‚  STAGE 2 : Fine-tuning (Instruction Following)           â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”             â”‚
â”‚  Objectif : Apprendre Ã  rÃ©pondre questions               â”‚
â”‚  Dataset : 158K instruction-following pairs              â”‚
â”‚  Frozen : CLIP â„ï¸                                        â”‚
â”‚  Trainable : Projector ğŸ”¥ + Llama (LoRA) ğŸ”¥             â”‚
â”‚  Epochs : 3                                               â”‚
â”‚  Batch size : 32                                          â”‚
â”‚  LR : 2e-5                                                â”‚
â”‚  Time : 10 hours on 8Ã— A100                              â”‚
â”‚  Loss : Instruction tuning loss                          â”‚
â”‚                                                           â”‚
â”‚  Exemple :                                                â”‚
â”‚    Image : [Diagramme circuit]                           â”‚
â”‚    Question : "What's wrong with this circuit?"          â”‚
â”‚    Answer : "The resistor value is too low..."           â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dataset Generation (The Secret Sauce)** ğŸ¤«

LLaVA utilise GPT-4 pour GÃ‰NÃ‰RER le dataset !

```python
"""
Bootstrapping avec GPT-4 : Le Hack GÃ©nial
==========================================

ProblÃ¨me : Besoin de 158k paires (image, question, answer)
          Annotation humaine = $$$$ (cher et lent)

Solution : Utiliser GPT-4 pour gÃ©nÃ©rer Q&A !

Pipeline :
1. Prendre image du dataset (ex: COCO)
2. Avoir caption existante : "A person skiing down a mountain"
3. Demander Ã  GPT-4 (text-only!) de gÃ©nÃ©rer Q&A diverse
"""

def generate_llava_dataset():
    """Pipeline de gÃ©nÃ©ration dataset LLaVA"""

    # Pour chaque image du dataset source
    for image, caption in coco_dataset:

        # Prompt Ã  GPT-4 (text-only, pas vision!)
        prompt = f"""
        Given an image with caption: "{caption}"

        Generate 3 diverse question-answer pairs that require
        visual understanding. Include:
        - 1 descriptive question (What/Where)
        - 1 reasoning question (Why/How)
        - 1 detailed analysis question

        Format: JSON
        """

        # GPT-4 gÃ©nÃ¨re les Q&A
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )

        qa_pairs = json.loads(response.choices[0].message.content)

        # Exemple output :
        # [
        #   {
        #     "question": "What activity is the person doing?",
        #     "answer": "The person is skiing down a snow-covered mountain..."
        #   },
        #   {
        #     "question": "Why might this be considered dangerous?",
        #     "answer": "Skiing down steep mountain slopes can be risky due to..."
        #   },
        #   {
        #     "question": "Describe the environment and conditions.",
        #     "answer": "The scene shows a mountainous terrain with deep snow..."
        #   }
        # ]

        # Sauvegarder paire (image, Q&A)
        save_training_sample(image, qa_pairs)

# RÃ©sultat : 158K Ã©chantillons gÃ©nÃ©rÃ©s en quelques jours
# CoÃ»t : ~$500 de crÃ©dits API GPT-4
# QualitÃ© : Proche annotation humaine !
```

**Pourquoi Ã‡a Marche** :
1. GPT-4 (text) gÃ©nÃ¨re questions sophistiquÃ©es
2. LLaVA apprend Ã  y rÃ©pondre en VOYANT l'image
3. Bootstrapping : Model A (GPT-4) aide Model B (LLaVA)
4. Cycle vertueux : LLaVA devient quasi aussi bon que GPT-4V !

### 5.4 Code Complet : Utiliser LLaVA

```python
"""
UTILISATION PRATIQUE DE LLAVA
=============================

Installation :
  pip install llava transformers torch pillow

Usage : Poser des questions sur images !
"""

from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
from PIL import Image
import torch

class LLaVAChatbot:
    """Interface simple pour discuter avec LLaVA"""

    def __init__(self, model_path="liuhaotian/llava-v1.5-7b"):
        """
        Charger modÃ¨le LLaVA

        Args:
            model_path: Chemin HuggingFace du modÃ¨le
                       Options :
                       - llava-v1.5-7b (rapide, 7B params)
                       - llava-v1.5-13b (meilleur, 13B params)
                       - llava-v1.6-34b (SOTA, 34B params)
        """
        print(f"Chargement de {model_path}...")

        self.tokenizer, self.model, self.image_processor, self.context_len = \
            load_pretrained_model(
                model_path=model_path,
                model_base=None,
                model_name=get_model_name_from_path(model_path),
                load_8bit=False,  # Mettre True si peu de VRAM
                load_4bit=False   # Ou quantization 4-bit (QLoRA)
            )

        print("ModÃ¨le chargÃ© ! ğŸ‰")

    def chat(self, image_path: str, question: str, temperature=0.2, max_tokens=512):
        """
        Poser une question sur une image

        Args:
            image_path: Chemin vers l'image
            question: Question en langage naturel
            temperature: CrÃ©ativitÃ© (0=factuel, 1=crÃ©atif)
            max_tokens: Longueur max rÃ©ponse

        Returns:
            answer: RÃ©ponse du modÃ¨le
        """

        # 1. Charger et preprocesser image
        image = Image.open(image_path).convert('RGB')
        image_tensor = process_images([image], self.image_processor, self.model.config)
        image_tensor = image_tensor.to(self.model.device, dtype=torch.float16)

        # 2. Formater prompt
        # LLaVA utilise format spÃ©cial avec token <image>
        prompt = f"USER: <image>\n{question}\nASSISTANT:"

        # 3. Tokenize
        input_ids = tokenizer_image_token(
            prompt,
            self.tokenizer,
            IMAGE_TOKEN_INDEX,
            return_tensors='pt'
        ).unsqueeze(0).to(self.model.device)

        # 4. GÃ©nÃ©ration
        with torch.inference_mode():
            output_ids = self.model.generate(
                input_ids,
                images=image_tensor,
                do_sample=True if temperature > 0 else False,
                temperature=temperature,
                max_new_tokens=max_tokens,
                use_cache=True
            )

        # 5. DÃ©coder rÃ©ponse
        answer = self.tokenizer.decode(
            output_ids[0, input_ids.shape[1]:],
            skip_special_tokens=True
        ).strip()

        return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ EXAMPLES D'UTILISATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # CrÃ©er chatbot
    bot = LLaVAChatbot(model_path="liuhaotian/llava-v1.5-7b")

    # Exemple 1 : Description simple
    answer = bot.chat(
        image_path="cat.jpg",
        question="What's in this image?"
    )
    print(f"Q: What's in this image?")
    print(f"A: {answer}\n")
    # Output : "The image shows a fluffy orange cat lying on a blue couch..."

    # Exemple 2 : Comptage
    answer = bot.chat(
        image_path="fruit_basket.jpg",
        question="How many apples are in the basket?"
    )
    print(f"Q: How many apples?")
    print(f"A: {answer}\n")
    # Output : "There are approximately 5-6 apples in the basket."

    # Exemple 3 : Reasoning
    answer = bot.chat(
        image_path="broken_circuit.jpg",
        question="What's wrong with this electronic circuit and how to fix it?"
    )
    print(f"Q: What's wrong with circuit?")
    print(f"A: {answer}\n")
    # Output : "The circuit appears to have a short circuit between..."

    # Exemple 4 : OCR
    answer = bot.chat(
        image_path="handwritten_note.jpg",
        question="Transcribe the handwritten text in this image."
    )
    print(f"Q: Transcribe text")
    print(f"A: {answer}\n")

    # Exemple 5 : CrÃ©atif
    answer = bot.chat(
        image_path="sunset.jpg",
        question="Write a haiku about this sunset.",
        temperature=0.7  # Plus crÃ©atif
    )
    print(f"Q: Write haiku")
    print(f"A: {answer}\n")
    # Output :
    # "Golden rays descend
    #  Painting clouds in crimson hues
    #  Day whispers goodbye"
```

---

## 6. BLIP-2 et Flamingo : Architectures Alternatives

### 6.1 BLIP-2 : Le Q-Former Genius

**BLIP-2** (Salesforce, 2023) a introduit une architecture rÃ©volutionnaire : le **Q-Former**.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¬ **DIALOGUE : Comprendre le Q-Former**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Alice** : "Bob, LLaVA envoie 256 tokens visuels au LLM. C'est pas un peu... beaucoup ?"

**Bob** : "Excellente observation ! Imagine que tu regardes une photo. Tu ne mÃ©morises pas CHAQUE pixel, n'est-ce pas ?"

**Alice** : "Non, j'extrais les infos importantes : 'chat orange', 'canapÃ© bleu', 'fenÃªtre lumineuse'..."

**Bob** : "Exactement ! C'est ce que fait le Q-Former. Au lieu de garder 256 patches, il pose des 'questions intelligentes' Ã  l'image et garde seulement les rÃ©ponses. RÃ©sultat : 32 tokens au lieu de 256."

**Alice** : "Attends... des 'questions' Ã  une image ? C'est pas un peu abstrait ?"

**Bob** : "Pense Ã  Ã§a comme un interrogatoire de dÃ©tective :
- Question 1 : 'Y a-t-il un objet principal ?' â†’ Oui, un chat
- Question 2 : 'Quelle est sa couleur ?' â†’ Orange
- Question 3 : 'OÃ¹ est-il situÃ© ?' â†’ Sur un canapÃ©
- ...32 questions au total

Chaque 'question' (query) extrait UNE information importante via attention. 32 queries = 32 infos essentielles = 32 tokens compressÃ©s !"

**Alice** : "C'est brillant ! Donc plus efficace que LLaVA qui garde tout ?"

**Bob** : "Pour les longs contextes, oui. Mais LLaVA est plus simple et fonctionne dÃ©jÃ  super bien. Trade-off complexitÃ© vs performance."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

### 6.2 Architecture BLIP-2

```python
"""
BLIP-2 Architecture
===================

Innovation : Q-Former = Learnable Queries + Cross-Attention
RÃ©sultat : Compression intelligente 256 â†’ 32 tokens
"""

import torch
import torch.nn as nn

class QFormer(nn.Module):
    """
    Q-Former : Query Transformer pour compression vision

    Intuition :
    - 32 queries apprenables (comme des 'questions')
    - Cross-attention : Queries 'interrogent' l'image
    - Self-attention : Queries se raffinent entre elles
    - Output : 32 tokens riches en information
    """

    def __init__(
        self,
        num_queries=32,
        hidden_dim=768,
        num_attention_heads=12,
        num_layers=6
    ):
        super().__init__()

        # Learnable queries (les "questions" posÃ©es Ã  l'image)
        self.query_tokens = nn.Parameter(torch.randn(num_queries, hidden_dim))
        # Shape : [32, 768]

        # Transformer Encoder Layers
        self.layers = nn.ModuleList([
            QFormerLayer(hidden_dim, num_attention_heads)
            for _ in range(num_layers)
        ])

        # Layer norm final
        self.ln = nn.LayerNorm(hidden_dim)

    def forward(self, vision_features):
        """
        Args:
            vision_features: [batch, 256, 1024] - CLIP output

        Returns:
            compressed: [batch, 32, 768] - Features compressÃ©es
        """
        batch_size = vision_features.shape[0]

        # RÃ©pÃ©ter queries pour chaque image du batch
        queries = self.query_tokens.unsqueeze(0).repeat(batch_size, 1, 1)
        # Shape : [batch, 32, 768]

        # Passer Ã  travers les layers
        for layer in self.layers:
            queries = layer(
                queries=queries,
                vision_features=vision_features
            )

        # Normalisation finale
        queries = self.ln(queries)

        return queries


class QFormerLayer(nn.Module):
    """Une couche du Q-Former avec cross + self attention"""

    def __init__(self, hidden_dim, num_heads):
        super().__init__()

        # Cross-Attention : Queries â†’ Vision
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        self.cross_attn_ln = nn.LayerNorm(hidden_dim)

        # Self-Attention : Queries â†’ Queries
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        self.self_attn_ln = nn.LayerNorm(hidden_dim)

        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        self.ffn_ln = nn.LayerNorm(hidden_dim)

    def forward(self, queries, vision_features):
        """
        Args:
            queries: [batch, 32, 768]
            vision_features: [batch, 256, 1024]
        """

        # 1. Cross-Attention : Queries "regardent" l'image
        attended, _ = self.cross_attention(
            query=queries,
            key=vision_features,
            value=vision_features
        )
        queries = self.cross_attn_ln(queries + attended)  # Residual

        # 2. Self-Attention : Queries se parlent entre elles
        self_attended, _ = self.self_attention(
            query=queries,
            key=queries,
            value=queries
        )
        queries = self.self_attn_ln(queries + self_attended)  # Residual

        # 3. FFN
        ffn_output = self.ffn(queries)
        queries = self.ffn_ln(queries + ffn_output)  # Residual

        return queries


class BLIP2Model(nn.Module):
    """ModÃ¨le BLIP-2 complet"""

    def __init__(self):
        super().__init__()

        # Vision Encoder (FROZEN â„ï¸)
        from transformers import CLIPVisionModel
        self.vision_encoder = CLIPVisionModel.from_pretrained(
            "openai/clip-vit-large-patch14"
        )
        self.vision_encoder.requires_grad_(False)

        # Q-Former (TRAINABLE ğŸ”¥)
        self.qformer = QFormer(
            num_queries=32,
            hidden_dim=768,
            num_attention_heads=12,
            num_layers=6
        )

        # Projection vers LLM (TRAINABLE ğŸ”¥)
        self.projection = nn.Linear(768, 4096)  # 768 â†’ Llama dim

        # LLM (FROZEN â„ï¸)
        from transformers import AutoModelForCausalLM
        self.llm = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-7b-hf"
        )
        self.llm.requires_grad_(False)

    def forward(self, images, input_ids):
        batch_size = images.shape[0]

        # 1. Vision encoding
        with torch.no_grad():
            vision_outputs = self.vision_encoder(images)
            vision_features = vision_outputs.last_hidden_state
            # [batch, 256, 1024]

        # 2. Q-Former compression (THE MAGIC âœ¨)
        compressed_features = self.qformer(vision_features)
        # [batch, 32, 768] - De 256 â†’ 32 tokens !

        # 3. Projection vers LLM space
        llm_features = self.projection(compressed_features)
        # [batch, 32, 4096]

        # 4. Concat avec texte et forward LLM
        # (Similaire Ã  LLaVA mais avec seulement 32 tokens visuels)
        outputs = self.llm(
            inputs_embeds=llm_features,
            # ... reste similaire Ã  LLaVA
        )

        return outputs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¡ POURQUOI Q-FORMER EST GÃ‰NIAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# AVANTAGES :
# 1. Compression 8Ã— (256 â†’ 32 tokens)
# 2. Contexte plus long disponible pour texte
# 3. Inference plus rapide (moins de tokens Ã  traiter)
# 4. Flexible : Change facilement le nombre de queries
#
# INCONVÃ‰NIENTS :
# 1. Plus complexe Ã  entraÃ®ner
# 2. Plus de paramÃ¨tres (Q-Former = 188M params)
# 3. Risque de perte d'info si trop peu de queries
#
# USE CASES :
# - Long documents avec images
# - Multi-image conversations
# - Applications oÃ¹ contexte limitÃ© critique
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 6.3 Flamingo : Few-Shot Learning Master

**Flamingo** (DeepMind, 2022) a Ã©tÃ© le premier "vrai" LLM multimodal avec capacitÃ©s **few-shot**.

**Innovation** : Perceiver Resampler + Gated Cross-Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FLAMINGO ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  Vision Encoder (Normalizer-Free ResNet)                 â”‚
â”‚         â†“                                                 â”‚
â”‚  Perceiver Resampler (Compression)                       â”‚
â”‚         â†“                                                 â”‚
â”‚  LLM avec Gated Cross-Attention Layers                   â”‚
â”‚         â†“                                                 â”‚
â”‚  Text Generation                                          â”‚
â”‚                                                           â”‚
â”‚  INNOVATION : Gated Cross-Attention                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   â”‚
â”‚                                                           â”‚
â”‚  LLM layers alternent :                                  â”‚
â”‚  1. Self-Attention (text-only)                           â”‚
â”‚  2. Gated Cross-Attention (text â†” vision)               â”‚
â”‚  3. FFN                                                   â”‚
â”‚                                                           â”‚
â”‚  Le "gating" permet au modÃ¨le de choisir :              â”‚
â”‚  - Utiliser vision (gate=1)                              â”‚
â”‚  - Ignorer vision (gate=0)                               â”‚
â”‚                                                           â”‚
â”‚  RÃ©sultat : Flexible et adaptable !                      â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CapacitÃ© Few-Shot Impressionnante** :

```
Input (Few-shot examples) :
  [Image1: Red car] â†’ "This is a red car"
  [Image2: Blue truck] â†’ "This is a blue truck"
  [Image3: Green motorcycle] â†’ ???

Flamingo Output : "This is a green motorcycle"

Learned pattern from just 2 examples! ğŸ¤¯
```

âš ï¸ **Limitation** : Flamingo est **fermÃ©** (DeepMind n'a pas publiÃ© les poids)

---

## 7. Au-delÃ  de la Vision : Audio et VidÃ©o

### 7.1 Audio-Language Models

**Whisper** (OpenAI, 2022) + LLM = Chatbot vocal

```python
"""
AUDIO-LANGUAGE PIPELINE
=======================

Pipeline : Audio â†’ Transcription â†’ LLM â†’ Response
"""

from transformers import WhisperProcessor, WhisperForConditionalGeneration
import torch

class AudioLLMChatbot:
    """Chatbot qui comprend l'audio"""

    def __init__(self):
        # Audio â†’ Text (Whisper)
        self.whisper_processor = WhisperProcessor.from_pretrained(
            "openai/whisper-large-v3"
        )
        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
            "openai/whisper-large-v3"
        )

        # Text â†’ Response (LLM)
        from transformers import AutoModelForCausalLM, AutoTokenizer
        self.llm = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
        self.tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

    def process_audio(self, audio_path):
        """
        Traiter audio et gÃ©nÃ©rer rÃ©ponse

        Args:
            audio_path: Chemin vers fichier audio (.wav, .mp3)

        Returns:
            response: RÃ©ponse textuelle du LLM
        """
        import librosa

        # 1. Charger audio
        audio, sr = librosa.load(audio_path, sr=16000)

        # 2. Transcription avec Whisper
        inputs = self.whisper_processor(
            audio,
            sampling_rate=16000,
            return_tensors="pt"
        )

        with torch.no_grad():
            generated_ids = self.whisper_model.generate(inputs.input_features)

        transcription = self.whisper_processor.batch_decode(
            generated_ids,
            skip_special_tokens=True
        )[0]

        print(f"ğŸ¤ Transcription: {transcription}")

        # 3. RÃ©ponse LLM
        prompt = f"USER: {transcription}\nASSISTANT:"
        inputs = self.tokenizer(prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = self.llm.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("ASSISTANT:")[-1].strip()

        print(f"ğŸ¤– Response: {response}")

        return response


# Exemple d'utilisation
if __name__ == "__main__":
    bot = AudioLLMChatbot()

    # User enregistre : "What's the weather like today?"
    response = bot.process_audio("user_question.wav")

    # Bot rÃ©pond en texte (peut Ãªtre converti en audio avec TTS)
```

**AudioLM** (Google, 2023) : GÃ©nÃ©ration audio directe (pas de texte intermÃ©diaire)

### 7.2 Video Understanding

**VidÃ©o = SÃ©quence d'images + Audio**

**Challenge** : Une vidÃ©o de 1 minute = 1800 frames (30 fps) !

**Solutions** :

**1. Frame Sampling** : Prendre 1 frame toutes les N secondes
```python
# Extraire 8 frames d'une vidÃ©o de 30 secondes
frames = extract_frames(video, num_frames=8)  # 1 frame tous les 4 sec

# Traiter comme multi-image
for frame in frames:
    features = vision_encoder(frame)
    # Concat toutes les features
```

**2. Video-Specific Encoders** : ViViT, VideoMAE
- Utilisent attention spatio-temporelle
- Capturent le mouvement entre frames

**3. Gemini 1.5 Pro Approach** : Long-context
```
Gemini 1.5 Pro peut analyser 1 HEURE de vidÃ©o !

Comment ?
- Compression spatiale (comme Q-Former)
- Compression temporelle (sampling intelligent)
- Long context window (1M tokens)

RÃ©sultat : Peut rÃ©pondre "Ã€ quelle minute le personnage
            principal apparaÃ®t-il pour la premiÃ¨re fois ?"
```

**Exemple Code** :

```python
"""
VIDEO UNDERSTANDING avec LLaVA-Video (conceptuel)
"""

class VideoLLM:
    """Analyser des vidÃ©os avec un LLM"""

    def __init__(self):
        self.vision_encoder = load_clip()
        self.llm = load_llama()

    def analyze_video(self, video_path, question):
        """
        Analyser vidÃ©o et rÃ©pondre Ã  question

        Args:
            video_path: Chemin vers vidÃ©o
            question: Question sur la vidÃ©o
        """

        # 1. Extraire frames (intelligent sampling)
        frames = self.extract_key_frames(video_path, num_frames=16)
        # Prend frames aux moments clÃ©s (changements de scÃ¨ne, etc.)

        # 2. Encoder chaque frame
        frame_features = []
        for frame in frames:
            features = self.vision_encoder(frame)
            frame_features.append(features)

        # 3. Concat temporellement
        video_features = torch.cat(frame_features, dim=1)
        # Shape : [1, 16*256, 1024] = [1, 4096, 1024]

        # 4. Projection et LLM
        projected = self.projection(video_features)

        # 5. Question + RÃ©ponse
        prompt = f"Video context: <VIDEO_FEATURES>\nQuestion: {question}\nAnswer:"
        response = self.llm.generate(prompt, video_features=projected)

        return response

    def extract_key_frames(self, video_path, num_frames=16):
        """
        Extraction intelligente de frames clÃ©s

        MÃ©thodes :
        1. Uniform sampling : 1 frame tous les N secondes
        2. Change detection : Frames oÃ¹ scÃ¨ne change
        3. Importance sampling : Frames avec le plus d'action
        """
        import cv2

        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # Uniform sampling pour simplification
        frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)

        frames = []
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)

        cap.release()
        return frames


# Use case : Analyse de film
vlm = VideoLLM()

response = vlm.analyze_video(
    "movie.mp4",
    "Summarize the main plot points of this movie."
)

print(response)
# "The movie follows a hero's journey where the protagonist
#  discovers hidden powers, faces betrayal from a trusted ally,
#  and ultimately saves their world in a climactic battle..."
```

**Applications VidÃ©o RÃ©elles** :
- ğŸ“¹ **Surveillance** : "DÃ©tecte si quelqu'un vole dans cette vidÃ©o"
- ğŸ¬ **Editing** : "Trouve tous les plans oÃ¹ le personnage sourit"
- ğŸ€ **Sports Analysis** : "Combien de shoots Ã  3 points dans ce match ?"
- ğŸ“ **Education** : "RÃ©sume ce cours de 1 heure en 3 bullet points"

---

## 8. Training Paradigms

### 8.1 Les Trois Approches

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TRAINING STRATEGIES MULTIMODAL                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  1. Freeze Vision + LLM, Train Projector Only            â”‚
â”‚     âœ… Rapide, cheap ($500)                              â”‚
â”‚     âœ… Stable (pas de catastrophic forgetting)           â”‚
â”‚     âŒ Performance limitÃ©e                               â”‚
â”‚     ğŸ‘‰ LLaVA, MiniGPT-4                                  â”‚
â”‚                                                           â”‚
â”‚  2. Freeze Vision, Fine-tune LLM + Projector             â”‚
â”‚     âœ… Meilleure performance                             â”‚
â”‚     âœ… Adaptable au domaine                              â”‚
â”‚     âŒ Plus cher (~$5k-10k)                              â”‚
â”‚     âŒ Risque overfitting                                â”‚
â”‚     ğŸ‘‰ LLaVA-1.5, Qwen-VL                                â”‚
â”‚                                                           â”‚
â”‚  3. Joint Training (Vision + LLM + Projector)            â”‚
â”‚     âœ… Performance SOTA                                   â”‚
â”‚     âœ… Alignement optimal                                â”‚
â”‚     âŒ TrÃ¨s cher ($100k+)                                â”‚
â”‚     âŒ Instable, difficile                               â”‚
â”‚     ğŸ‘‰ GPT-4V, Gemini, Claude 3                          â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 Recipe ComplÃ¨te : EntraÃ®ner Votre ModÃ¨le Multimodal

**Ã‰tape par Ã‰tape** :

```python
"""
TRAINING RECIPE : CrÃ©er Votre LLaVA
====================================

Dataset : 595K image-caption (stage 1) + 158K instruction (stage 2)
Hardware : 8Ã— A100 40GB
Temps : ~14 heures total
CoÃ»t : ~$500 sur cloud
"""

import torch
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 1 : Pre-training (Feature Alignment)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def stage1_pretraining():
    """
    Objectif : Aligner vision features avec LLM space
    Dataset : LAION-CC-SBU (595K image-caption pairs)
    Frozen : CLIP + Llama
    Trainable : Projector ONLY
    """

    # 1. Charger modÃ¨le
    model = LLaVAModel(
        vision_tower="openai/clip-vit-large-patch14",
        language_model="meta-llama/Llama-2-7b-hf"
    )

    # Freeze vision + LLM
    model.vision_tower.requires_grad_(False)
    model.language_model.requires_grad_(False)
    # Projector reste trainable

    # 2. Dataset
    dataset = load_dataset("liuhaotian/LLaVA-Pretrain")
    # Format : {"image": PIL.Image, "caption": "A cat sitting..."}

    # 3. Training args
    training_args = TrainingArguments(
        output_dir="./llava-stage1",
        num_train_epochs=1,
        per_device_train_batch_size=16,  # Ã— 8 GPUs = 128 total
        gradient_accumulation_steps=1,
        learning_rate=2e-3,
        warmup_steps=1000,
        lr_scheduler_type="cosine",
        save_steps=5000,
        logging_steps=100,
        bf16=True,                       # Mixed precision
        dataloader_num_workers=4,
        remove_unused_columns=False
    )

    # 4. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=llava_collate_fn  # Custom collator
    )

    # 5. Train !
    trainer.train()

    # 6. Save projector weights
    torch.save(
        model.mm_projector.state_dict(),
        "projector_weights.pth"
    )

    print("âœ… Stage 1 terminÃ© !")
    print("Projector alignÃ© : vision features â†’ LLM space")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 2 : Instruction Tuning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def stage2_instruction_tuning():
    """
    Objectif : Apprendre Ã  suivre instructions
    Dataset : LLaVA-Instruct (158K instruction pairs)
    Frozen : CLIP
    Trainable : Projector + Llama (avec LoRA)
    """

    # 1. Charger modÃ¨le avec projector prÃ©-entraÃ®nÃ©
    model = LLaVAModel(
        vision_tower="openai/clip-vit-large-patch14",
        language_model="meta-llama/Llama-2-7b-hf"
    )
    model.mm_projector.load_state_dict(
        torch.load("projector_weights.pth")
    )

    # Freeze vision
    model.vision_tower.requires_grad_(False)

    # Unfreeze LLM avec LoRA
    from peft import LoraConfig, get_peft_model

    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model.language_model = get_peft_model(
        model.language_model,
        lora_config
    )

    # 2. Dataset
    dataset = load_dataset("liuhaotian/LLaVA-Instruct-150K")
    # Format : {
    #   "image": PIL.Image,
    #   "conversations": [
    #     {"from": "human", "value": "What's in this image?"},
    #     {"from": "gpt", "value": "The image shows..."}
    #   ]
    # }

    # 3. Training args
    training_args = TrainingArguments(
        output_dir="./llava-stage2",
        num_train_epochs=3,
        per_device_train_batch_size=4,   # Ã— 8 GPUs = 32 total
        gradient_accumulation_steps=4,   # Effective batch = 128
        learning_rate=2e-5,              # Plus petit que stage 1
        warmup_steps=100,
        lr_scheduler_type="cosine",
        save_steps=1000,
        logging_steps=50,
        bf16=True,
        dataloader_num_workers=4,
        remove_unused_columns=False
    )

    # 4. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=llava_instruction_collate_fn
    )

    # 5. Train !
    trainer.train()

    # 6. Save final model
    model.save_pretrained("./llava-final")

    print("âœ… Stage 2 terminÃ© !")
    print("ModÃ¨le prÃªt pour instruction following !")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPER FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def llava_collate_fn(batch):
    """Collate function pour stage 1 (caption)"""
    images = [item['image'] for item in batch]
    captions = [item['caption'] for item in batch]

    # Process images
    from PIL import Image
    import torchvision.transforms as transforms

    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],
            std=[0.26862954, 0.26130258, 0.27577711]
        )
    ])

    images_tensor = torch.stack([transform(img) for img in images])

    # Tokenize captions
    # Format : "<image>\n{caption}"
    # ...

    return {
        'images': images_tensor,
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels
    }


def llava_instruction_collate_fn(batch):
    """Collate function pour stage 2 (instruction)"""
    # Similar mais avec conversations multi-turn
    # ...
    pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN : ExÃ©cuter le training
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("ğŸš€ DÃ©marrage training LLaVA !")
    print()

    # Stage 1 : ~4 hours
    print("=" * 60)
    print("STAGE 1 : Pre-training")
    print("=" * 60)
    stage1_pretraining()

    # Stage 2 : ~10 hours
    print()
    print("=" * 60)
    print("STAGE 2 : Instruction Tuning")
    print("=" * 60)
    stage2_instruction_tuning()

    print()
    print("ğŸ‰ Training complet terminÃ© !")
    print("Total time : ~14 hours")
    print("Total cost : ~$500")
    print()
    print("Votre modÃ¨le multimodal est prÃªt ! ğŸ¦™ğŸ‘ï¸")
```

---

## 9. Projet Pratique : CrÃ©er Votre Chatbot Vision

### 9.1 Objectif du Projet

CrÃ©er un **chatbot multimodal complet** avec :
- âœ… Interface web (Gradio)
- âœ… Support images (upload ou URL)
- âœ… Conversation multi-turn
- âœ… Historique
- âœ… DÃ©ploiement

**Temps estimÃ©** : 2-3 heures
**Niveau** : IntermÃ©diaire

### 9.2 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           VISION CHATBOT ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  Frontend (Gradio)                                        â”‚
â”‚      â†“                                                    â”‚
â”‚  API Layer (FastAPI)                                     â”‚
â”‚      â†“                                                    â”‚
â”‚  LLaVA Model (Inference)                                 â”‚
â”‚      â†“                                                    â”‚
â”‚  Response                                                 â”‚
â”‚                                                           â”‚
â”‚  Features :                                              â”‚
â”‚  - Image upload                                          â”‚
â”‚  - Multi-turn conversation                               â”‚
â”‚  - History tracking                                      â”‚
â”‚  - Temperature control                                   â”‚
â”‚  - Max tokens slider                                     â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.3 Code Complet

**Partie 1 : Backend (FastAPI)**

```python
"""
vision_chatbot/backend.py
=========================

Backend FastAPI pour chatbot vision
"""

from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import torch
from PIL import Image
import io
import base64

# Importer LLaVA
from llava.model.builder import load_pretrained_model
from llava.mm_utils import process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX

app = FastAPI(title="Vision Chatbot API")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Message(BaseModel):
    role: str  # "user" or "assistant"
    content: str
    image_data: Optional[str] = None  # Base64 encoded

class ChatRequest(BaseModel):
    messages: List[Message]
    temperature: float = 0.7
    max_tokens: int = 512

class ChatResponse(BaseModel):
    response: str
    finish_reason: str = "stop"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD MODEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("Loading LLaVA model...")
tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path="liuhaotian/llava-v1.5-7b",
    model_base=None,
    model_name="llava-v1.5-7b",
    load_8bit=True  # Quantization pour Ã©conomiser VRAM
)
print("Model loaded! âœ…")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat endpoint avec support images

    Request format :
    {
        "messages": [
            {"role": "user", "content": "What's this?", "image_data": "base64..."},
            {"role": "assistant", "content": "It's a cat."},
            {"role": "user", "content": "What color?"}
        ],
        "temperature": 0.7,
        "max_tokens": 512
    }
    """

    # Construire le prompt
    prompt_parts = []
    images = []

    for msg in request.messages:
        if msg.role == "user":
            if msg.image_data:
                # DÃ©coder image
                image_bytes = base64.b64decode(msg.image_data)
                image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
                images.append(image)
                prompt_parts.append(f"USER: <image>\n{msg.content}")
            else:
                prompt_parts.append(f"USER: {msg.content}")
        else:
            prompt_parts.append(f"ASSISTANT: {msg.content}")

    prompt_parts.append("ASSISTANT:")
    prompt = "\n".join(prompt_parts)

    # Process images
    if images:
        image_tensors = process_images(images, image_processor, model.config)
        image_tensors = image_tensors.to(model.device, dtype=torch.float16)
    else:
        image_tensors = None

    # Tokenize
    input_ids = tokenizer_image_token(
        prompt,
        tokenizer,
        IMAGE_TOKEN_INDEX,
        return_tensors='pt'
    ).unsqueeze(0).to(model.device)

    # Generate
    with torch.inference_mode():
        output_ids = model.generate(
            input_ids,
            images=image_tensors,
            do_sample=request.temperature > 0,
            temperature=request.temperature,
            max_new_tokens=request.max_tokens,
            use_cache=True
        )

    # Decode
    response = tokenizer.decode(
        output_ids[0, input_ids.shape[1]:],
        skip_special_tokens=True
    ).strip()

    return ChatResponse(response=response)


@app.post("/upload-image")
async def upload_image(file: UploadFile = File(...)):
    """Upload image et retourner base64"""
    contents = await file.read()
    base64_image = base64.b64encode(contents).decode('utf-8')
    return {"image_data": base64_image}


@app.get("/health")
async def health():
    """Health check"""
    return {"status": "healthy", "model": "llava-v1.5-7b"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RUN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Partie 2 : Frontend (Gradio)**

```python
"""
vision_chatbot/frontend.py
==========================

Interface Gradio pour chatbot vision
"""

import gradio as gr
import requests
import base64
from PIL import Image
import io

API_URL = "http://localhost:8000"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STATE MANAGEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ChatState:
    def __init__(self):
        self.messages = []
        self.current_image = None

    def add_user_message(self, text, image=None):
        msg = {"role": "user", "content": text}
        if image:
            # Convertir PIL Image en base64
            buffered = io.BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            msg["image_data"] = img_str
            self.current_image = image
        self.messages.append(msg)

    def add_assistant_message(self, text):
        self.messages.append({"role": "assistant", "content": text})

    def clear(self):
        self.messages = []
        self.current_image = None

# Global state
chat_state = ChatState()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def chat_fn(user_message, image, temperature, max_tokens, history):
    """
    Fonction principale de chat

    Args:
        user_message: Message de l'utilisateur
        image: Image uploadÃ©e (PIL Image ou None)
        temperature: TempÃ©rature de sampling
        max_tokens: Nombre max de tokens
        history: Historique de conversation (pour Gradio)

    Returns:
        ("", history_updated) - Vider input et update history
    """

    # Ajouter message utilisateur
    chat_state.add_user_message(user_message, image)

    # Construire affichage pour historique
    if image:
        display_msg = f"ğŸ–¼ï¸ [Image] {user_message}"
    else:
        display_msg = user_message

    history.append([display_msg, None])  # User message, no response yet

    # Appeler API
    try:
        response = requests.post(
            f"{API_URL}/chat",
            json={
                "messages": chat_state.messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            timeout=60
        )

        if response.status_code == 200:
            assistant_response = response.json()["response"]
            chat_state.add_assistant_message(assistant_response)
            history[-1][1] = assistant_response
        else:
            history[-1][1] = f"âŒ Error: {response.status_code}"

    except Exception as e:
        history[-1][1] = f"âŒ Error: {str(e)}"

    return "", history


def clear_fn():
    """Clear conversation"""
    chat_state.clear()
    return None, []


def retry_fn(history, temperature, max_tokens):
    """Retry derniÃ¨re rÃ©ponse"""
    if len(chat_state.messages) >= 2:
        # Remove derniÃ¨re rÃ©ponse
        chat_state.messages.pop()
        history.pop()

        # Re-gÃ©nÃ©rer
        response = requests.post(
            f"{API_URL}/chat",
            json={
                "messages": chat_state.messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
        )

        assistant_response = response.json()["response"]
        chat_state.add_assistant_message(assistant_response)
        history.append([history[-1][0], assistant_response])

    return history


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with gr.Blocks(title="Vision Chatbot", theme=gr.themes.Soft()) as demo:

    gr.Markdown("""
    # ğŸ¦™ğŸ‘ï¸ Vision Chatbot

    Chatbot multimodal powered by LLaVA.
    Upload une image et pose des questions !
    """)

    with gr.Row():
        with gr.Column(scale=2):
            # Chatbox
            chatbot = gr.Chatbot(
                label="Conversation",
                height=500,
                bubble_full_width=False
            )

            # Input
            with gr.Row():
                user_input = gr.Textbox(
                    label="Message",
                    placeholder="Pose une question sur l'image...",
                    scale=4
                )
                submit_btn = gr.Button("ğŸ“¤ Send", scale=1, variant="primary")

            # Buttons
            with gr.Row():
                retry_btn = gr.Button("ğŸ”„ Retry")
                clear_btn = gr.Button("ğŸ—‘ï¸ Clear")

        with gr.Column(scale=1):
            # Image upload
            image_input = gr.Image(
                label="Upload Image",
                type="pil",
                height=300
            )

            gr.Markdown("### âš™ï¸ ParamÃ¨tres")

            temperature = gr.Slider(
                minimum=0.0,
                maximum=1.0,
                value=0.7,
                step=0.1,
                label="Temperature",
                info="CrÃ©ativitÃ© (0=factuel, 1=crÃ©atif)"
            )

            max_tokens = gr.Slider(
                minimum=50,
                maximum=1024,
                value=512,
                step=50,
                label="Max Tokens",
                info="Longueur max de la rÃ©ponse"
            )

            gr.Markdown("""
            ### ğŸ’¡ Tips
            - Upload une image en premier
            - Pose des questions descriptives
            - Utilise temperature=0 pour rÃ©ponses factuelles
            - Utilise temperature=0.7-1.0 pour crÃ©ativitÃ©

            ### ğŸ“ Exemples
            - "What's in this image?"
            - "Describe the scene in detail"
            - "What color is the car?"
            - "How many people are there?"
            - "What emotion does this convey?"
            """)

    # Events
    submit_btn.click(
        fn=chat_fn,
        inputs=[user_input, image_input, temperature, max_tokens, chatbot],
        outputs=[user_input, chatbot]
    )

    user_input.submit(
        fn=chat_fn,
        inputs=[user_input, image_input, temperature, max_tokens, chatbot],
        outputs=[user_input, chatbot]
    )

    retry_btn.click(
        fn=retry_fn,
        inputs=[chatbot, temperature, max_tokens],
        outputs=[chatbot]
    )

    clear_btn.click(
        fn=clear_fn,
        outputs=[image_input, chatbot]
    )

# Launch
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
```

**Partie 3 : Docker Deployment**

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Install Python
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Expose ports
EXPOSE 8000 7860

# Run both backend and frontend
CMD ["bash", "start.sh"]
```

```bash
# start.sh
#!/bin/bash

# Start backend
python3 backend.py &

# Wait for backend
sleep 10

# Start frontend
python3 frontend.py
```

```txt
# requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
gradio==4.10.0
torch==2.1.0
torchvision==0.16.0
transformers==4.36.0
accelerate==0.25.0
bitsandbytes==0.41.3
llava @ git+https://github.com/haotian-liu/LLaVA.git
pillow==10.1.0
requests==2.31.0
```

### 9.4 Utilisation

**1. Lancer le backend** :
```bash
python backend.py
# Backend running on http://localhost:8000
```

**2. Lancer le frontend** :
```bash
python frontend.py
# Gradio running on http://localhost:7860
```

**3. Utiliser** :
- Ouvrir http://localhost:7860
- Upload une image
- Poser des questions
- Conversation multi-turn !

**4. DÃ©ployer avec Docker** :
```bash
docker build -t vision-chatbot .
docker run -p 8000:8000 -p 7860:7860 --gpus all vision-chatbot
```

---

## 10. Best Practices et Troubleshooting

### 10.1 Best Practices

**âœ… DO** :

1. **PrÃ©traiter les images** :
```python
# Resize pour consistency
image = image.resize((224, 224))

# Normaliser avec stats CLIP
normalize = transforms.Normalize(
    mean=[0.48145466, 0.4578275, 0.40821073],
    std=[0.26862954, 0.26130258, 0.27577711]
)
```

2. **Utiliser batch processing** :
```python
# Traiter plusieurs images en batch
images = [image1, image2, image3]
features = vision_encoder(torch.stack(images))
# Plus rapide que boucle !
```

3. **Cache les embeddings** :
```python
# Pour images fixes, cache les features
@lru_cache(maxsize=100)
def get_image_features(image_path):
    image = load_image(image_path)
    return vision_encoder(image)
```

4. **Monitorer la mÃ©moire** :
```python
# Clear GPU cache rÃ©guliÃ¨rement
if batch_idx % 100 == 0:
    torch.cuda.empty_cache()
```

**âŒ DON'T** :

1. âŒ Oublier de normaliser les images
2. âŒ Utiliser des rÃ©solutions inconsistantes
3. âŒ Charger tout le dataset en RAM
4. âŒ Oublier de freeze les modÃ¨les prÃ©-entraÃ®nÃ©s
5. âŒ Ignorer les warnings GPU memory

### 10.2 Troubleshooting Commun

**ProblÃ¨me 1** : Out of Memory (OOM)

```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**Solutions** :
```python
# Solution 1 : Quantization
model = load_model(load_8bit=True)  # INT8

# Solution 2 : Gradient checkpointing
model.gradient_checkpointing_enable()

# Solution 3 : RÃ©duire batch size
batch_size = 4  # Au lieu de 16

# Solution 4 : RÃ©duire rÃ©solution
image_size = 224  # Au lieu de 336

# Solution 5 : Clear cache
torch.cuda.empty_cache()
```

**ProblÃ¨me 2** : GÃ©nÃ©ration lente

**Solutions** :
```python
# Solution 1 : Quantization
load_4bit=True  # 3-4Ã— plus rapide

# Solution 2 : Compiler le modÃ¨le (PyTorch 2.0+)
model = torch.compile(model)

# Solution 3 : RÃ©duire max_tokens
max_new_tokens=256  # Au lieu de 512

# Solution 4 : Utiliser cache KV
use_cache=True
```

**ProblÃ¨me 3** : Mauvaise qualitÃ© de rÃ©ponses

**Solutions** :
```python
# Solution 1 : Ajuster temperature
temperature=0.2  # Plus factuel

# Solution 2 : Better prompting
prompt = """<image>
Analyze this image in detail. Include:
1. Main objects
2. Colors and composition
3. Context and setting
Describe:"""

# Solution 3 : Fine-tune sur votre domaine
# Train sur dataset spÃ©cifique

# Solution 4 : Utiliser un modÃ¨le plus grand
model = "llava-v1.6-34b"  # Au lieu de 7b
```

**ProblÃ¨me 4** : Images mal comprises

**Causes + Solutions** :
```python
# Cause 1 : Image corrompue
try:
    image = Image.open(path)
    image.verify()  # Check integrity
except:
    print("Image corrompue!")

# Cause 2 : Mauvais format
image = image.convert('RGB')  # Force RGB

# Cause 3 : RÃ©solution trop basse
if image.size[0] < 224 or image.size[1] < 224:
    print("Warning: Image trop petite!")

# Cause 4 : Image trop complexe
# Crop ou focus sur partie importante
```

### 10.3 Monitoring en Production

```python
"""
MONITORING SETUP pour chatbot vision en production
"""

from prometheus_client import Counter, Histogram, Gauge
import time

# Metrics
REQUEST_COUNT = Counter('chat_requests_total', 'Total chat requests')
REQUEST_LATENCY = Histogram('chat_latency_seconds', 'Request latency')
ACTIVE_USERS = Gauge('active_users', 'Number of active users')
GPU_MEMORY = Gauge('gpu_memory_used_bytes', 'GPU memory used')

@app.post("/chat")
async def chat(request: ChatRequest):
    REQUEST_COUNT.inc()

    start_time = time.time()

    try:
        # Process...
        response = process_chat(request)

        # Metrics
        REQUEST_LATENCY.observe(time.time() - start_time)
        GPU_MEMORY.set(torch.cuda.memory_allocated())

        return response

    except Exception as e:
        # Log error
        logger.error(f"Chat error: {e}")
        raise


# Dashboard Grafana :
# - Latency p50, p95, p99
# - Throughput (requests/sec)
# - GPU utilization
# - Error rate
```

---

## 11. Quiz et Exercices

### 11.1 Quiz de ComprÃ©hension

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ QUIZ : Testez Vos Connaissances Multimodal !
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Question 1** [Facile] : Quel est le rÃ´le du vision encoder dans un modÃ¨le multimodal ?

a) GÃ©nÃ©rer du texte depuis une image
b) Convertir pixels en embeddings sÃ©mantiques  âœ…
c) Traduire texte en image
d) Compresser l'image

**RÃ©ponse** : b) Le vision encoder (comme CLIP) transforme les pixels bruts en vecteurs qui capturent le sens sÃ©mantique de l'image.

---

**Question 2** [Moyen] : Pourquoi LLaVA envoie-t-il 256 tokens visuels au LLM ?

a) C'est le nombre de pixels
b) C'est le nombre de patches (14Ã—14 + CLS)  âœ…
c) C'est arbitraire
d) Pour ralentir le modÃ¨le

**RÃ©ponse** : b) CLIP dÃ©coupe l'image en patches 14Ã—14 = 196, plus un token CLS = 197 (arrondi Ã  256).

---

**Question 3** [Moyen] : Quelle est l'innovation principale de BLIP-2 ?

a) Utiliser CLIP
b) Le Q-Former qui compresse intelligemment  âœ…
c) Multi-modal attention
d) Training plus rapide

**RÃ©ponse** : b) Le Q-Former utilise des learnable queries pour compresser 256 tokens â†’ 32 tokens tout en gardant l'information importante.

---

**Question 4** [AvancÃ©] : Comment LLaVA gÃ©nÃ¨re-t-il son dataset d'instruction ?

a) Annotation humaine
b) Scraping d'Internet
c) Utilise GPT-4 pour gÃ©nÃ©rer Q&A depuis captions  âœ…
d) SynthÃ¨se automatique

**RÃ©ponse** : c) LLaVA utilise GPT-4 (text-only) pour gÃ©nÃ©rer des questions-rÃ©ponses sophistiquÃ©es depuis les captions existantes. GÃ©nie !

---

**Question 5** [AvancÃ©] : Pourquoi freeze-t-on CLIP et LLM pendant training ?

a) Pour Ã©conomiser VRAM
b) Pour Ã©viter catastrophic forgetting  âœ…
c) Parce que c'est plus rapide
d) Par paresse

**RÃ©ponse** : b) Si on fine-tune CLIP/LLM, ils risquent d'oublier ce qu'ils ont appris. Mieux vaut entraÃ®ner seulement le "pont" (projector).

---

**Question 6** [Expert] : Calculez la mÃ©moire nÃ©cessaire pour LLaVA-7B en FP16.

Indices :
- Llama-2-7B : 7B params
- CLIP : 427M params
- Projector : 100M params
- FP16 : 2 bytes/param

**RÃ©ponse** :
```
Total params = 7B + 0.427B + 0.1B = 7.527B
Memory = 7.527B Ã— 2 bytes = 15.054 GB
+ Activations (~2GB) = ~17 GB total

Avec 8-bit quantization : ~8.5 GB
Avec 4-bit quantization : ~5 GB
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 11.2 Exercices Pratiques

**Exercice 1** [DÃ©butant] : Compter les objets

```python
"""
EXERCICE : CrÃ©er un compteur d'objets automatique

Input : Image avec plusieurs objets
Output : "Il y a X chats, Y chiens, Z voitures"

DifficultÃ© : â­âšªâšªâšªâšª
Temps : 30 minutes
"""

def count_objects(image_path):
    """
    TODO : ImplÃ©menter

    Hint : Utiliser LLaVA avec prompt spÃ©cifique
    """
    pass

# Test
image = "street_scene.jpg"
result = count_objects(image)
# Expected : "2 cars, 3 people, 1 dog"
```

**Solution** :
```python
def count_objects(image_path):
    bot = LLaVAChatbot()

    prompt = """List all objects in this image with their counts.
    Format: "X object1, Y object2, Z object3"
    Be specific and accurate."""

    response = bot.chat(image_path, prompt, temperature=0.1)
    return response
```

---

**Exercice 2** [IntermÃ©diaire] : Comparaison d'images

```python
"""
EXERCICE : Comparer deux images

Input : Deux images
Output : SimilaritÃ©s et diffÃ©rences

DifficultÃ© : â­â­â­âšªâšª
Temps : 1 heure
"""

def compare_images(image1_path, image2_path):
    """
    TODO : ImplÃ©menter

    Hints :
    1. Encoder les deux images sÃ©parÃ©ment
    2. Utiliser LLM pour comparer
    3. Ou calculer similaritÃ© cosine des features
    """
    pass

# Test
img1 = "cat1.jpg"
img2 = "cat2.jpg"
diff = compare_images(img1, img2)
# Expected : "Both show cats. Image 1 has orange cat,
#             Image 2 has black cat..."
```

**Solution** :
```python
def compare_images(image1_path, image2_path):
    bot = LLaVAChatbot()

    # DÃ©crire image 1
    desc1 = bot.chat(image1_path, "Describe this image in detail.")

    # DÃ©crire image 2
    desc2 = bot.chat(image2_path, "Describe this image in detail.")

    # Comparer
    comparison_prompt = f"""
    Compare these two descriptions:

    Image 1: {desc1}
    Image 2: {desc2}

    List:
    - Similarities
    - Differences
    - Which is better for [specific use case]?
    """

    # Note : IdÃ©alement, multi-image input si supportÃ©
    comparison = bot.chat(None, comparison_prompt)

    return comparison
```

---

**Exercice 3** [AvancÃ©] : Video Summarization

```python
"""
EXERCICE : RÃ©sumer une vidÃ©o

Input : VidÃ©o MP4
Output : RÃ©sumÃ© textuel des Ã©vÃ©nements

DifficultÃ© : â­â­â­â­âšª
Temps : 2-3 heures
"""

def summarize_video(video_path):
    """
    TODO : ImplÃ©menter

    Steps :
    1. Extraire frames clÃ©s (1 par seconde)
    2. Analyser chaque frame avec LLaVA
    3. AgrÃ©ger les descriptions
    4. GÃ©nÃ©rer rÃ©sumÃ© cohÃ©rent
    """
    pass

# Test
video = "cooking_tutorial.mp4"
summary = summarize_video(video)
# Expected : "The video shows a cooking tutorial where...
#             First, ingredients are prepared...
#             Then, the mixture is cooked...
#             Finally, the dish is plated..."
```

**Solution** :
```python
import cv2

def summarize_video(video_path, frames_per_second=1):
    bot = LLaVAChatbot()

    # 1. Extraire frames
    frames = extract_frames_from_video(video_path, fps=frames_per_second)

    # 2. Analyser chaque frame
    descriptions = []
    for i, frame in enumerate(frames):
        # Save frame temporairement
        frame_path = f"temp_frame_{i}.jpg"
        cv2.imwrite(frame_path, frame)

        # Analyser
        desc = bot.chat(
            frame_path,
            f"Describe what's happening at timestamp {i} seconds."
        )
        descriptions.append(f"[{i}s] {desc}")

    # 3. AgrÃ©ger et rÃ©sumer
    all_descriptions = "\n".join(descriptions)

    summary_prompt = f"""
    Based on these frame descriptions from a video:

    {all_descriptions}

    Write a coherent summary of the entire video in 2-3 paragraphs.
    Focus on the main storyline and key events.
    """

    summary = bot.chat(None, summary_prompt)

    return summary


def extract_frames_from_video(video_path, fps=1):
    """Extract frames at specified FPS"""
    cap = cv2.VideoCapture(video_path)
    video_fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(video_fps / fps)

    frames = []
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            frames.append(frame)

        frame_count += 1

    cap.release()
    return frames
```

---

**Exercice 4** [Expert] : Fine-tune pour Domaine SpÃ©cifique

```python
"""
EXERCICE FINAL : Fine-tune LLaVA pour domaine mÃ©dical

Objectif : Adapter LLaVA pour analyser radiographies

Dataset : 1000 X-rays avec annotations mÃ©dicales
Steps :
1. PrÃ©parer dataset (images + reports)
2. Fine-tune avec LoRA
3. Ã‰valuer sur test set

DifficultÃ© : â­â­â­â­â­
Temps : 1-2 jours
CoÃ»t : ~$50-100 GPU
"""

# Starter code fourni dans les ressources du chapitre
# Voir : medical_llava_finetune.py
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## ğŸ‰ CONCLUSION DU CHAPITRE

FÃ©licitations ! Vous maÃ®trisez maintenant les modÃ¨les multimodaux ! ğŸ¦™ğŸ‘ï¸

**Ce que vous avez appris** :
- âœ… Architecture complÃ¨te (Vision â†’ Projection â†’ LLM)
- âœ… GPT-4V et ses capacitÃ©s impressionnantes
- âœ… LLaVA : Open-source hero ($500 training!)
- âœ… BLIP-2 et Q-Former compression
- âœ… Audio et vidÃ©o understanding
- âœ… Training pipeline complet
- âœ… Projet pratique : Chatbot vision dÃ©ployable
- âœ… Best practices et troubleshooting

**Points clÃ©s Ã  retenir** :
1. **Multimodal = Vision Encoder + Projection + LLM** (simple!)
2. **CLIP** est le standard pour vision encoding
3. **LLaVA** dÃ©montre qu'open-source peut rivaliser
4. **Q-Former** (BLIP-2) = compression intelligente
5. **Training** = Freeze encoders, train projector only
6. **VidÃ©o** = SÃ©quence d'images avec sampling intelligent

**L'avenir du multimodal** :
- ğŸš€ ModÃ¨les natifs (Gemini 1.5 approach)
- ğŸ¥ VidÃ©os longues (>1h context)
- ğŸ¨ GÃ©nÃ©ration image+texte jointe
- ğŸŒ Multimodal pour toutes les langues
- ğŸ¤– Agents autonomes avec vision

**Prochaines Ã©tapes** :
- Pratiquer avec les exercices
- DÃ©ployer votre chatbot vision
- Fine-tune pour votre domaine
- Contribuer Ã  la communautÃ© open-source!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¬ **DERNIER MOT de Bob**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Bob** : "Alice, tu te souviens quand tu m'as demandÃ© comment un LLM pouvait 'voir' ?"

**Alice** : "Oui ! Et maintenant je peux crÃ©er mon propre modÃ¨le vision pour $500. C'est dingue !"

**Bob** : "Le plus fou ? On n'a fait qu'effleurer la surface. Dans 2 ans, chaque app aura de la vision AI. Chaque robot, chaque voiture, chaque phone. La multimodalitÃ© n'est pas l'avenirâ€”c'est le prÃ©sent."

**Alice** : "Je vais commencer par ce chatbot mÃ©dical. Imagine l'impact : aider les mÃ©decins Ã  dÃ©tecter les maladies plus tÃ´t..."

**Bob** : "VoilÃ  l'esprit ! La tech n'est qu'un outil. Ce qui compte, c'est ce que TU vas en faire. Go build something amazing! ğŸš€"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

---

**Ressources Additionnelles** :
- ğŸ“– Papers : GPT-4V, LLaVA, BLIP-2, Flamingo
- ğŸ’» Code : github.com/haotian-liu/LLaVA
- ğŸ¥ Demos : https://llava.hliu.cc
- ğŸ’¬ Community : HuggingFace Discord, r/LocalLLaMA
- ğŸ“š Datasets : LAION, COCO, Visual Genome

**Prochain chapitre** : Chapitre 23 - Deployment & Production ğŸš€

---

*Fin du Chapitre 22 : Multimodal LLMs*
